{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OztDWF9tl4r0"
   },
   "source": [
    "# West Ham Topic Modelling\n",
    "\n",
    "Here I will try Topic Modelling on some of the West Ham text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bpd0mo9xl4r1"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim import models, corpora\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xD_roSl-l4r2",
    "outputId": "f8ed4736-e384-4831-ac13-a91ca59438d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dglover\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iMoyh208l4r3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dglover\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "b3V5AnQul4r3"
   },
   "outputs": [],
   "source": [
    "NUM_TOPICS = 10\n",
    "STOPWORDS = stopwords.words('english')\n",
    "STOPWRODS = STOPWORDS.append('wrote')\n",
    "\n",
    "def clean_text(text):\n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    cleaned_text = [t for t in tokenized_text if t not in STOPWORDS and re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', t) and pd.notnull(t)]\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "G8K87To5l4r3",
    "outputId": "e02dcb6b-6e44-46b8-e91f-17dc9bc1b3c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'wrote']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VJzFrw6ql4r4",
    "outputId": "4b2e00f0-e67d-4fb7-a6ff-56472a2d43a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3558, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>PDF URL</th>\n",
       "      <th>Author</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Journal Ref</th>\n",
       "      <th>Primary Category</th>\n",
       "      <th>Category</th>\n",
       "      <th>Entry ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Natural Language Processing using Hadoop and K...</td>\n",
       "      <td>http://arxiv.org/pdf/1608.04434v1</td>\n",
       "      <td>[arxiv.Result.Author('Emre Erturk'), arxiv.Res...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-15 23:09:21+00:00</td>\n",
       "      <td>Natural language processing, as a data analyti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1608.04434v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Integrating AI Planning with Natural Language ...</td>\n",
       "      <td>http://arxiv.org/pdf/2202.07138v2</td>\n",
       "      <td>[arxiv.Result.Author('Kebing Jin'), arxiv.Resu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-15 02:19:09+00:00</td>\n",
       "      <td>Natural language processing (NLP) aims at inve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>['cs.AI', 'cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/2202.07138v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Simple Natural Language Processing Tools for D...</td>\n",
       "      <td>http://arxiv.org/pdf/1906.11608v2</td>\n",
       "      <td>[arxiv.Result.Author('Leon Derczynski')]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-06-27 13:15:12+00:00</td>\n",
       "      <td>This technical note describes a set of baselin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1906.11608v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Towards the Study of Morphological Processing ...</td>\n",
       "      <td>http://arxiv.org/pdf/2006.16212v1</td>\n",
       "      <td>[arxiv.Result.Author('Mirinso Shadang'), arxiv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-06-29 17:24:09+00:00</td>\n",
       "      <td>There is no or little work on natural language...</td>\n",
       "      <td>In proceeding of Regional International Confer...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/2006.16212v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Natural Language Understanding with Distribute...</td>\n",
       "      <td>http://arxiv.org/pdf/1511.07916v1</td>\n",
       "      <td>[arxiv.Result.Author('Kyunghyun Cho')]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-11-24 23:23:13+00:00</td>\n",
       "      <td>This is a lecture note for the course DS-GA 30...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL', 'stat.ML']</td>\n",
       "      <td>http://arxiv.org/abs/1511.07916v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              Title  \\\n",
       "0           0  Natural Language Processing using Hadoop and K...   \n",
       "1           1  Integrating AI Planning with Natural Language ...   \n",
       "2           2  Simple Natural Language Processing Tools for D...   \n",
       "3           3  Towards the Study of Morphological Processing ...   \n",
       "4           4  Natural Language Understanding with Distribute...   \n",
       "\n",
       "                             PDF URL  \\\n",
       "0  http://arxiv.org/pdf/1608.04434v1   \n",
       "1  http://arxiv.org/pdf/2202.07138v2   \n",
       "2  http://arxiv.org/pdf/1906.11608v2   \n",
       "3  http://arxiv.org/pdf/2006.16212v1   \n",
       "4  http://arxiv.org/pdf/1511.07916v1   \n",
       "\n",
       "                                              Author  DOI  \\\n",
       "0  [arxiv.Result.Author('Emre Erturk'), arxiv.Res...  NaN   \n",
       "1  [arxiv.Result.Author('Kebing Jin'), arxiv.Resu...  NaN   \n",
       "2           [arxiv.Result.Author('Leon Derczynski')]  NaN   \n",
       "3  [arxiv.Result.Author('Mirinso Shadang'), arxiv...  NaN   \n",
       "4             [arxiv.Result.Author('Kyunghyun Cho')]  NaN   \n",
       "\n",
       "              Published Date  \\\n",
       "0  2016-08-15 23:09:21+00:00   \n",
       "1  2022-02-15 02:19:09+00:00   \n",
       "2  2019-06-27 13:15:12+00:00   \n",
       "3  2020-06-29 17:24:09+00:00   \n",
       "4  2015-11-24 23:23:13+00:00   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  Natural language processing, as a data analyti...   \n",
       "1  Natural language processing (NLP) aims at inve...   \n",
       "2  This technical note describes a set of baselin...   \n",
       "3  There is no or little work on natural language...   \n",
       "4  This is a lecture note for the course DS-GA 30...   \n",
       "\n",
       "                                         Journal Ref Primary Category  \\\n",
       "0                                                NaN            cs.CL   \n",
       "1                                                NaN            cs.AI   \n",
       "2                                                NaN            cs.CL   \n",
       "3  In proceeding of Regional International Confer...            cs.CL   \n",
       "4                                                NaN            cs.CL   \n",
       "\n",
       "               Category                           Entry ID  \n",
       "0             ['cs.CL']  http://arxiv.org/abs/1608.04434v1  \n",
       "1    ['cs.AI', 'cs.CL']  http://arxiv.org/abs/2202.07138v2  \n",
       "2             ['cs.CL']  http://arxiv.org/abs/1906.11608v2  \n",
       "3             ['cs.CL']  http://arxiv.org/abs/2006.16212v1  \n",
       "4  ['cs.CL', 'stat.ML']  http://arxiv.org/abs/1511.07916v1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load some data\n",
    "hammerchat = pd.read_csv('../data/arxiv_papers_full_v2.csv')\n",
    "print (hammerchat.shape)\n",
    "hammerchat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-mmRM1Ell4r4",
    "outputId": "a1378057-f98c-4ca7-c2e2-676d0f000e16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3558 entries, 0 to 3557\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Unnamed: 0        3558 non-null   int64 \n",
      " 1   Title             3558 non-null   object\n",
      " 2   PDF URL           3558 non-null   object\n",
      " 3   Author            3558 non-null   object\n",
      " 4   DOI               403 non-null    object\n",
      " 5   Published Date    3558 non-null   object\n",
      " 6   Summary           3558 non-null   object\n",
      " 7   Journal Ref       515 non-null    object\n",
      " 8   Primary Category  3558 non-null   object\n",
      " 9   Category          3558 non-null   object\n",
      " 10  Entry ID          3558 non-null   object\n",
      "dtypes: int64(1), object(10)\n",
      "memory usage: 305.9+ KB\n"
     ]
    }
   ],
   "source": [
    "hammerchat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QETvez4Il4r4",
    "outputId": "f7579789-778f-4024-a1e2-ec52b327b611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3558, 11)\n"
     ]
    }
   ],
   "source": [
    "# Reduce the Dataframe\n",
    "hammerchat = hammerchat.dropna(subset=['Summary'])\n",
    "print(hammerchat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xu7H5frfl4r4",
    "outputId": "84de5ead-84ef-4d44-c8e1-148005326e7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3558\n"
     ]
    }
   ],
   "source": [
    "# For gensim we need to tokenize the data and filter out stopwords\n",
    "tokenized_data = []\n",
    "for q,text in enumerate(hammerchat['Summary']):\n",
    "    tokenized_data.append(clean_text(text))\n",
    "\n",
    "\n",
    "print(len(tokenized_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7joIqwSnl4r4",
    "outputId": "af7a6d94-b63c-4210-87ba-e69a395e8de2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(28, 4), (31, 1), (39, 1), (45, 1), (52, 1), (76, 1), (92, 1), (115, 3), (184, 2), (196, 1), (201, 1), (206, 2), (207, 1), (212, 2), (294, 1), (357, 1), (450, 1), (504, 2), (556, 1), (622, 3), (801, 1), (802, 1), (803, 1), (804, 1), (805, 1), (806, 2), (807, 1), (808, 1), (809, 1), (810, 1), (811, 1), (812, 1), (813, 1), (814, 1), (815, 1), (816, 1), (817, 1), (818, 1), (819, 1), (820, 1), (821, 1), (822, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Build a Dictionary - association word to numeric id\n",
    "dictionary = corpora.Dictionary(tokenized_data)\n",
    "\n",
    "# Transform the collection of texts to a numerical form\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n",
    "\n",
    "# Have a look at how the 20th document looks like: [(word_id, count), ...]\n",
    "print(corpus[20])\n",
    "# [(12, 3), (14, 1), (21, 1), (25, 5), (30, 2), (31, 5), (33, 1), (42, 1), (43, 2),  ...\n",
    "\n",
    "# Build the LDA model\n",
    "lda_model = models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)\n",
    "\n",
    "# Build the LSI model\n",
    "#lsi_model = models.LsiModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "STtuOYI5l4r5",
    "outputId": "cbddd70c-3406-4382-f675-47c8423af025"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"natural\" + 0.014*\"language\" + 0.013*\"nlp\" + 0.009*\"processing\" + 0.008*\"learning\" + 0.007*\"models\" + 0.007*\"research\" + 0.007*\"data\"'),\n",
       " (1,\n",
       "  '0.017*\"language\" + 0.011*\"natural\" + 0.008*\"model\" + 0.008*\"research\" + 0.007*\"processing\" + 0.006*\"models\" + 0.006*\"speech\" + 0.006*\"nlp\"'),\n",
       " (2,\n",
       "  '0.026*\"language\" + 0.014*\"natural\" + 0.012*\"models\" + 0.009*\"processing\" + 0.009*\"tasks\" + 0.009*\"nlp\" + 0.007*\"learning\" + 0.007*\"data\"'),\n",
       " (3,\n",
       "  '0.018*\"language\" + 0.015*\"embeddings\" + 0.012*\"models\" + 0.011*\"word\" + 0.011*\"natural\" + 0.010*\"processing\" + 0.008*\"nlp\" + 0.008*\"task\"'),\n",
       " (4,\n",
       "  '0.024*\"language\" + 0.019*\"models\" + 0.014*\"natural\" + 0.011*\"tasks\" + 0.010*\"model\" + 0.008*\"data\" + 0.008*\"nlp\" + 0.008*\"processing\"'),\n",
       " (5,\n",
       "  '0.023*\"language\" + 0.012*\"processing\" + 0.010*\"natural\" + 0.008*\"nlp\" + 0.007*\"text\" + 0.006*\"data\" + 0.006*\"models\" + 0.006*\"word\"'),\n",
       " (6,\n",
       "  '0.032*\"language\" + 0.017*\"models\" + 0.013*\"natural\" + 0.009*\"model\" + 0.009*\"processing\" + 0.007*\"paper\" + 0.006*\"tasks\" + 0.006*\"languages\"'),\n",
       " (7,\n",
       "  '0.027*\"language\" + 0.013*\"natural\" + 0.010*\"learning\" + 0.009*\"model\" + 0.009*\"models\" + 0.007*\"processing\" + 0.006*\"knowledge\" + 0.005*\"data\"'),\n",
       " (8,\n",
       "  '0.030*\"language\" + 0.013*\"natural\" + 0.010*\"models\" + 0.009*\"languages\" + 0.006*\"text\" + 0.006*\"model\" + 0.005*\"processing\" + 0.005*\"nlp\"'),\n",
       " (9,\n",
       "  '0.023*\"language\" + 0.015*\"model\" + 0.013*\"natural\" + 0.011*\"processing\" + 0.010*\"data\" + 0.009*\"models\" + 0.009*\"tasks\" + 0.009*\"text\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(num_words=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "I7NqIGQfl4r5"
   },
   "outputs": [],
   "source": [
    "topics = lda_model.print_topics(num_words=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "zqnCeAqfl4r5"
   },
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.014*\"natural\" + 0.014*\"language\" + 0.013*\"nl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.017*\"language\" + 0.011*\"natural\" + 0.008*\"mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.026*\"language\" + 0.014*\"natural\" + 0.012*\"mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.018*\"language\" + 0.015*\"embeddings\" + 0.012*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.024*\"language\" + 0.019*\"models\" + 0.014*\"nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.023*\"language\" + 0.012*\"processing\" + 0.010*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.032*\"language\" + 0.017*\"models\" + 0.013*\"nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.027*\"language\" + 0.013*\"natural\" + 0.010*\"le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.030*\"language\" + 0.013*\"natural\" + 0.010*\"mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.023*\"language\" + 0.015*\"model\" + 0.013*\"natu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  1\n",
       "0  0  0.014*\"natural\" + 0.014*\"language\" + 0.013*\"nl...\n",
       "1  1  0.017*\"language\" + 0.011*\"natural\" + 0.008*\"mo...\n",
       "2  2  0.026*\"language\" + 0.014*\"natural\" + 0.012*\"mo...\n",
       "3  3  0.018*\"language\" + 0.015*\"embeddings\" + 0.012*...\n",
       "4  4  0.024*\"language\" + 0.019*\"models\" + 0.014*\"nat...\n",
       "5  5  0.023*\"language\" + 0.012*\"processing\" + 0.010*...\n",
       "6  6  0.032*\"language\" + 0.017*\"models\" + 0.013*\"nat...\n",
       "7  7  0.027*\"language\" + 0.013*\"natural\" + 0.010*\"le...\n",
       "8  8  0.030*\"language\" + 0.013*\"natural\" + 0.010*\"mo...\n",
       "9  9  0.023*\"language\" + 0.015*\"model\" + 0.013*\"natu..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvJCDzo0l4r5"
   },
   "outputs": [],
   "source": [
    "#topics_df.to_csv('Hammerschat_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRziT0tal4r5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CI9mUZ02l4r5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzS_i8cyl4r5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
