{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87fc752e",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd90fd",
   "metadata": {},
   "source": [
    "## 0.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "36bd3d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dglover\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "217c8d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd49c49d",
   "metadata": {},
   "source": [
    "## 1.0 Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aa4a4912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "feb04083",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PDF URL</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/pdf/1608.04434v1</th>\n",
       "      <td>Hadoop is one of the platform s that can proce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/pdf/2202.07138v2</th>\n",
       "      <td>Integrating AI Planning with Natural Language ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/pdf/1906.11608v2</th>\n",
       "      <td>The tools are machine learning based using nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/pdf/2006.16212v1</th>\n",
       "      <td>The name Tangkhul also known as Hao or Ihao re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/pdf/1511.07916v1</th>\n",
       "      <td>This is a lecture note for the course DS GA at...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             Summary\n",
       "PDF URL                                                                             \n",
       "http://arxiv.org/pdf/1608.04434v1  Hadoop is one of the platform s that can proce...\n",
       "http://arxiv.org/pdf/2202.07138v2  Integrating AI Planning with Natural Language ...\n",
       "http://arxiv.org/pdf/1906.11608v2  The tools are machine learning based using nat...\n",
       "http://arxiv.org/pdf/2006.16212v1  The name Tangkhul also known as Hao or Ihao re...\n",
       "http://arxiv.org/pdf/1511.07916v1  This is a lecture note for the course DS GA at..."
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dataset\n",
    "df = pd.read_csv('..\\\\data\\\\summaries_full.csv', index_col=0)\n",
    "#print(df.target_names.unique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "863d7d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3557, 1)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bacb412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "714afa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = list(df['Summary'])\n",
    "vectorizer = CountVectorizer(decode_error='ignore')\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "01b1a7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aa', 'aaaa', 'aaai', ..., 'zwnj', 'zynq', 'zzy'], dtype=object)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f06b0f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_df = pd.DataFrame(X.toarray())\n",
    "CV_df.columns = vectorizer.get_feature_names_out()\n",
    "#CV_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "631d3066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3556, 24780)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9c1cb703",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CV_stopwords = []\n",
    "for col in CV_df.columns:\n",
    "    if sum(CV_df[col]) > 0.25 * (CV_df.shape[0]):\n",
    "        CV_stopwords.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "59666fe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['also',\n",
       " 'an',\n",
       " 'and',\n",
       " 'are',\n",
       " 'as',\n",
       " 'at',\n",
       " 'based',\n",
       " 'be',\n",
       " 'by',\n",
       " 'can',\n",
       " 'data',\n",
       " 'for',\n",
       " 'from',\n",
       " 'has',\n",
       " 'have',\n",
       " 'in',\n",
       " 'information',\n",
       " 'is',\n",
       " 'it',\n",
       " 'language',\n",
       " 'languages',\n",
       " 'learning',\n",
       " 'model',\n",
       " 'models',\n",
       " 'natural',\n",
       " 'new',\n",
       " 'nlp',\n",
       " 'of',\n",
       " 'on',\n",
       " 'or',\n",
       " 'our',\n",
       " 'paper',\n",
       " 'processing',\n",
       " 'published',\n",
       " 'study',\n",
       " 'such',\n",
       " 'task',\n",
       " 'tasks',\n",
       " 'text',\n",
       " 'that',\n",
       " 'the',\n",
       " 'this',\n",
       " 'to',\n",
       " 'university',\n",
       " 'use',\n",
       " 'used',\n",
       " 'using',\n",
       " 'we',\n",
       " 'with']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d35830de",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stopwords = ['author','article','use', 'show', 'however', 'approach', 'well', 'provide',' present', 'include', 'word', 'nlp', 'natural', 'language', 'processing']\n",
    "full_stopwords = CV_stopwords + extra_stopwords\n",
    "stop_words.extend(full_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f1f6db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "827f680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "df2 = pd.read_csv('..\\\\data\\\\arxiv_papers_full_v2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "05f61cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>PDF URL</th>\n",
       "      <th>Author</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Journal Ref</th>\n",
       "      <th>Primary Category</th>\n",
       "      <th>Category</th>\n",
       "      <th>Entry ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Natural Language Processing using Hadoop and K...</td>\n",
       "      <td>http://arxiv.org/pdf/1608.04434v1</td>\n",
       "      <td>[arxiv.Result.Author('Emre Erturk'), arxiv.Res...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-15 23:09:21+00:00</td>\n",
       "      <td>Natural language processing, as a data analyti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1608.04434v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Integrating AI Planning with Natural Language ...</td>\n",
       "      <td>http://arxiv.org/pdf/2202.07138v2</td>\n",
       "      <td>[arxiv.Result.Author('Kebing Jin'), arxiv.Resu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-15 02:19:09+00:00</td>\n",
       "      <td>Natural language processing (NLP) aims at inve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>['cs.AI', 'cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/2202.07138v2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Natural Language Processing using Hadoop and K...   \n",
       "1  Integrating AI Planning with Natural Language ...   \n",
       "\n",
       "                             PDF URL  \\\n",
       "0  http://arxiv.org/pdf/1608.04434v1   \n",
       "1  http://arxiv.org/pdf/2202.07138v2   \n",
       "\n",
       "                                              Author  DOI  \\\n",
       "0  [arxiv.Result.Author('Emre Erturk'), arxiv.Res...  NaN   \n",
       "1  [arxiv.Result.Author('Kebing Jin'), arxiv.Resu...  NaN   \n",
       "\n",
       "              Published Date  \\\n",
       "0  2016-08-15 23:09:21+00:00   \n",
       "1  2022-02-15 02:19:09+00:00   \n",
       "\n",
       "                                             Summary Journal Ref  \\\n",
       "0  Natural language processing, as a data analyti...         NaN   \n",
       "1  Natural language processing (NLP) aims at inve...         NaN   \n",
       "\n",
       "  Primary Category            Category                           Entry ID  \n",
       "0            cs.CL           ['cs.CL']  http://arxiv.org/abs/1608.04434v1  \n",
       "1            cs.AI  ['cs.AI', 'cs.CL']  http://arxiv.org/abs/2202.07138v2  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "454d6e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.rename(columns = {'Summary' : 'Abstract'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d63e95d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2['Filename'] = df2['PDF URL'].map(lambda x: x.split('/')[-1] + '.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4f9755d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df2, how = 'left', on = 'PDF URL', indicator = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "26ab14fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "both          3568\n",
       "left_only        0\n",
       "right_only       0\n",
       "Name: _merge, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['_merge'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "796e6d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PDF URL                0\n",
       "Summary                0\n",
       "Title                  0\n",
       "Author                 0\n",
       "DOI                 3165\n",
       "Published Date         0\n",
       "Abstract               0\n",
       "Journal Ref         3052\n",
       "Primary Category       0\n",
       "Category               0\n",
       "Entry ID               0\n",
       "_merge                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "18bd48f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3568, 12)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d50a2586",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Abstract'] = df['Abstract'].map(lambda x: str(x).lower().replace('natural langauge processing', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0e2fa055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = df['Abstract'].values.tolist()\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bebe6739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cf715a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dd9a7962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ']):#,  'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "30802b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dglover\\Anaconda3\\lib\\site-packages\\spacy\\language.py:1895: UserWarning: [W123] Argument disable with value ['parser', 'ner'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_trigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "#nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b41c0fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e832d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6284bd38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.029*\"llm\" + 0.024*\"research\" + 0.013*\"field\" + 0.012*\"knowledge\" + '\n",
      "  '0.012*\"attention\" + 0.011*\"challenge\" + 0.011*\"application\" + 0.010*\"large\" '\n",
      "  '+ 0.009*\"potential\" + 0.008*\"various\"'),\n",
      " (1,\n",
      "  '0.036*\"semantic\" + 0.020*\"structure\" + 0.016*\"representation\" + '\n",
      "  '0.014*\"entity\" + 0.013*\"sentence\" + 0.011*\"relation\" + 0.010*\"memory\" + '\n",
      "  '0.009*\"legal\" + 0.009*\"process\" + 0.008*\"property\"'),\n",
      " (2,\n",
      "  '0.020*\"performance\" + 0.015*\"propose\" + 0.014*\"dataset\" + 0.013*\"method\" + '\n",
      "  '0.012*\"pre_traine\" + 0.011*\"state\" + 0.011*\"training\" + 0.011*\"domain\" + '\n",
      "  '0.010*\"large\" + 0.010*\"art\"'),\n",
      " (3,\n",
      "  '0.014*\"system\" + 0.011*\"speech\" + 0.011*\"embedding\" + 0.010*\"result\" + '\n",
      "  '0.009*\"different\" + 0.009*\"feature\" + 0.009*\"method\" + 0.009*\"present\" + '\n",
      "  '0.009*\"word\" + 0.009*\"sentence\"'),\n",
      " (4,\n",
      "  '0.019*\"system\" + 0.018*\"process\" + 0.014*\"human\" + 0.013*\"generate\" + '\n",
      "  '0.013*\"user\" + 0.011*\"question\" + 0.010*\"image\" + 0.008*\"framework\" + '\n",
      "  '0.008*\"graph\" + 0.008*\"dialogue\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "91679646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.714437952072352\n",
      "\n",
      "Coherence Score:  0.3769663286712831\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "15c032eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dglover\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el294430294378446087115944284\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el294430294378446087115944284_data = {\"mdsDat\": {\"x\": [0.1793068277726953, 0.08239850983129139, -0.05649595976258866, 0.11222847060146522, -0.31743784844286305], \"y\": [-0.1574569536372742, -0.11294686510513229, -0.03334853660709021, 0.3073971380751986, -0.0036447827257018225], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [32.80239501923355, 27.37519285749067, 15.423789152913253, 13.99162130012646, 10.40700167023607]}, \"tinfo\": {\"Term\": [\"semantic\", \"llm\", \"performance\", \"research\", \"system\", \"process\", \"structure\", \"representation\", \"pre_traine\", \"sentence\", \"large\", \"state\", \"speech\", \"generate\", \"training\", \"user\", \"embedding\", \"art\", \"dataset\", \"entity\", \"field\", \"feature\", \"attention\", \"transformer\", \"question\", \"knowledge\", \"human\", \"result\", \"image\", \"word\", \"performance\", \"pre_traine\", \"training\", \"art\", \"transformer\", \"state\", \"fine_tune\", \"downstream\", \"inference\", \"parameter\", \"plm\", \"pretraine\", \"bias\", \"transfer\", \"improvement\", \"lm\", \"sample\", \"small\", \"significantly\", \"gpt\", \"experimental_result\", \"setting\", \"token\", \"size\", \"augmentation\", \"biomedical\", \"adversarial\", \"generative\", \"large_scale\", \"less\", \"architecture\", \"benchmark\", \"effectiveness\", \"layer\", \"improve\", \"demonstrate\", \"baseline\", \"train\", \"achieve\", \"outperform\", \"domain\", \"propose\", \"neural\", \"dataset\", \"large\", \"method\", \"compare\", \"specific\", \"result\", \"representation\", \"introduce\", \"evaluation\", \"work\", \"generation\", \"level\", \"knowledge\", \"experiment\", \"evaluate\", \"classification\", \"speech\", \"feature\", \"annotate\", \"similarity\", \"detection\", \"translation\", \"annotation\", \"emotion\", \"vector\", \"sentiment\", \"error\", \"detect\", \"encoder\", \"measure\", \"classifier\", \"arabic\", \"recognition\", \"style\", \"speaker\", \"speak\", \"identification\", \"parallel\", \"cluster\", \"characteristic\", \"social_media\", \"embedding\", \"character\", \"segmentation\", \"observe\", \"usually\", \"embed\", \"text\", \"word\", \"share\", \"test\", \"english\", \"prediction\", \"automatic\", \"resource\", \"different\", \"system\", \"sentence\", \"present\", \"result\", \"find\", \"accuracy\", \"many\", \"method\", \"classification\", \"set\", \"work\", \"dataset\", \"analysis\", \"technique\", \"problem\", \"develop\", \"score\", \"make\", \"propose\", \"user\", \"question\", \"dialogue\", \"visual\", \"answer\", \"query\", \"program\", \"description\", \"response\", \"object\", \"interaction\", \"action\", \"explanation\", \"facilitate\", \"student\", \"game\", \"temporal\", \"agent\", \"programming\", \"web\", \"requirement\", \"environment\", \"abstract\", \"conversation\", \"causal\", \"teacher\", \"feedback\", \"interface\", \"patient\", \"software\", \"risk\", \"image\", \"process\", \"search\", \"support\", \"graph\", \"retrieval\", \"generate\", \"human\", \"system\", \"concept\", \"code\", \"framework\", \"design\", \"reasoning\", \"document\", \"complex\", \"generation\", \"tool\", \"extract\", \"problem\", \"present\", \"llm\", \"attention\", \"clinical\", \"medical\", \"future\", \"single\", \"survey\", \"researcher\", \"review\", \"summarization\", \"chatgpt\", \"comprehensive\", \"technology\", \"ai\", \"instruction\", \"year\", \"direction\", \"privacy\", \"recommendation\", \"note\", \"span\", \"practitioner\", \"previously\", \"distinct\", \"last\", \"summary\", \"clinical_note\", \"maintain\", \"trend\", \"paper\", \"discuss\", \"research\", \"field\", \"potential\", \"area\", \"community\", \"development\", \"current\", \"literature\", \"application\", \"knowledge\", \"challenge\", \"various\", \"large\", \"document\", \"include\", \"aim\", \"study\", \"present\", \"analysis\", \"focus\", \"human\", \"work\", \"semantic\", \"structure\", \"entity\", \"relation\", \"memory\", \"legal\", \"syntactic\", \"parse\", \"event\", \"interpretation\", \"usage\", \"dependency\", \"tree\", \"hierarchical\", \"database\", \"computation\", \"grammar\", \"discourse\", \"parser\", \"protein\", \"combination\", \"distribute\", \"successfully\", \"constraint\", \"compositional\", \"recognize\", \"theory\", \"argument\", \"reference\", \"transformation\", \"aware\", \"element\", \"property\", \"mean\", \"meaning\", \"computational\", \"representation\", \"type\", \"sentence\", \"network\", \"space\", \"process\", \"represent\", \"order\", \"level\", \"linguistic\"], \"Freq\": [950.0, 1033.0, 1634.0, 905.0, 1696.0, 909.0, 532.0, 1121.0, 958.0, 1058.0, 1217.0, 911.0, 774.0, 854.0, 875.0, 511.0, 739.0, 812.0, 1693.0, 372.0, 526.0, 649.0, 412.0, 733.0, 434.0, 971.0, 940.0, 1432.0, 437.0, 647.0, 1634.173911527702, 957.656229289316, 875.1398882399967, 811.649847725024, 732.8909115532634, 910.5616057616226, 433.84800750725026, 376.7862245352745, 370.9408852316211, 357.6882565634637, 342.8069344077243, 333.33856186315364, 290.8335971989675, 283.15843010994234, 264.38825507137557, 263.9092219178465, 253.3963628309714, 251.31566054564067, 238.28730517883437, 234.05665670724116, 224.75827793945783, 217.28813050044138, 200.07344275863818, 196.9965707863199, 180.6070395306198, 180.88805436190108, 175.8632168154548, 172.22791201653902, 154.14082806066375, 150.88616275549478, 485.14477410065336, 422.83968907479175, 251.19842677447986, 310.0790931485967, 789.0840297880507, 729.7020760400219, 270.34188414231943, 749.3789614924558, 706.9284494625857, 384.5346170495349, 870.3426707834466, 1199.0176174500145, 368.505615704686, 1161.4644957200424, 858.3630142962513, 1111.3614555098684, 464.1230951544576, 390.5514119456621, 758.1854777506207, 611.7960620076748, 421.8809303287243, 476.47016747376335, 579.0633929098988, 435.98129668845525, 407.6342285250242, 440.1009241055546, 412.80193932039293, 401.5336869776035, 398.2773771297874, 773.9807510451624, 648.2200857430295, 295.2993316905529, 272.7877682180832, 258.30934366993176, 255.6701609795897, 251.75774528101692, 245.90357660934015, 232.46537342848248, 228.35042894279886, 229.60126933329002, 229.99865402487413, 221.95065089695245, 203.4166584172525, 204.6984792752973, 201.37891415089948, 171.6353538034035, 169.53831324319398, 154.83621935053887, 153.6670999757552, 143.52687190434216, 143.70172694737923, 139.9782278926606, 138.06881981184705, 131.8997740150243, 734.6774852216298, 127.79579308340237, 127.32785960689696, 122.02233414792228, 119.7652224880005, 406.68489952725076, 274.64868303854473, 627.5267386227869, 234.32514855298734, 406.98083522663853, 266.332064810083, 326.95726020408216, 308.7316598190144, 336.8468675294254, 652.1058389770393, 946.5390017405491, 625.6434472366416, 629.4509306469251, 672.5528833542029, 349.7689650145402, 329.86971309112516, 409.2113004740531, 646.4340698145915, 419.12835681469, 378.8825784870755, 473.04352002974616, 531.1288146980763, 361.29864297764925, 362.42989555297237, 354.3650859227042, 302.93619351513223, 280.79790454654426, 300.410787795785, 314.1324702807861, 510.2541531144678, 433.097342500781, 297.70756105196114, 278.91241857796155, 277.1899910124167, 271.1956269031706, 209.3536870024125, 202.2204919311536, 197.7903381951917, 194.56424134959414, 177.79117500716072, 164.4694574475857, 158.11551867674726, 151.02199095459406, 138.32330220149248, 137.54342185932822, 135.9037295672943, 129.52862764166625, 123.81532650737617, 125.08707836081761, 119.09952444625377, 118.40171915952365, 114.2901884762278, 108.67114477750076, 105.8499967591278, 106.37332359192567, 95.44278024516444, 89.1219337749137, 92.80354491714985, 88.81219201199269, 90.03125303645791, 392.42316987210404, 684.3505126178569, 173.28763757976162, 238.23415802365852, 306.0319523272122, 233.82449818032623, 522.3457306476043, 535.3229048119376, 746.9980090131073, 256.67801021631675, 273.22991254711536, 329.21790677506476, 239.83745692472087, 212.9766020443988, 231.861009620011, 189.6717359900616, 256.9622657526578, 198.12934661148844, 197.90532466954866, 211.0378580958376, 215.32685178053606, 1032.4553200828807, 411.5151992580511, 268.3293152621743, 248.62923301673067, 240.66925742288998, 249.86045226326, 213.2825996647101, 209.59481620366225, 200.44659548479103, 168.34896324063055, 156.48625153525182, 151.48722408901506, 144.2657010969043, 156.6063296647394, 140.06314705943058, 129.23822182803886, 130.26197074534693, 119.05986104244231, 106.93729455465987, 93.98700448079695, 90.44155741415199, 83.30650774021554, 83.08119305928373, 79.2873936383262, 74.62948456239793, 72.62749597786842, 73.3519602487172, 74.03526354828702, 68.8851046984365, 68.30066731164275, 232.31524572608194, 850.0186718925071, 449.12013171956164, 304.0059911799934, 206.8010880912603, 181.590620811803, 203.686965669433, 246.3074490366493, 136.62207509276791, 381.5041665405768, 437.4995945308688, 387.4694799894639, 283.807793086387, 358.075804411077, 224.33242936205176, 211.7786366810473, 182.01061531882192, 178.56823886851697, 231.38450074945928, 205.9729789030969, 179.83524527006304, 186.9318824130989, 182.06341819128588, 949.7786788772491, 532.2120502614825, 371.5000710764676, 289.8093205797371, 255.52711374191304, 226.22312912296644, 198.04095683049434, 180.54578124347935, 137.82929489615628, 136.90959332435364, 133.51709840150573, 127.46259979973351, 126.06222612877649, 128.42398061230938, 123.41607642752841, 115.69838694519144, 110.67990123720394, 109.1930812815041, 108.72659110849278, 106.78483888344556, 101.74249898040935, 96.82760113656902, 89.02424647385514, 84.01900292582046, 79.16317609964992, 78.8295407949949, 76.83662013160456, 71.64958674716667, 73.63800385569746, 70.87360837868621, 112.16834442541409, 90.0898620026312, 220.34727153062246, 171.293185067918, 95.73392687439981, 117.65731126153237, 411.1476105749977, 200.27669721125025, 342.87545197974646, 181.09965746742367, 140.32636350632245, 223.98970764579676, 139.67505671749012, 143.05037954904435, 124.43652825777407, 112.4159178951314], \"Total\": [950.0, 1033.0, 1634.0, 905.0, 1696.0, 909.0, 532.0, 1121.0, 958.0, 1058.0, 1217.0, 911.0, 774.0, 854.0, 875.0, 511.0, 739.0, 812.0, 1693.0, 372.0, 526.0, 649.0, 412.0, 733.0, 434.0, 971.0, 940.0, 1432.0, 437.0, 647.0, 1634.9859515207838, 958.4627729263871, 875.9486314051952, 812.4597943970541, 733.7004968710437, 911.730037240457, 434.654042387759, 377.59614457640146, 371.75238864982475, 358.49677477056684, 343.61902825893816, 334.14465512673394, 291.64415548170615, 283.96626237145847, 265.1999220424905, 264.7203565717406, 254.2060992590521, 252.12685983049883, 239.10015324164772, 234.87202124033988, 225.57111720879394, 218.10192012137225, 200.88111368309714, 197.8034534982116, 181.41477759314844, 181.69819205851704, 176.66841717713248, 173.04454597275986, 154.9572571160106, 151.70085163938103, 487.8897057616702, 426.04752362377644, 253.61998215108352, 316.34468019080504, 869.5573741374867, 803.2141913531361, 280.8548673295094, 885.7701526724725, 844.2256126166939, 422.43114171531295, 1099.9460136622035, 1636.2384819062206, 406.2625401655338, 1693.201282390507, 1217.0635991209008, 1798.046165687502, 566.4425479674583, 464.5848585824881, 1432.174638604411, 1121.8643416504403, 594.087023242592, 790.147769565691, 1274.283608067797, 693.5459587267367, 631.6789481529463, 971.1837188476043, 688.3777217950438, 624.8343690564196, 818.0116571232169, 774.7639572923972, 649.0060747339202, 296.0860210492265, 273.5661007507993, 259.09478019003865, 256.4508386342258, 252.54648640906882, 246.69611179011602, 233.2521425722747, 229.12924953656827, 230.38700462425373, 230.78865272697325, 222.7552450508212, 204.19730841574082, 205.49081410214927, 202.15926805890058, 172.4215924992419, 170.32811925886972, 155.61120718072905, 154.45040260598952, 144.30914347170858, 144.49246541337968, 140.75768932423154, 138.85727373478989, 132.67843202344787, 739.1182238736706, 128.57319709581623, 128.11151487940754, 122.8253798670742, 120.55750859708506, 409.62507625385376, 276.8284185868716, 647.7767787555823, 237.4094064770677, 420.9210585008877, 278.2638063201706, 363.93766443863177, 347.9445580871742, 391.35601291016513, 950.8240253581504, 1696.0238030950243, 1058.9732388183556, 1093.6019531194427, 1432.174638604411, 505.9075275263082, 460.78847702955795, 723.5688034136123, 1798.046165687502, 818.0116571232169, 688.6278120303639, 1274.283608067797, 1693.201282390507, 695.8168563230937, 783.154754557571, 833.6492574583159, 526.6227630318476, 392.9903204087987, 675.8308000382492, 1636.2384819062206, 511.0308721701901, 434.17027101071676, 298.4815905549742, 279.6853439644753, 277.96137934518845, 271.96542956557306, 210.12825627996745, 202.99211770119632, 198.56653934508057, 195.3340963376908, 178.56650405081106, 165.2432407441809, 158.88887398691793, 151.82549071790365, 139.10682038069834, 138.329121443249, 136.6883284196002, 130.2966923893725, 124.58495117905107, 125.88383108846783, 119.87232311714352, 119.17916982604285, 115.07458949470053, 109.44720039488304, 106.6433646500865, 107.18019705469514, 96.21629242360262, 89.88992822301998, 93.60751771104475, 89.58434250788815, 90.85179736334932, 437.225816381027, 909.1175699859502, 191.35278261710857, 278.47968375283375, 379.15396841658026, 274.52221637926664, 854.6950386945532, 940.8897801600242, 1696.0238030950243, 386.1403069954083, 433.27994311204776, 606.7902767030802, 401.75084937441255, 352.14680724891144, 456.7846457957293, 295.0505365441525, 693.5459587267367, 392.02999558526216, 443.4736204954523, 833.6492574583159, 1093.6019531194427, 1033.2557319913194, 412.32310771647263, 269.1201368742345, 249.42599482557878, 241.4605138539677, 250.68422100361107, 214.06985674245664, 210.39066116392905, 201.23848294562507, 169.14788996441553, 157.2833679706089, 152.2793419653288, 145.05998924613147, 157.48101558578324, 140.87101224446587, 130.04584177117084, 131.08175740585557, 119.86100328913871, 107.73533139056418, 94.79087295713802, 91.25816909819589, 84.11303366737425, 83.9163668155504, 80.10833633239551, 75.44011381884559, 73.42517718809135, 74.15995133338623, 74.87789924865716, 69.67474169916999, 69.0909289383965, 239.20403465504572, 905.4753247897432, 526.0531875920655, 347.5612172419951, 229.67993189816272, 209.38373231874684, 266.9056352760568, 374.5896383946051, 166.59039047949142, 763.9353190498487, 971.1837188476043, 848.4966757120994, 669.4471341047449, 1217.0635991209008, 456.7846457957293, 491.8498684643511, 353.94757727237527, 344.0238009341049, 1093.6019531194427, 695.8168563230937, 432.26673269718, 940.8897801600242, 1274.283608067797, 950.6150979523559, 532.9961555796646, 372.291308434136, 290.59101790830107, 256.3236099368242, 227.02903559429836, 198.8190903655674, 181.3269907171214, 138.6090212874717, 137.70177194982955, 134.31797838884947, 128.24905160631593, 126.8411098509268, 129.2204749013056, 124.21311724121664, 116.4913454800564, 111.45269809388417, 109.9719528530899, 109.50476828941206, 107.57575902318112, 102.5426026037791, 97.62173392365497, 89.83907882051538, 84.80497784672811, 79.950442094608, 79.61666359284435, 77.62019189424169, 72.43292402825308, 74.45424982811166, 71.6598629362902, 114.05515846350549, 94.31940119950413, 282.9262968693021, 229.407930791383, 104.55157983708835, 147.56908267624013, 1121.8643416504403, 384.61843142887847, 1058.9732388183556, 384.5915567533712, 235.73579882560364, 909.1175699859502, 261.67891000445064, 305.76257971026837, 631.6789481529463, 359.0249572021662], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.923, -4.4574, -4.5475, -4.6228, -4.7249, -4.5078, -5.2492, -5.3902, -5.4059, -5.4422, -5.4847, -5.5127, -5.6491, -5.6759, -5.7445, -5.7463, -5.7869, -5.7952, -5.8484, -5.8663, -5.9069, -5.9407, -6.0232, -6.0387, -6.1256, -6.124, -6.1522, -6.1731, -6.284, -6.3054, -5.1375, -5.2749, -5.7957, -5.5851, -4.651, -4.7293, -5.7222, -4.7027, -4.761, -5.3699, -4.553, -4.2326, -5.4124, -4.2645, -4.5669, -4.3086, -5.1818, -5.3543, -4.691, -4.9055, -5.2772, -5.1555, -4.9605, -5.2443, -5.3115, -5.2349, -5.2989, -5.3266, -5.3348, -4.4895, -4.6668, -5.453, -5.5323, -5.5869, -5.5971, -5.6126, -5.6361, -5.6923, -5.7102, -5.7047, -5.703, -5.7386, -5.8258, -5.8195, -5.8358, -5.9957, -6.008, -6.0987, -6.1062, -6.1745, -6.1733, -6.1995, -6.2133, -6.259, -4.5416, -6.2906, -6.2943, -6.3368, -6.3555, -5.133, -5.5255, -4.6992, -5.6843, -5.1323, -5.5563, -5.3512, -5.4086, -5.3214, -4.6608, -4.2882, -4.7023, -4.6962, -4.63, -5.2838, -5.3423, -5.1268, -4.6696, -5.1029, -5.2038, -4.9818, -4.866, -5.2513, -5.2482, -5.2707, -5.4275, -5.5034, -5.4359, -5.3912, -4.3324, -4.4963, -4.8712, -4.9364, -4.9426, -4.9645, -5.2233, -5.258, -5.2801, -5.2965, -5.3867, -5.4646, -5.504, -5.5499, -5.6377, -5.6434, -5.6554, -5.7034, -5.7485, -5.7383, -5.7874, -5.7932, -5.8286, -5.879, -5.9053, -5.9004, -6.0088, -6.0773, -6.0368, -6.0808, -6.0672, -4.595, -4.0388, -5.4124, -5.0941, -4.8436, -5.1127, -4.309, -4.2844, -3.9512, -5.0195, -4.957, -4.7706, -5.0873, -5.2061, -5.1212, -5.322, -5.0184, -5.2784, -5.2795, -5.2153, -5.1952, -3.5302, -4.45, -4.8776, -4.9539, -4.9864, -4.949, -5.1072, -5.1247, -5.1693, -5.3438, -5.4169, -5.4494, -5.4982, -5.4161, -5.5278, -5.6082, -5.6003, -5.6902, -5.7976, -5.9267, -5.9652, -6.0473, -6.05, -6.0968, -6.1573, -6.1845, -6.1746, -6.1653, -6.2374, -6.2459, -5.0218, -3.7246, -4.3626, -4.7528, -5.1381, -5.2681, -5.1533, -4.9633, -5.5526, -4.5257, -4.3888, -4.5102, -4.8216, -4.5891, -5.0567, -5.1143, -5.2658, -5.2849, -5.0258, -5.1421, -5.2778, -5.2391, -5.2655, -3.3176, -3.8968, -4.2563, -4.5047, -4.6305, -4.7524, -4.8854, -4.9779, -5.2479, -5.2546, -5.2796, -5.3261, -5.3371, -5.3185, -5.3583, -5.4229, -5.4672, -5.4808, -5.485, -5.5031, -5.5514, -5.6009, -5.685, -5.7428, -5.8024, -5.8066, -5.8322, -5.9021, -5.8747, -5.913, -5.4539, -5.6731, -4.7787, -5.0305, -5.6123, -5.4061, -4.1549, -4.8742, -4.3365, -4.9748, -5.2299, -4.7623, -5.2346, -5.2107, -5.3501, -5.4517], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.1142, 1.1138, 1.1137, 1.1137, 1.1136, 1.1134, 1.1128, 1.1125, 1.1125, 1.1124, 1.1123, 1.1123, 1.1119, 1.1118, 1.1116, 1.1116, 1.1115, 1.1114, 1.1113, 1.1112, 1.1111, 1.1109, 1.1106, 1.1106, 1.1102, 1.1102, 1.1101, 1.1099, 1.1094, 1.1093, 1.109, 1.1071, 1.1051, 1.0947, 1.0176, 1.0187, 1.0765, 0.9475, 0.9372, 1.0207, 0.8805, 0.8038, 1.0171, 0.7377, 0.7655, 0.6336, 0.9154, 0.9411, 0.4786, 0.5083, 0.7724, 0.6089, 0.3259, 0.6505, 0.6767, 0.3232, 0.6033, 0.6725, 0.3949, 1.2945, 1.2943, 1.2929, 1.2927, 1.2925, 1.2925, 1.2924, 1.2923, 1.2922, 1.2921, 1.2921, 1.2921, 1.2919, 1.2917, 1.2917, 1.2917, 1.291, 1.2909, 1.2905, 1.2904, 1.2901, 1.29, 1.29, 1.2898, 1.2896, 1.2895, 1.2895, 1.2894, 1.289, 1.2889, 1.2883, 1.2876, 1.2638, 1.2825, 1.2619, 1.2517, 1.1884, 1.176, 1.1455, 0.9184, 0.7123, 0.7693, 0.7431, 0.5397, 0.9265, 0.9613, 0.7256, 0.2725, 0.6268, 0.6981, 0.3046, 0.1362, 0.6402, 0.525, 0.44, 0.7426, 0.9594, 0.4847, -0.3548, 1.8677, 1.8668, 1.8667, 1.8665, 1.8665, 1.8664, 1.8656, 1.8655, 1.8653, 1.8653, 1.8649, 1.8646, 1.8644, 1.864, 1.8636, 1.8636, 1.8635, 1.8633, 1.8631, 1.8629, 1.8628, 1.8627, 1.8624, 1.8621, 1.8618, 1.8617, 1.8612, 1.8607, 1.8606, 1.8606, 1.8602, 1.7612, 1.5853, 1.7701, 1.7132, 1.655, 1.7088, 1.3768, 1.3053, 1.0493, 1.4609, 1.4082, 1.2578, 1.3534, 1.3664, 1.1912, 1.4274, 0.8764, 1.1868, 1.0624, 0.4955, 0.2442, 1.9659, 1.9648, 1.9638, 1.9635, 1.9634, 1.9634, 1.963, 1.9629, 1.9628, 1.962, 1.9616, 1.9615, 1.9612, 1.9611, 1.961, 1.9605, 1.9604, 1.96, 1.9593, 1.9582, 1.9577, 1.9571, 1.9567, 1.9564, 1.9559, 1.9558, 1.9558, 1.9554, 1.9553, 1.9552, 1.9375, 1.9035, 1.8086, 1.8328, 1.8618, 1.8243, 1.6964, 1.5475, 1.7684, 1.2724, 1.1693, 1.1829, 1.1086, 0.7433, 1.2556, 1.1241, 1.3016, 1.311, 0.4136, 0.7494, 1.0897, 0.3506, 0.0209, 2.2618, 2.2612, 2.2606, 2.26, 2.2596, 2.2591, 2.2588, 2.2584, 2.2571, 2.2569, 2.2567, 2.2565, 2.2565, 2.2565, 2.2563, 2.2559, 2.2557, 2.2556, 2.2556, 2.2553, 2.2549, 2.2545, 2.2536, 2.2534, 2.2528, 2.2528, 2.2525, 2.2518, 2.2517, 2.2517, 2.246, 2.2168, 2.0127, 1.9706, 2.1746, 2.0362, 1.2589, 1.6101, 1.135, 1.5096, 1.744, 0.8618, 1.6349, 1.5031, 0.6381, 1.1015]}, \"token.table\": {\"Topic\": [3, 1, 2, 1, 2, 3, 1, 3, 4, 1, 2, 4, 2, 3, 4, 5, 2, 2, 3, 1, 2, 3, 4, 5, 2, 1, 4, 2, 4, 5, 1, 4, 1, 2, 3, 2, 5, 1, 2, 1, 4, 1, 1, 3, 1, 2, 3, 4, 2, 2, 4, 1, 2, 2, 4, 4, 2, 1, 3, 5, 2, 4, 1, 2, 2, 3, 4, 5, 5, 4, 5, 1, 5, 2, 3, 5, 3, 1, 2, 4, 5, 1, 2, 1, 3, 5, 5, 3, 1, 3, 4, 2, 2, 1, 2, 3, 4, 5, 3, 4, 3, 1, 2, 3, 4, 4, 5, 4, 5, 4, 5, 3, 4, 1, 3, 4, 1, 1, 3, 4, 5, 1, 2, 1, 2, 2, 2, 1, 2, 5, 3, 2, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 1, 3, 2, 3, 3, 2, 3, 1, 2, 4, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 1, 3, 5, 4, 3, 1, 2, 3, 1, 3, 1, 1, 5, 3, 5, 5, 2, 3, 4, 5, 2, 1, 3, 1, 2, 1, 1, 2, 3, 4, 1, 4, 3, 3, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 4, 1, 4, 1, 5, 5, 1, 1, 3, 5, 2, 5, 1, 2, 4, 4, 1, 4, 1, 2, 3, 4, 5, 1, 2, 4, 5, 2, 5, 2, 5, 2, 4, 5, 1, 2, 4, 1, 2, 5, 1, 2, 4, 3, 2, 1, 2, 5, 1, 2, 4, 2, 1, 5, 5, 3, 1, 1, 1, 3, 4, 4, 1, 2, 3, 1, 2, 3, 4, 5, 1, 4, 4, 1, 2, 3, 4, 5, 3, 5, 3, 3, 2, 5, 1, 2, 3, 5, 3, 3, 3, 4, 2, 5, 4, 5, 5, 2, 4, 5, 1, 2, 5, 3, 2, 4, 4, 1, 2, 3, 1, 2, 3, 2, 3, 4, 3, 1, 1, 2, 1, 3, 2, 5, 1, 2, 5, 2, 1, 2, 3, 1, 1, 2, 1, 2, 4, 1, 1, 2, 3, 1, 2, 5, 4, 2, 2, 1, 3, 5, 2, 1, 5, 3, 1, 2, 4, 2, 5, 4, 4, 2, 3, 4, 5, 2, 3, 4, 3, 1, 2, 3, 4, 4, 3, 1, 2, 1, 2, 5, 1, 2, 3, 1, 2, 1, 1, 5, 1, 2, 5, 4, 2, 4, 5, 5, 3, 2, 1, 2, 4, 2, 3, 3, 2, 5, 1, 2, 3, 4, 5, 4], \"Freq\": [0.9906618003208256, 0.28212511050197325, 0.7161637420434706, 0.8374538623729262, 0.16227889553761085, 0.9924762989482541, 0.996216544033095, 0.9977229476517647, 0.996945564619367, 0.059330820009652875, 0.42379157149752056, 0.5142004400836583, 0.5188146805003158, 0.15952473555549876, 0.2960549146345292, 0.024431716256247557, 0.9963320759103115, 0.9978360957745275, 0.9965413204256892, 0.0863947488147139, 0.30107260950582115, 0.04319737440735695, 0.5000423340487986, 0.0706866126665841, 0.9942655705571568, 0.9940771331562347, 0.00409928714703602, 0.09578546901413455, 0.9012541857239024, 0.9940231043539783, 0.9994340711993074, 0.9992163725233298, 0.9977136504608315, 0.8880725185033157, 0.11208682272371946, 0.008767687612480696, 0.981981012597838, 0.961350617018953, 0.035605578408109365, 0.9928469866510301, 0.007041467990432837, 0.9977912964494619, 0.9961574077837154, 0.9939671384881987, 0.2463179950877204, 0.19681868507009237, 0.10017717503567576, 0.45610078516242963, 0.9955418616884119, 0.9938262237783292, 0.9918404088927653, 0.48654563359119635, 0.5122176393836967, 0.9976115034421671, 0.9958377812703106, 0.9843587905260123, 0.9946170661946132, 0.36696829042668616, 0.6300776307326121, 0.9947085153877389, 0.12894984582134417, 0.8692174792401717, 0.8191475051882163, 0.18007121881292687, 0.2101334934895876, 0.6439574800487362, 0.10845599663978714, 0.03728174884492683, 0.9881121095805411, 0.991598716222322, 0.9957821288952275, 0.19651812882529524, 0.7996254897029255, 0.3340754582285396, 0.6655611842227495, 0.9905078939094474, 0.9959140079118557, 0.31234179488101144, 0.02936546789479595, 0.6567186456472548, 0.9902335818618833, 0.6856833927983264, 0.31360713314031985, 0.9088484838274636, 0.08963985045969504, 0.00124499792305132, 0.990260734167843, 0.9951125309079404, 0.33105094913203376, 0.5973851713660759, 0.07218404154006751, 0.9965827924481788, 0.9957745957319725, 0.011393354828524853, 0.575364418840505, 0.18988924714208086, 0.20508038691344732, 0.017090032242787278, 0.23229183578636045, 0.7643150725873796, 0.9983865317988999, 0.27765390120486083, 0.6857209984301866, 0.001051719322745685, 0.035758456973353295, 0.9917474603082546, 0.9911618114630708, 0.9698833062517754, 0.025083188954787294, 0.9861645318934517, 0.9936311936014044, 0.5078979824198133, 0.49038425888809567, 0.7909479094372893, 0.13909773579759227, 0.06909430013475171, 0.9984212111671049, 0.9896696540672305, 0.00788581397663132, 0.04240909027337028, 0.9542045311508314, 0.0048825135860592565, 0.9935915147630587, 0.005411854113184031, 0.9944281932975658, 0.997178262012057, 0.9966095296627072, 0.03953083279304887, 0.9559274111773637, 0.9992175255571739, 0.9901059066969169, 0.9983201976826561, 0.6433704993005935, 0.2768733740771211, 0.08002120638067083, 0.6024189630524883, 0.244258108968761, 0.010124688454663668, 0.14174563836529136, 0.9956061930037828, 0.5999613103733851, 0.3994899766408738, 0.9974681279418176, 0.99440568766954, 0.5524567610724715, 0.4464752599687729, 0.9945628977452974, 0.9984498223158657, 0.9873587685311365, 0.08934457790311261, 0.0551275055146865, 0.8535258612446289, 0.2846369981962993, 0.6918260372826718, 0.019766458208076338, 0.0019766458208076336, 0.0019766458208076336, 0.9984952575520384, 0.30074023783613757, 0.2683528276076304, 0.013880318669360195, 0.4164095600808058, 0.41200396512340975, 0.5421972181024073, 0.04449642823332826, 0.9980927985010161, 0.997620736401597, 0.38376261139994644, 0.00468003184634081, 0.6107441559474758, 0.6286533639391991, 0.37055943700085825, 0.9939637162969338, 0.996287249389115, 0.9959382042639935, 0.8070599953837084, 0.1925339204673553, 0.9905550966111387, 0.20193651159404163, 0.5686107036990119, 0.19874804035834623, 0.030821888611722144, 0.9978577693396872, 0.10063449675546043, 0.896561880185011, 0.907358184136623, 0.09200082982373871, 0.9954754057495603, 0.30497110931091337, 0.250076309634949, 0.014231985101175959, 0.4310258344927576, 0.9979761027156884, 0.993816951900975, 0.9968274898261442, 0.9900998004936433, 0.9949036825024645, 0.7103336438770836, 0.07237996845193032, 0.11782785561942145, 0.031981846525271536, 0.06733020321109798, 0.4530553709468061, 0.015445069464095663, 0.44996635705398697, 0.08031436121329745, 0.7049754841240371, 0.29415060992588027, 0.9938224441124823, 0.9941660504396582, 0.9799437746606701, 0.018966653703109744, 0.9954673833168315, 0.9953800414974131, 0.6458977320567795, 0.1567251849843656, 0.19630225189960945, 0.6851891353655337, 0.31195602910951126, 0.036016483200083785, 0.13806318560032116, 0.822376366401913, 0.9987846842243988, 0.9972788017473626, 0.9882755892263777, 0.20419312051506053, 0.44389808807621856, 0.10653554113829246, 0.21603040286375969, 0.02811354557816051, 0.3192509114685448, 0.5652537783144364, 0.09674270044501357, 0.017966501511216804, 0.25282473801110017, 0.7453970724120367, 0.07651725600383612, 0.9182070720460334, 0.9941365122536134, 0.9982920993223795, 0.9987374946189937, 0.6178929224407302, 0.35927887299433997, 0.02224636984485077, 0.2808173973233054, 0.24701530320105566, 0.47062915662516924, 0.9082796554406641, 0.09107411179215331, 0.9916566549872831, 0.998289615873753, 0.9932800544320118, 0.07849227339310681, 0.4513305720103642, 0.46768312896726144, 0.9113911404274765, 0.08758823946965358, 0.9842102435853887, 0.9965917571413099, 0.9986142838498763, 0.998196679292872, 0.9953904446601084, 0.993509947428367, 0.99939696636545, 0.9981985041338524, 0.054666628661191916, 0.0690525835720319, 0.8746660585790706, 0.9867674054918082, 0.9995171717259564, 0.8985055188074377, 0.0989180387677913, 0.012801732806041292, 0.5751635667857123, 0.1965980395213484, 0.2112285912996813, 0.00274322845843742, 0.9965743724785908, 0.9890799989284023, 0.9928166520760574, 0.23271177688261827, 0.4246390155486952, 0.2531040459908889, 0.07916998594975673, 0.009596361933303845, 0.7523779350239274, 0.2463927740429236, 0.9946306303590879, 0.9953048006720299, 0.2191383433991677, 0.777587670126079, 0.7327782675072908, 0.1919035663029936, 0.0751724160995803, 0.994647873941033, 0.9964501754244457, 0.9973045805094106, 0.6048613692227602, 0.3947217386007684, 0.9975548741133233, 0.9922545913755199, 0.9931746495687803, 0.993898940232957, 0.9979661521799426, 0.4471128376299414, 0.01910738622350177, 0.5350068142580496, 0.5455205030401902, 0.0882459637270896, 0.3663544554730689, 0.9927228980430198, 0.06074157792513156, 0.9387334770247605, 0.9981431630008298, 0.13798178185241164, 0.8611085274863467, 0.9971468539112927, 0.5292650627710015, 0.4699147588982639, 0.000698238869091031, 0.14570769727699537, 0.8523900290704228, 0.99384569527907, 0.9906243201778092, 0.9952554275347146, 0.28499429676409, 0.7150303338456186, 0.0888411433975147, 0.9040892828100024, 0.9913238487543152, 0.999352947419328, 0.08498798336058574, 0.5911386398191852, 0.32389864769645454, 0.9950715609689629, 0.3122150982634433, 0.5503698708923023, 0.13650334528727287, 0.9949476826212303, 0.008424266037635656, 0.9856391264033717, 0.995398776509625, 0.9979306619159112, 0.9972705860748962, 0.9959381219892661, 0.995530583963738, 0.9948866442487957, 0.9934771803695863, 0.19937574281948753, 0.20361777990075322, 0.5938851913771969, 0.9862130797644858, 0.9970838366337023, 0.9960722161867224, 0.8416115867249623, 0.15497706967825392, 0.00430491860217372, 0.9990139483319964, 0.9991992835481579, 0.9981310267077232, 0.9920433780481125, 0.3488130753574956, 0.13371167888704, 0.5203128374082643, 0.9980736048733619, 0.9906602023136086, 0.9932136903117325, 0.9942093815177022, 0.14363704906926794, 0.8546404419621443, 0.9950023008435804, 0.9958802227489254, 0.5583648049466331, 0.4404419316738489, 0.0011792287327278418, 0.9889886650041063, 0.22473208388989224, 0.46223303618261924, 0.12896557086863134, 0.18259481816053744, 0.9926927524837127, 0.9949642487580418, 0.030884651022924727, 0.9669271512561819, 0.007224691779151206, 0.9933951196332909, 0.992009915472939, 0.995613755484813, 0.49230926759027716, 0.5050633936936522, 0.8455918250803316, 0.15353870255130186, 0.998917023931331, 0.9965972634798619, 0.9907917359976414, 0.9990452550134135, 0.9982420075651659, 0.993368791459525, 0.990315834939392, 0.3145975078481767, 0.16379870243334818, 0.519995880740788, 0.9976326446194052, 0.9979827595037217, 0.9953755796418429, 0.46605621878901493, 0.10904520503717335, 0.4242306606925649, 0.9946318067715639, 0.997549589282153, 0.992978994356736, 0.9694697627266375, 0.0308748332078547, 0.4543729483250128, 0.3711889543311417, 0.023542639809586152, 0.142825348178156, 0.007847546603195385, 0.9919578991767295], \"Term\": [\"abstract\", \"accuracy\", \"accuracy\", \"achieve\", \"achieve\", \"action\", \"adversarial\", \"agent\", \"ai\", \"aim\", \"aim\", \"aim\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"annotate\", \"annotation\", \"answer\", \"application\", \"application\", \"application\", \"application\", \"application\", \"arabic\", \"architecture\", \"architecture\", \"area\", \"area\", \"argument\", \"art\", \"attention\", \"augmentation\", \"automatic\", \"automatic\", \"aware\", \"aware\", \"baseline\", \"baseline\", \"benchmark\", \"benchmark\", \"bias\", \"biomedical\", \"causal\", \"challenge\", \"challenge\", \"challenge\", \"challenge\", \"character\", \"characteristic\", \"chatgpt\", \"classification\", \"classification\", \"classifier\", \"clinical\", \"clinical_note\", \"cluster\", \"code\", \"code\", \"combination\", \"community\", \"community\", \"compare\", \"compare\", \"complex\", \"complex\", \"complex\", \"complex\", \"compositional\", \"comprehensive\", \"computation\", \"computational\", \"computational\", \"concept\", \"concept\", \"constraint\", \"conversation\", \"current\", \"current\", \"current\", \"database\", \"dataset\", \"dataset\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"dependency\", \"description\", \"design\", \"design\", \"design\", \"detect\", \"detection\", \"develop\", \"develop\", \"develop\", \"develop\", \"develop\", \"development\", \"development\", \"dialogue\", \"different\", \"different\", \"different\", \"different\", \"direction\", \"discourse\", \"discuss\", \"discuss\", \"distinct\", \"distribute\", \"document\", \"document\", \"domain\", \"domain\", \"domain\", \"downstream\", \"effectiveness\", \"effectiveness\", \"element\", \"element\", \"embed\", \"embed\", \"embedding\", \"embedding\", \"emotion\", \"encoder\", \"english\", \"english\", \"entity\", \"environment\", \"error\", \"evaluate\", \"evaluate\", \"evaluate\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"event\", \"experiment\", \"experiment\", \"experimental_result\", \"explanation\", \"extract\", \"extract\", \"facilitate\", \"feature\", \"feedback\", \"field\", \"field\", \"field\", \"find\", \"find\", \"find\", \"find\", \"find\", \"fine_tune\", \"focus\", \"focus\", \"focus\", \"focus\", \"framework\", \"framework\", \"framework\", \"future\", \"game\", \"generate\", \"generate\", \"generate\", \"generation\", \"generation\", \"generative\", \"gpt\", \"grammar\", \"graph\", \"graph\", \"hierarchical\", \"human\", \"human\", \"human\", \"human\", \"identification\", \"image\", \"image\", \"improve\", \"improve\", \"improvement\", \"include\", \"include\", \"include\", \"include\", \"inference\", \"instruction\", \"interaction\", \"interface\", \"interpretation\", \"introduce\", \"introduce\", \"introduce\", \"introduce\", \"introduce\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"large\", \"large\", \"large_scale\", \"last\", \"layer\", \"layer\", \"legal\", \"less\", \"level\", \"level\", \"level\", \"linguistic\", \"linguistic\", \"literature\", \"literature\", \"literature\", \"llm\", \"lm\", \"maintain\", \"make\", \"make\", \"make\", \"make\", \"make\", \"many\", \"many\", \"many\", \"many\", \"mean\", \"mean\", \"meaning\", \"meaning\", \"measure\", \"medical\", \"memory\", \"method\", \"method\", \"method\", \"network\", \"network\", \"network\", \"neural\", \"neural\", \"note\", \"object\", \"observe\", \"order\", \"order\", \"order\", \"outperform\", \"outperform\", \"paper\", \"parallel\", \"parameter\", \"parse\", \"parser\", \"patient\", \"performance\", \"plm\", \"potential\", \"potential\", \"potential\", \"practitioner\", \"pre_traine\", \"prediction\", \"prediction\", \"present\", \"present\", \"present\", \"present\", \"present\", \"pretraine\", \"previously\", \"privacy\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"process\", \"process\", \"program\", \"programming\", \"property\", \"property\", \"propose\", \"propose\", \"propose\", \"protein\", \"query\", \"question\", \"reasoning\", \"reasoning\", \"recognition\", \"recognize\", \"recommendation\", \"reference\", \"relation\", \"represent\", \"represent\", \"represent\", \"representation\", \"representation\", \"representation\", \"requirement\", \"research\", \"research\", \"researcher\", \"resource\", \"resource\", \"response\", \"result\", \"result\", \"result\", \"retrieval\", \"retrieval\", \"review\", \"risk\", \"sample\", \"score\", \"score\", \"search\", \"search\", \"segmentation\", \"semantic\", \"sentence\", \"sentence\", \"sentence\", \"sentiment\", \"set\", \"set\", \"set\", \"setting\", \"share\", \"share\", \"significantly\", \"similarity\", \"single\", \"size\", \"small\", \"social_media\", \"software\", \"space\", \"space\", \"space\", \"span\", \"speak\", \"speaker\", \"specific\", \"specific\", \"specific\", \"speech\", \"state\", \"structure\", \"student\", \"study\", \"study\", \"study\", \"style\", \"successfully\", \"summarization\", \"summary\", \"support\", \"support\", \"survey\", \"syntactic\", \"system\", \"system\", \"system\", \"teacher\", \"technique\", \"technique\", \"technique\", \"technique\", \"technology\", \"temporal\", \"test\", \"test\", \"text\", \"text\", \"theory\", \"token\", \"tool\", \"tool\", \"train\", \"train\", \"training\", \"transfer\", \"transformation\", \"transformer\", \"translation\", \"tree\", \"trend\", \"type\", \"type\", \"type\", \"usage\", \"user\", \"usually\", \"various\", \"various\", \"various\", \"vector\", \"visual\", \"web\", \"word\", \"word\", \"work\", \"work\", \"work\", \"work\", \"work\", \"year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 4, 5, 1, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el294430294378446087115944284\", ldavis_el294430294378446087115944284_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el294430294378446087115944284\", ldavis_el294430294378446087115944284_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el294430294378446087115944284\", ldavis_el294430294378446087115944284_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2      0.179307 -0.157457       1        1  32.802395\n",
       "3      0.082399 -0.112947       2        1  27.375193\n",
       "4     -0.056496 -0.033349       3        1  15.423789\n",
       "0      0.112228  0.307397       4        1  13.991621\n",
       "1     -0.317438 -0.003645       5        1  10.407002, topic_info=             Term         Freq        Total Category  logprob  loglift\n",
       "255      semantic   950.000000   950.000000  Default  30.0000  30.0000\n",
       "1885          llm  1033.000000  1033.000000  Default  29.0000  29.0000\n",
       "24    performance  1634.000000  1634.000000  Default  28.0000  28.0000\n",
       "33       research   905.000000   905.000000  Default  27.0000  27.0000\n",
       "548        system  1696.000000  1696.000000  Default  26.0000  26.0000\n",
       "...           ...          ...          ...      ...      ...      ...\n",
       "27        process   223.989708   909.117570   Topic5  -4.7623   0.8618\n",
       "307     represent   139.675057   261.678910   Topic5  -5.2346   1.6349\n",
       "162         order   143.050380   305.762580   Topic5  -5.2107   1.5031\n",
       "1034        level   124.436528   631.678948   Topic5  -5.3501   0.6381\n",
       "301    linguistic   112.415918   359.024957   Topic5  -5.4517   1.1015\n",
       "\n",
       "[299 rows x 6 columns], token_table=      Topic      Freq      Term\n",
       "term                           \n",
       "1487      3  0.990662  abstract\n",
       "980       1  0.282125  accuracy\n",
       "980       2  0.716164  accuracy\n",
       "218       1  0.837454   achieve\n",
       "218       2  0.162279   achieve\n",
       "...     ...       ...       ...\n",
       "107       2  0.371189      work\n",
       "107       3  0.023543      work\n",
       "107       4  0.142825      work\n",
       "107       5  0.007848      work\n",
       "205       4  0.991958      year\n",
       "\n",
       "[419 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 4, 5, 1, 2])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "84f0b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1e031b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dglover\\AppData\\Local\\Temp\\ipykernel_2944\\1533496723.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "199dffc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.4399</td>\n",
       "      <td>system, speech, embedding, result, different, ...</td>\n",
       "      <td>[analytic, relate, technology, widely, many, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.3579</td>\n",
       "      <td>llm, research, field, knowledge, attention, ch...</td>\n",
       "      <td>[aim, investigate, interaction, agent, human, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.3794</td>\n",
       "      <td>llm, research, field, knowledge, attention, ch...</td>\n",
       "      <td>[technical, note, describe, set, baseline, too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.6887</td>\n",
       "      <td>system, speech, embedding, result, different, ...</td>\n",
       "      <td>[little, work, tangkhul, current, work, humble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.4473</td>\n",
       "      <td>llm, research, field, knowledge, attention, ch...</td>\n",
       "      <td>[note, course, understanding, distribute, repr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3563</th>\n",
       "      <td>1</td>\n",
       "      <td>0.2932</td>\n",
       "      <td>semantic, structure, representation, entity, s...</td>\n",
       "      <td>[large, document, write, juridical, difficult,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3564</th>\n",
       "      <td>2</td>\n",
       "      <td>0.8751</td>\n",
       "      <td>performance, propose, dataset, method, pre_tra...</td>\n",
       "      <td>[promise, performance, various, current, syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3565</th>\n",
       "      <td>2</td>\n",
       "      <td>0.4089</td>\n",
       "      <td>performance, propose, dataset, method, pre_tra...</td>\n",
       "      <td>[recent_advancement, speech, ser, state, art, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3566</th>\n",
       "      <td>0</td>\n",
       "      <td>0.3257</td>\n",
       "      <td>llm, research, field, knowledge, attention, ch...</td>\n",
       "      <td>[recent_advance, large, lead, renew, interest,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3567</th>\n",
       "      <td>2</td>\n",
       "      <td>0.4242</td>\n",
       "      <td>performance, propose, dataset, method, pre_tra...</td>\n",
       "      <td>[pre_traine, czech, often, evaluate, purely, l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3568 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dominant_Topic  Perc_Contribution  \\\n",
       "0                  3             0.4399   \n",
       "1                  0             0.3579   \n",
       "2                  0             0.3794   \n",
       "3                  3             0.6887   \n",
       "4                  0             0.4473   \n",
       "...              ...                ...   \n",
       "3563               1             0.2932   \n",
       "3564               2             0.8751   \n",
       "3565               2             0.4089   \n",
       "3566               0             0.3257   \n",
       "3567               2             0.4242   \n",
       "\n",
       "                                         Topic_Keywords  \\\n",
       "0     system, speech, embedding, result, different, ...   \n",
       "1     llm, research, field, knowledge, attention, ch...   \n",
       "2     llm, research, field, knowledge, attention, ch...   \n",
       "3     system, speech, embedding, result, different, ...   \n",
       "4     llm, research, field, knowledge, attention, ch...   \n",
       "...                                                 ...   \n",
       "3563  semantic, structure, representation, entity, s...   \n",
       "3564  performance, propose, dataset, method, pre_tra...   \n",
       "3565  performance, propose, dataset, method, pre_tra...   \n",
       "3566  llm, research, field, knowledge, attention, ch...   \n",
       "3567  performance, propose, dataset, method, pre_tra...   \n",
       "\n",
       "                                                      0  \n",
       "0     [analytic, relate, technology, widely, many, r...  \n",
       "1     [aim, investigate, interaction, agent, human, ...  \n",
       "2     [technical, note, describe, set, baseline, too...  \n",
       "3     [little, work, tangkhul, current, work, humble...  \n",
       "4     [note, course, understanding, distribute, repr...  \n",
       "...                                                 ...  \n",
       "3563  [large, document, write, juridical, difficult,...  \n",
       "3564  [promise, performance, various, current, syste...  \n",
       "3565  [recent_advancement, speech, ser, state, art, ...  \n",
       "3566  [recent_advance, large, lead, renew, interest,...  \n",
       "3567  [pre_traine, czech, often, evaluate, purely, l...  \n",
       "\n",
       "[3568 rows x 4 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_sents_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3725cf6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PDF URL</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Journal Ref</th>\n",
       "      <th>Primary Category</th>\n",
       "      <th>Category</th>\n",
       "      <th>Entry ID</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/pdf/1608.04434v1</td>\n",
       "      <td>Hadoop is one of the platform s that can proce...</td>\n",
       "      <td>Natural Language Processing using Hadoop and K...</td>\n",
       "      <td>[arxiv.Result.Author('Emre Erturk'), arxiv.Res...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-15 23:09:21+00:00</td>\n",
       "      <td>natural language processing, as a data analyti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1608.04434v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/pdf/2202.07138v2</td>\n",
       "      <td>Integrating AI Planning with Natural Language ...</td>\n",
       "      <td>Integrating AI Planning with Natural Language ...</td>\n",
       "      <td>[arxiv.Result.Author('Kebing Jin'), arxiv.Resu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-15 02:19:09+00:00</td>\n",
       "      <td>natural language processing (nlp) aims at inve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>['cs.AI', 'cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/2202.07138v2</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/pdf/1906.11608v2</td>\n",
       "      <td>The tools are machine learning based using nat...</td>\n",
       "      <td>Simple Natural Language Processing Tools for D...</td>\n",
       "      <td>[arxiv.Result.Author('Leon Derczynski')]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-06-27 13:15:12+00:00</td>\n",
       "      <td>this technical note describes a set of baselin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1906.11608v2</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/pdf/2006.16212v1</td>\n",
       "      <td>The name Tangkhul also known as Hao or Ihao re...</td>\n",
       "      <td>Towards the Study of Morphological Processing ...</td>\n",
       "      <td>[arxiv.Result.Author('Mirinso Shadang'), arxiv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-06-29 17:24:09+00:00</td>\n",
       "      <td>there is no or little work on natural language...</td>\n",
       "      <td>In proceeding of Regional International Confer...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/2006.16212v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/pdf/1511.07916v1</td>\n",
       "      <td>This is a lecture note for the course DS GA at...</td>\n",
       "      <td>Natural Language Understanding with Distribute...</td>\n",
       "      <td>[arxiv.Result.Author('Kyunghyun Cho')]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-11-24 23:23:13+00:00</td>\n",
       "      <td>this is a lecture note for the course ds-ga 30...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL', 'stat.ML']</td>\n",
       "      <td>http://arxiv.org/abs/1511.07916v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3563</th>\n",
       "      <td>http://arxiv.org/pdf/2307.01211v1</td>\n",
       "      <td>An automated method for the ontologicalreprese...</td>\n",
       "      <td>An automated method for the ontological repres...</td>\n",
       "      <td>[arxiv.Result.Author('Giampaolo Bella'), arxiv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-06-30 09:04:47+00:00</td>\n",
       "      <td>large documents written in juridical language ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>['cs.AI', 'cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/2307.01211v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3564</th>\n",
       "      <td>http://arxiv.org/pdf/2307.01488v1</td>\n",
       "      <td>SCAT modifies ran driven augmentations of the ...</td>\n",
       "      <td>SCAT: Robust Self-supervised Contrastive Learn...</td>\n",
       "      <td>[arxiv.Result.Author('Junjie Wu'), arxiv.Resul...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-04 05:41:31+00:00</td>\n",
       "      <td>despite their promising performance across var...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/2307.01488v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3565</th>\n",
       "      <td>http://arxiv.org/pdf/2307.06090v1</td>\n",
       "      <td>Large language models LLMs have revolutionised...</td>\n",
       "      <td>Can Large Language Models Aid in Annotating Sp...</td>\n",
       "      <td>[arxiv.Result.Author('Siddique Latif'), arxiv....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-12 11:27:40+00:00</td>\n",
       "      <td>despite recent advancements in speech emotion ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>['cs.SD', 'eess.AS']</td>\n",
       "      <td>http://arxiv.org/abs/2307.06090v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3566</th>\n",
       "      <td>http://arxiv.org/pdf/2307.07051v1</td>\n",
       "      <td>Predictive Power Varies with Clinical Note Typ...</td>\n",
       "      <td>Making the Most Out of the Limited Context Len...</td>\n",
       "      <td>[arxiv.Result.Author('Hongyi Zheng'), arxiv.Re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-13 20:04:05+00:00</td>\n",
       "      <td>recent advances in large language models have ...</td>\n",
       "      <td>Association for Computational Linguistics - St...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL', 'cs.IR', 'cs.LG']</td>\n",
       "      <td>http://arxiv.org/abs/2307.07051v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3567</th>\n",
       "      <td>http://arxiv.org/pdf/2307.10666v1</td>\n",
       "      <td>A Dataset and Strong Baselines for Classificat...</td>\n",
       "      <td>A Dataset and Strong Baselines for Classificat...</td>\n",
       "      <td>[arxiv.Result.Author('Hynek Kydlíček'), arxiv....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-20 07:47:08+00:00</td>\n",
       "      <td>pre-trained models for czech natural language ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/2307.10666v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3568 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                PDF URL  \\\n",
       "0     http://arxiv.org/pdf/1608.04434v1   \n",
       "1     http://arxiv.org/pdf/2202.07138v2   \n",
       "2     http://arxiv.org/pdf/1906.11608v2   \n",
       "3     http://arxiv.org/pdf/2006.16212v1   \n",
       "4     http://arxiv.org/pdf/1511.07916v1   \n",
       "...                                 ...   \n",
       "3563  http://arxiv.org/pdf/2307.01211v1   \n",
       "3564  http://arxiv.org/pdf/2307.01488v1   \n",
       "3565  http://arxiv.org/pdf/2307.06090v1   \n",
       "3566  http://arxiv.org/pdf/2307.07051v1   \n",
       "3567  http://arxiv.org/pdf/2307.10666v1   \n",
       "\n",
       "                                                Summary  \\\n",
       "0     Hadoop is one of the platform s that can proce...   \n",
       "1     Integrating AI Planning with Natural Language ...   \n",
       "2     The tools are machine learning based using nat...   \n",
       "3     The name Tangkhul also known as Hao or Ihao re...   \n",
       "4     This is a lecture note for the course DS GA at...   \n",
       "...                                                 ...   \n",
       "3563  An automated method for the ontologicalreprese...   \n",
       "3564  SCAT modifies ran driven augmentations of the ...   \n",
       "3565  Large language models LLMs have revolutionised...   \n",
       "3566  Predictive Power Varies with Clinical Note Typ...   \n",
       "3567  A Dataset and Strong Baselines for Classificat...   \n",
       "\n",
       "                                                  Title  \\\n",
       "0     Natural Language Processing using Hadoop and K...   \n",
       "1     Integrating AI Planning with Natural Language ...   \n",
       "2     Simple Natural Language Processing Tools for D...   \n",
       "3     Towards the Study of Morphological Processing ...   \n",
       "4     Natural Language Understanding with Distribute...   \n",
       "...                                                 ...   \n",
       "3563  An automated method for the ontological repres...   \n",
       "3564  SCAT: Robust Self-supervised Contrastive Learn...   \n",
       "3565  Can Large Language Models Aid in Annotating Sp...   \n",
       "3566  Making the Most Out of the Limited Context Len...   \n",
       "3567  A Dataset and Strong Baselines for Classificat...   \n",
       "\n",
       "                                                 Author  DOI  \\\n",
       "0     [arxiv.Result.Author('Emre Erturk'), arxiv.Res...  NaN   \n",
       "1     [arxiv.Result.Author('Kebing Jin'), arxiv.Resu...  NaN   \n",
       "2              [arxiv.Result.Author('Leon Derczynski')]  NaN   \n",
       "3     [arxiv.Result.Author('Mirinso Shadang'), arxiv...  NaN   \n",
       "4                [arxiv.Result.Author('Kyunghyun Cho')]  NaN   \n",
       "...                                                 ...  ...   \n",
       "3563  [arxiv.Result.Author('Giampaolo Bella'), arxiv...  NaN   \n",
       "3564  [arxiv.Result.Author('Junjie Wu'), arxiv.Resul...  NaN   \n",
       "3565  [arxiv.Result.Author('Siddique Latif'), arxiv....  NaN   \n",
       "3566  [arxiv.Result.Author('Hongyi Zheng'), arxiv.Re...  NaN   \n",
       "3567  [arxiv.Result.Author('Hynek Kydlíček'), arxiv....  NaN   \n",
       "\n",
       "                 Published Date  \\\n",
       "0     2016-08-15 23:09:21+00:00   \n",
       "1     2022-02-15 02:19:09+00:00   \n",
       "2     2019-06-27 13:15:12+00:00   \n",
       "3     2020-06-29 17:24:09+00:00   \n",
       "4     2015-11-24 23:23:13+00:00   \n",
       "...                         ...   \n",
       "3563  2023-06-30 09:04:47+00:00   \n",
       "3564  2023-07-04 05:41:31+00:00   \n",
       "3565  2023-07-12 11:27:40+00:00   \n",
       "3566  2023-07-13 20:04:05+00:00   \n",
       "3567  2023-07-20 07:47:08+00:00   \n",
       "\n",
       "                                               Abstract  \\\n",
       "0     natural language processing, as a data analyti...   \n",
       "1     natural language processing (nlp) aims at inve...   \n",
       "2     this technical note describes a set of baselin...   \n",
       "3     there is no or little work on natural language...   \n",
       "4     this is a lecture note for the course ds-ga 30...   \n",
       "...                                                 ...   \n",
       "3563  large documents written in juridical language ...   \n",
       "3564  despite their promising performance across var...   \n",
       "3565  despite recent advancements in speech emotion ...   \n",
       "3566  recent advances in large language models have ...   \n",
       "3567  pre-trained models for czech natural language ...   \n",
       "\n",
       "                                            Journal Ref Primary Category  \\\n",
       "0                                                   NaN            cs.CL   \n",
       "1                                                   NaN            cs.AI   \n",
       "2                                                   NaN            cs.CL   \n",
       "3     In proceeding of Regional International Confer...            cs.CL   \n",
       "4                                                   NaN            cs.CL   \n",
       "...                                                 ...              ...   \n",
       "3563                                                NaN            cs.AI   \n",
       "3564                                                NaN            cs.CL   \n",
       "3565                                                NaN            cs.SD   \n",
       "3566  Association for Computational Linguistics - St...            cs.CL   \n",
       "3567                                                NaN            cs.CL   \n",
       "\n",
       "                         Category                           Entry ID _merge  \n",
       "0                       ['cs.CL']  http://arxiv.org/abs/1608.04434v1   both  \n",
       "1              ['cs.AI', 'cs.CL']  http://arxiv.org/abs/2202.07138v2   both  \n",
       "2                       ['cs.CL']  http://arxiv.org/abs/1906.11608v2   both  \n",
       "3                       ['cs.CL']  http://arxiv.org/abs/2006.16212v1   both  \n",
       "4            ['cs.CL', 'stat.ML']  http://arxiv.org/abs/1511.07916v1   both  \n",
       "...                           ...                                ...    ...  \n",
       "3563           ['cs.AI', 'cs.CL']  http://arxiv.org/abs/2307.01211v1   both  \n",
       "3564                    ['cs.CL']  http://arxiv.org/abs/2307.01488v1   both  \n",
       "3565         ['cs.SD', 'eess.AS']  http://arxiv.org/abs/2307.06090v1   both  \n",
       "3566  ['cs.CL', 'cs.IR', 'cs.LG']  http://arxiv.org/abs/2307.07051v1   both  \n",
       "3567                    ['cs.CL']  http://arxiv.org/abs/2307.10666v1   both  \n",
       "\n",
       "[3568 rows x 12 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "940cc19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'original_df' is your original dataframe and 'lda_output' is the output from LDA\n",
    "merged_df = df.merge(df_topic_sents_keywords, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f7018c4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PDF URL</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Journal Ref</th>\n",
       "      <th>Primary Category</th>\n",
       "      <th>Category</th>\n",
       "      <th>Entry ID</th>\n",
       "      <th>_merge</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/pdf/1608.04434v1</td>\n",
       "      <td>Hadoop is one of the platform s that can proce...</td>\n",
       "      <td>Natural Language Processing using Hadoop and K...</td>\n",
       "      <td>[arxiv.Result.Author('Emre Erturk'), arxiv.Res...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-15 23:09:21+00:00</td>\n",
       "      <td>natural language processing, as a data analyti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1608.04434v1</td>\n",
       "      <td>both</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4399</td>\n",
       "      <td>system, speech, embedding, result, different, ...</td>\n",
       "      <td>[analytic, relate, technology, widely, many, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/pdf/2202.07138v2</td>\n",
       "      <td>Integrating AI Planning with Natural Language ...</td>\n",
       "      <td>Integrating AI Planning with Natural Language ...</td>\n",
       "      <td>[arxiv.Result.Author('Kebing Jin'), arxiv.Resu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-15 02:19:09+00:00</td>\n",
       "      <td>natural language processing (nlp) aims at inve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>['cs.AI', 'cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/2202.07138v2</td>\n",
       "      <td>both</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3579</td>\n",
       "      <td>llm, research, field, knowledge, attention, ch...</td>\n",
       "      <td>[aim, investigate, interaction, agent, human, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/pdf/1906.11608v2</td>\n",
       "      <td>The tools are machine learning based using nat...</td>\n",
       "      <td>Simple Natural Language Processing Tools for D...</td>\n",
       "      <td>[arxiv.Result.Author('Leon Derczynski')]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-06-27 13:15:12+00:00</td>\n",
       "      <td>this technical note describes a set of baselin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1906.11608v2</td>\n",
       "      <td>both</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3794</td>\n",
       "      <td>llm, research, field, knowledge, attention, ch...</td>\n",
       "      <td>[technical, note, describe, set, baseline, too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/pdf/2006.16212v1</td>\n",
       "      <td>The name Tangkhul also known as Hao or Ihao re...</td>\n",
       "      <td>Towards the Study of Morphological Processing ...</td>\n",
       "      <td>[arxiv.Result.Author('Mirinso Shadang'), arxiv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-06-29 17:24:09+00:00</td>\n",
       "      <td>there is no or little work on natural language...</td>\n",
       "      <td>In proceeding of Regional International Confer...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/2006.16212v1</td>\n",
       "      <td>both</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6887</td>\n",
       "      <td>system, speech, embedding, result, different, ...</td>\n",
       "      <td>[little, work, tangkhul, current, work, humble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/pdf/1511.07916v1</td>\n",
       "      <td>This is a lecture note for the course DS GA at...</td>\n",
       "      <td>Natural Language Understanding with Distribute...</td>\n",
       "      <td>[arxiv.Result.Author('Kyunghyun Cho')]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-11-24 23:23:13+00:00</td>\n",
       "      <td>this is a lecture note for the course ds-ga 30...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL', 'stat.ML']</td>\n",
       "      <td>http://arxiv.org/abs/1511.07916v1</td>\n",
       "      <td>both</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4473</td>\n",
       "      <td>llm, research, field, knowledge, attention, ch...</td>\n",
       "      <td>[note, course, understanding, distribute, repr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             PDF URL  \\\n",
       "0  http://arxiv.org/pdf/1608.04434v1   \n",
       "1  http://arxiv.org/pdf/2202.07138v2   \n",
       "2  http://arxiv.org/pdf/1906.11608v2   \n",
       "3  http://arxiv.org/pdf/2006.16212v1   \n",
       "4  http://arxiv.org/pdf/1511.07916v1   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  Hadoop is one of the platform s that can proce...   \n",
       "1  Integrating AI Planning with Natural Language ...   \n",
       "2  The tools are machine learning based using nat...   \n",
       "3  The name Tangkhul also known as Hao or Ihao re...   \n",
       "4  This is a lecture note for the course DS GA at...   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Natural Language Processing using Hadoop and K...   \n",
       "1  Integrating AI Planning with Natural Language ...   \n",
       "2  Simple Natural Language Processing Tools for D...   \n",
       "3  Towards the Study of Morphological Processing ...   \n",
       "4  Natural Language Understanding with Distribute...   \n",
       "\n",
       "                                              Author  DOI  \\\n",
       "0  [arxiv.Result.Author('Emre Erturk'), arxiv.Res...  NaN   \n",
       "1  [arxiv.Result.Author('Kebing Jin'), arxiv.Resu...  NaN   \n",
       "2           [arxiv.Result.Author('Leon Derczynski')]  NaN   \n",
       "3  [arxiv.Result.Author('Mirinso Shadang'), arxiv...  NaN   \n",
       "4             [arxiv.Result.Author('Kyunghyun Cho')]  NaN   \n",
       "\n",
       "              Published Date  \\\n",
       "0  2016-08-15 23:09:21+00:00   \n",
       "1  2022-02-15 02:19:09+00:00   \n",
       "2  2019-06-27 13:15:12+00:00   \n",
       "3  2020-06-29 17:24:09+00:00   \n",
       "4  2015-11-24 23:23:13+00:00   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  natural language processing, as a data analyti...   \n",
       "1  natural language processing (nlp) aims at inve...   \n",
       "2  this technical note describes a set of baselin...   \n",
       "3  there is no or little work on natural language...   \n",
       "4  this is a lecture note for the course ds-ga 30...   \n",
       "\n",
       "                                         Journal Ref Primary Category  \\\n",
       "0                                                NaN            cs.CL   \n",
       "1                                                NaN            cs.AI   \n",
       "2                                                NaN            cs.CL   \n",
       "3  In proceeding of Regional International Confer...            cs.CL   \n",
       "4                                                NaN            cs.CL   \n",
       "\n",
       "               Category                           Entry ID _merge  \\\n",
       "0             ['cs.CL']  http://arxiv.org/abs/1608.04434v1   both   \n",
       "1    ['cs.AI', 'cs.CL']  http://arxiv.org/abs/2202.07138v2   both   \n",
       "2             ['cs.CL']  http://arxiv.org/abs/1906.11608v2   both   \n",
       "3             ['cs.CL']  http://arxiv.org/abs/2006.16212v1   both   \n",
       "4  ['cs.CL', 'stat.ML']  http://arxiv.org/abs/1511.07916v1   both   \n",
       "\n",
       "   Dominant_Topic  Perc_Contribution  \\\n",
       "0               3             0.4399   \n",
       "1               0             0.3579   \n",
       "2               0             0.3794   \n",
       "3               3             0.6887   \n",
       "4               0             0.4473   \n",
       "\n",
       "                                      Topic_Keywords  \\\n",
       "0  system, speech, embedding, result, different, ...   \n",
       "1  llm, research, field, knowledge, attention, ch...   \n",
       "2  llm, research, field, knowledge, attention, ch...   \n",
       "3  system, speech, embedding, result, different, ...   \n",
       "4  llm, research, field, knowledge, attention, ch...   \n",
       "\n",
       "                                                   0  \n",
       "0  [analytic, relate, technology, widely, many, r...  \n",
       "1  [aim, investigate, interaction, agent, human, ...  \n",
       "2  [technical, note, describe, set, baseline, too...  \n",
       "3  [little, work, tangkhul, current, work, humble...  \n",
       "4  [note, course, understanding, distribute, repr...  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0ab5e733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1391\n",
       "3    1169\n",
       "4     452\n",
       "0     336\n",
       "1     220\n",
       "Name: Dominant_Topic, dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['Dominant_Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "63f0b4b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Title</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>A survey paper proposes a clearer view of natu...</td>\n",
       "      <td>Natural Language Reasoning, A Survey</td>\n",
       "      <td>0.7702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>This study provides an overview of the history...</td>\n",
       "      <td>Sejarah dan Perkembangan Teknik Natural Langua...</td>\n",
       "      <td>0.7223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3066</th>\n",
       "      <td>Natural Language QA Approaches using Reasoning...</td>\n",
       "      <td>Natural Language QA Approaches using Reasoning...</td>\n",
       "      <td>0.6729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Exploring the Landscape of Natural Language Pr...</td>\n",
       "      <td>Exploring the Landscape of Natural Language Pr...</td>\n",
       "      <td>0.6586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>A A Bibliometric Review of Large Language Mode...</td>\n",
       "      <td>A Bibliometric Review of Large Language Models...</td>\n",
       "      <td>0.6464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>Dual use the intentional harmful reuse of tech...</td>\n",
       "      <td>Thorny Roses: Investigating the Dual Use Dilem...</td>\n",
       "      <td>0.6438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>A Survey of the Usages of Deep Learning for Na...</td>\n",
       "      <td>A Survey of the Usages of Deep Learning in Nat...</td>\n",
       "      <td>0.6351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>A Survey of the Usages of Deep Learning for Na...</td>\n",
       "      <td>A Survey of the Usages of Deep Learning in Nat...</td>\n",
       "      <td>0.6351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>A Survey of the Usages of Deep Learning for Na...</td>\n",
       "      <td>A Survey of the Usages of Deep Learning in Nat...</td>\n",
       "      <td>0.6351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>A Survey of the Usages of Deep Learning for Na...</td>\n",
       "      <td>A Survey of the Usages of Deep Learning in Nat...</td>\n",
       "      <td>0.6351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Summary  \\\n",
       "36    A survey paper proposes a clearer view of natu...   \n",
       "136   This study provides an overview of the history...   \n",
       "3066  Natural Language QA Approaches using Reasoning...   \n",
       "231   Exploring the Landscape of Natural Language Pr...   \n",
       "1634  A A Bibliometric Review of Large Language Mode...   \n",
       "2508  Dual use the intentional harmful reuse of tech...   \n",
       "1802  A Survey of the Usages of Deep Learning for Na...   \n",
       "1817  A Survey of the Usages of Deep Learning for Na...   \n",
       "1816  A Survey of the Usages of Deep Learning for Na...   \n",
       "1801  A Survey of the Usages of Deep Learning for Na...   \n",
       "\n",
       "                                                  Title  Perc_Contribution  \n",
       "36                 Natural Language Reasoning, A Survey             0.7702  \n",
       "136   Sejarah dan Perkembangan Teknik Natural Langua...             0.7223  \n",
       "3066  Natural Language QA Approaches using Reasoning...             0.6729  \n",
       "231   Exploring the Landscape of Natural Language Pr...             0.6586  \n",
       "1634  A Bibliometric Review of Large Language Models...             0.6464  \n",
       "2508  Thorny Roses: Investigating the Dual Use Dilem...             0.6438  \n",
       "1802  A Survey of the Usages of Deep Learning in Nat...             0.6351  \n",
       "1817  A Survey of the Usages of Deep Learning in Nat...             0.6351  \n",
       "1816  A Survey of the Usages of Deep Learning in Nat...             0.6351  \n",
       "1801  A Survey of the Usages of Deep Learning in Nat...             0.6351  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.loc[merged_df['Dominant_Topic'] == 0].sort_values('Perc_Contribution', ascending=False)[['Summary','Title','Perc_Contribution']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4fdd89ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Title</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The thesis develops the translation between ca...</td>\n",
       "      <td>Categorical Tools for Natural Language Processing</td>\n",
       "      <td>0.7087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075</th>\n",
       "      <td>ArXiv v math CT Apr Volume Issue ISSN Pregroup...</td>\n",
       "      <td>Lambek pregroups are Frobenius spiders in preo...</td>\n",
       "      <td>0.7053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>Graph Interpolation Grammars are a rule based ...</td>\n",
       "      <td>Graph Interpolation Grammars: a Rule-based App...</td>\n",
       "      <td>0.6972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758</th>\n",
       "      <td>The cen tral role of the lexicon in Meaning T ...</td>\n",
       "      <td>A Formal Look at Dependency Grammars and Phras...</td>\n",
       "      <td>0.6745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>In this chapter we introduce a new dialogical ...</td>\n",
       "      <td>Logical Semantics, Dialogical Argumentation, a...</td>\n",
       "      <td>0.6735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914</th>\n",
       "      <td>Quantized Detector Networks QDN is a descripti...</td>\n",
       "      <td>Quantized Detector Networks: A review of recen...</td>\n",
       "      <td>0.6622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034</th>\n",
       "      <td>Category Theory for Quantum encompasses quantu...</td>\n",
       "      <td>Category Theory for Quantum Natural Language P...</td>\n",
       "      <td>0.6487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>Program equivalence is the fulcrum for reasoni...</td>\n",
       "      <td>Recursive Session Logical Relations</td>\n",
       "      <td>0.6444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Paper defines event expression over sentences ...</td>\n",
       "      <td>Natural Language Understanding Based on Semant...</td>\n",
       "      <td>0.6417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>For any monoid M the family of languages accep...</td>\n",
       "      <td>Rational semigroup automata</td>\n",
       "      <td>0.6187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Summary  \\\n",
       "28    The thesis develops the translation between ca...   \n",
       "3075  ArXiv v math CT Apr Volume Issue ISSN Pregroup...   \n",
       "234   Graph Interpolation Grammars are a rule based ...   \n",
       "2758  The cen tral role of the lexicon in Meaning T ...   \n",
       "80    In this chapter we introduce a new dialogical ...   \n",
       "1914  Quantized Detector Networks QDN is a descripti...   \n",
       "3034  Category Theory for Quantum encompasses quantu...   \n",
       "2937  Program equivalence is the fulcrum for reasoni...   \n",
       "25    Paper defines event expression over sentences ...   \n",
       "1572  For any monoid M the family of languages accep...   \n",
       "\n",
       "                                                  Title  Perc_Contribution  \n",
       "28    Categorical Tools for Natural Language Processing             0.7087  \n",
       "3075  Lambek pregroups are Frobenius spiders in preo...             0.7053  \n",
       "234   Graph Interpolation Grammars: a Rule-based App...             0.6972  \n",
       "2758  A Formal Look at Dependency Grammars and Phras...             0.6745  \n",
       "80    Logical Semantics, Dialogical Argumentation, a...             0.6735  \n",
       "1914  Quantized Detector Networks: A review of recen...             0.6622  \n",
       "3034  Category Theory for Quantum Natural Language P...             0.6487  \n",
       "2937                Recursive Session Logical Relations             0.6444  \n",
       "25    Natural Language Understanding Based on Semant...             0.6417  \n",
       "1572                        Rational semigroup automata             0.6187  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.loc[merged_df['Dominant_Topic'] == 1].sort_values('Perc_Contribution', ascending=False)[['Summary','Title','Perc_Contribution']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b0665b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Title</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2168</th>\n",
       "      <td>Deep and large pre trained models are the stat...</td>\n",
       "      <td>XtremeDistilTransformers: Task Transfer for Ta...</td>\n",
       "      <td>0.8853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>SwitchPrompt Learning Domain Speci c Gated Sof...</td>\n",
       "      <td>SwitchPrompt: Learning Domain-Specific Gated S...</td>\n",
       "      <td>0.8801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3564</th>\n",
       "      <td>SCAT modifies ran driven augmentations of the ...</td>\n",
       "      <td>SCAT: Robust Self-supervised Contrastive Learn...</td>\n",
       "      <td>0.8751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3411</th>\n",
       "      <td>Back Translated Task Adaptive Pretraining Impr...</td>\n",
       "      <td>Back-Translated Task Adaptive Pretraining: Imp...</td>\n",
       "      <td>0.8484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1876</th>\n",
       "      <td>Recent work has shown that deep transformer la...</td>\n",
       "      <td>GPT-3 Models are Poor Few-Shot Learners in the...</td>\n",
       "      <td>0.8475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Cross lingual Adaption Model Agnostic Meta Lea...</td>\n",
       "      <td>Cross-lingual Adaption Model-Agnostic Meta-Lea...</td>\n",
       "      <td>0.8405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>NTK approximating MLP Fusion for Efficient Lan...</td>\n",
       "      <td>NTK-approximating MLP Fusion for Efficient Lan...</td>\n",
       "      <td>0.8257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>Fine Tuning Transformers Vocabulary Transfer T...</td>\n",
       "      <td>Fine-Tuning Transformers: Vocabulary Transfer</td>\n",
       "      <td>0.8196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2194</th>\n",
       "      <td>Prompt Tuning for Discriminative Pre trained L...</td>\n",
       "      <td>Prompt Tuning for Discriminative Pre-trained L...</td>\n",
       "      <td>0.8172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>HyPe Better Pre trained Language Model Fine tu...</td>\n",
       "      <td>HyPe: Better Pre-trained Language Model Fine-t...</td>\n",
       "      <td>0.8149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Summary  \\\n",
       "2168  Deep and large pre trained models are the stat...   \n",
       "1161  SwitchPrompt Learning Domain Speci c Gated Sof...   \n",
       "3564  SCAT modifies ran driven augmentations of the ...   \n",
       "3411  Back Translated Task Adaptive Pretraining Impr...   \n",
       "1876  Recent work has shown that deep transformer la...   \n",
       "39    Cross lingual Adaption Model Agnostic Meta Lea...   \n",
       "1325  NTK approximating MLP Fusion for Efficient Lan...   \n",
       "511   Fine Tuning Transformers Vocabulary Transfer T...   \n",
       "2194  Prompt Tuning for Discriminative Pre trained L...   \n",
       "654   HyPe Better Pre trained Language Model Fine tu...   \n",
       "\n",
       "                                                  Title  Perc_Contribution  \n",
       "2168  XtremeDistilTransformers: Task Transfer for Ta...             0.8853  \n",
       "1161  SwitchPrompt: Learning Domain-Specific Gated S...             0.8801  \n",
       "3564  SCAT: Robust Self-supervised Contrastive Learn...             0.8751  \n",
       "3411  Back-Translated Task Adaptive Pretraining: Imp...             0.8484  \n",
       "1876  GPT-3 Models are Poor Few-Shot Learners in the...             0.8475  \n",
       "39    Cross-lingual Adaption Model-Agnostic Meta-Lea...             0.8405  \n",
       "1325  NTK-approximating MLP Fusion for Efficient Lan...             0.8257  \n",
       "511       Fine-Tuning Transformers: Vocabulary Transfer             0.8196  \n",
       "2194  Prompt Tuning for Discriminative Pre-trained L...             0.8172  \n",
       "654   HyPe: Better Pre-trained Language Model Fine-t...             0.8149  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.loc[merged_df['Dominant_Topic'] == 2].sort_values('Perc_Contribution', ascending=False)[['Summary','Title','Perc_Contribution']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "427f322c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Title</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>The aim of this challenge is to build a multi ...</td>\n",
       "      <td>Multi-Speaker Multi-Lingual VQTTS System for L...</td>\n",
       "      <td>0.8278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>Exploiting Transliterated Words for Finding Si...</td>\n",
       "      <td>Exploiting Transliterated Words for Finding Si...</td>\n",
       "      <td>0.8168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>An annotated dataset and tagger tool for the l...</td>\n",
       "      <td>UzbekTagger: The rule-based POS tagger for Uzb...</td>\n",
       "      <td>0.7859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Automatic Language Identi cation for RomanceLa...</td>\n",
       "      <td>Automatic Language Identification for Romance ...</td>\n",
       "      <td>0.7854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2535</th>\n",
       "      <td>Building a Lemmatizer and a Spell checker for ...</td>\n",
       "      <td>Building a Lemmatizer and a Spell-checker for ...</td>\n",
       "      <td>0.7837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>Experiments with POS Tagging Code mixed Indian...</td>\n",
       "      <td>Experiments with POS Tagging Code-mixed Indian...</td>\n",
       "      <td>0.7800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>This paper reports about our work in the NLP o...</td>\n",
       "      <td>JU_KS@SAIL_CodeMixed-2017: Sentiment Analysis ...</td>\n",
       "      <td>0.7757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>The models hauWE Hausa Word s Embedding are bi...</td>\n",
       "      <td>hauWE: Hausa Words Embedding for Natural Langu...</td>\n",
       "      <td>0.7615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>Mapping supervised bilingual word embeddings f...</td>\n",
       "      <td>Mapping Supervised Bilingual Word Embeddings f...</td>\n",
       "      <td>0.7613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3317</th>\n",
       "      <td>The complexities of Arabic language in morphol...</td>\n",
       "      <td>Improving Sentiment Analysis in Arabic Using W...</td>\n",
       "      <td>0.7603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Summary  \\\n",
       "3200  The aim of this challenge is to build a multi ...   \n",
       "801   Exploiting Transliterated Words for Finding Si...   \n",
       "532   An annotated dataset and tagger tool for the l...   \n",
       "82    Automatic Language Identi cation for RomanceLa...   \n",
       "2535  Building a Lemmatizer and a Spell checker for ...   \n",
       "2092  Experiments with POS Tagging Code mixed Indian...   \n",
       "628   This paper reports about our work in the NLP o...   \n",
       "1443  The models hauWE Hausa Word s Embedding are bi...   \n",
       "1102  Mapping supervised bilingual word embeddings f...   \n",
       "3317  The complexities of Arabic language in morphol...   \n",
       "\n",
       "                                                  Title  Perc_Contribution  \n",
       "3200  Multi-Speaker Multi-Lingual VQTTS System for L...             0.8278  \n",
       "801   Exploiting Transliterated Words for Finding Si...             0.8168  \n",
       "532   UzbekTagger: The rule-based POS tagger for Uzb...             0.7859  \n",
       "82    Automatic Language Identification for Romance ...             0.7854  \n",
       "2535  Building a Lemmatizer and a Spell-checker for ...             0.7837  \n",
       "2092  Experiments with POS Tagging Code-mixed Indian...             0.7800  \n",
       "628   JU_KS@SAIL_CodeMixed-2017: Sentiment Analysis ...             0.7757  \n",
       "1443  hauWE: Hausa Words Embedding for Natural Langu...             0.7615  \n",
       "1102  Mapping Supervised Bilingual Word Embeddings f...             0.7613  \n",
       "3317  Improving Sentiment Analysis in Arabic Using W...             0.7603  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.loc[merged_df['Dominant_Topic'] == 3].sort_values('Perc_Contribution', ascending=False)[['Summary','Title','Perc_Contribution']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6f95c967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Title</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Most musical programming languages are develop...</td>\n",
       "      <td>Composition by Conversation</td>\n",
       "      <td>0.7401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>Interactive Task and Concept Learning from Nat...</td>\n",
       "      <td>Interactive Task and Concept Learning from Nat...</td>\n",
       "      <td>0.7328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>The approach is general and could be applied t...</td>\n",
       "      <td>Processing Natural Language About Ongoing Actions</td>\n",
       "      <td>0.7322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>Current version of Business process management...</td>\n",
       "      <td>Towards ontology based BPMN Implementation</td>\n",
       "      <td>0.7273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783</th>\n",
       "      <td>Automating a factory where robots are involved...</td>\n",
       "      <td>Towards VEsNA, a Framework for Managing Virtua...</td>\n",
       "      <td>0.7052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>The database systems course is offered as part...</td>\n",
       "      <td>Towards Enhancing Database Education: Natural ...</td>\n",
       "      <td>0.6893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>Intelligent Home D Automatic D House Design fr...</td>\n",
       "      <td>Intelligent Home 3D: Automatic 3D-House Design...</td>\n",
       "      <td>0.6833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>Knowledge graphs KG have become an important d...</td>\n",
       "      <td>Visual Diagrammatic Queries in ViziQuer: Overv...</td>\n",
       "      <td>0.6792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>Sense Plan Ask is a novel approach for generat...</td>\n",
       "      <td>SPA: Verbal Interactions between Agents and Av...</td>\n",
       "      <td>0.6760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>The paper intends to present a review on Objec...</td>\n",
       "      <td>Object Oriented Analysis using Natural Languag...</td>\n",
       "      <td>0.6649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Summary  \\\n",
       "41    Most musical programming languages are develop...   \n",
       "601   Interactive Task and Concept Learning from Nat...   \n",
       "262   The approach is general and could be applied t...   \n",
       "1009  Current version of Business process management...   \n",
       "1783  Automating a factory where robots are involved...   \n",
       "1484  The database systems course is offered as part...   \n",
       "2540  Intelligent Home D Automatic D House Design fr...   \n",
       "2565  Knowledge graphs KG have become an important d...   \n",
       "682   Sense Plan Ask is a novel approach for generat...   \n",
       "602   The paper intends to present a review on Objec...   \n",
       "\n",
       "                                                  Title  Perc_Contribution  \n",
       "41                          Composition by Conversation             0.7401  \n",
       "601   Interactive Task and Concept Learning from Nat...             0.7328  \n",
       "262   Processing Natural Language About Ongoing Actions             0.7322  \n",
       "1009         Towards ontology based BPMN Implementation             0.7273  \n",
       "1783  Towards VEsNA, a Framework for Managing Virtua...             0.7052  \n",
       "1484  Towards Enhancing Database Education: Natural ...             0.6893  \n",
       "2540  Intelligent Home 3D: Automatic 3D-House Design...             0.6833  \n",
       "2565  Visual Diagrammatic Queries in ViziQuer: Overv...             0.6792  \n",
       "682   SPA: Verbal Interactions between Agents and Av...             0.6760  \n",
       "602   Object Oriented Analysis using Natural Languag...             0.6649  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.loc[merged_df['Dominant_Topic'] == 4].sort_values('Perc_Contribution', ascending=False)[['Summary','Title','Perc_Contribution']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "53de37ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('../data/Dominant_topics_of_summaries_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b35a269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaa810f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06bf94bd",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6e682120",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [113], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    break\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f82fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=texts)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcdeb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.to_csv('../data/Dominant_topics_of_absracts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea71fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4634e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic['Dominant_Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db13218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0c711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display setting to show more characters in column\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8870b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46af1a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75152243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce302396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20e2173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696279a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79536c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a89ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
