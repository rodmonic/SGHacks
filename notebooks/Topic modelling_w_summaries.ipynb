{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87fc752e",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd90fd",
   "metadata": {},
   "source": [
    "## 0.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36bd3d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dglover\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "217c8d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd49c49d",
   "metadata": {},
   "source": [
    "## 1.0 Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa4a4912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['use', 'show', 'however', 'approach', 'well', 'provide',' present', 'include', 'word', 'nlp', 'natural', 'language', 'processing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feb04083",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Filename</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1809.09190v1.pdf</th>\n",
       "      <td>In this paper we formulate audio play hot n c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903.10625v2.pdf</th>\n",
       "      <td>Finite State Transducers FST are an efficient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904.04307v1.pdf</th>\n",
       "      <td>The quantification of semantic similarity bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809.02794v3.pdf</th>\n",
       "      <td>This paper focuses on the aim of semantic rol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805.01083v1.pdf</th>\n",
       "      <td>The mainconstructusedinextractionlanguagesand...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            Summary\n",
       "Filename                                                           \n",
       "1809.09190v1.pdf   In this paper we formulate audio play hot n c...\n",
       "1903.10625v2.pdf   Finite State Transducers FST are an efficient...\n",
       "1904.04307v1.pdf   The quantification of semantic similarity bet...\n",
       "1809.02794v3.pdf   This paper focuses on the aim of semantic rol...\n",
       "1805.01083v1.pdf   The mainconstructusedinextractionlanguagesand..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dataset\n",
    "df = pd.read_csv('..\\\\data\\\\50_summaries.csv', index_col=0)\n",
    "#print(df.target_names.unique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f6db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "827f680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "df2 = pd.read_csv('..\\\\data\\\\arxiv_papers_full_v2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05f61cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>PDF URL</th>\n",
       "      <th>Author</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Journal Ref</th>\n",
       "      <th>Primary Category</th>\n",
       "      <th>Category</th>\n",
       "      <th>Entry ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Natural Language Processing using Hadoop and K...</td>\n",
       "      <td>http://arxiv.org/pdf/1608.04434v1</td>\n",
       "      <td>[arxiv.Result.Author('Emre Erturk'), arxiv.Res...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-15 23:09:21+00:00</td>\n",
       "      <td>Natural language processing, as a data analyti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1608.04434v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Integrating AI Planning with Natural Language ...</td>\n",
       "      <td>http://arxiv.org/pdf/2202.07138v2</td>\n",
       "      <td>[arxiv.Result.Author('Kebing Jin'), arxiv.Resu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-15 02:19:09+00:00</td>\n",
       "      <td>Natural language processing (NLP) aims at inve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>['cs.AI', 'cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/2202.07138v2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Natural Language Processing using Hadoop and K...   \n",
       "1  Integrating AI Planning with Natural Language ...   \n",
       "\n",
       "                             PDF URL  \\\n",
       "0  http://arxiv.org/pdf/1608.04434v1   \n",
       "1  http://arxiv.org/pdf/2202.07138v2   \n",
       "\n",
       "                                              Author  DOI  \\\n",
       "0  [arxiv.Result.Author('Emre Erturk'), arxiv.Res...  NaN   \n",
       "1  [arxiv.Result.Author('Kebing Jin'), arxiv.Resu...  NaN   \n",
       "\n",
       "              Published Date  \\\n",
       "0  2016-08-15 23:09:21+00:00   \n",
       "1  2022-02-15 02:19:09+00:00   \n",
       "\n",
       "                                             Summary Journal Ref  \\\n",
       "0  Natural language processing, as a data analyti...         NaN   \n",
       "1  Natural language processing (NLP) aims at inve...         NaN   \n",
       "\n",
       "  Primary Category            Category                           Entry ID  \n",
       "0            cs.CL           ['cs.CL']  http://arxiv.org/abs/1608.04434v1  \n",
       "1            cs.AI  ['cs.AI', 'cs.CL']  http://arxiv.org/abs/2202.07138v2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "454d6e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.rename(columns = {'Summary' : 'Abstract'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d63e95d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Filename'] = df2['PDF URL'].map(lambda x: x.split('/')[-1] + '.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f9755d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df2, how = 'left', on = 'Filename', indicator = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26ab14fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "both          50\n",
       "left_only      0\n",
       "right_only     0\n",
       "Name: _merge, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['_merge'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "796e6d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filename             0\n",
       "Summary              0\n",
       "Title                0\n",
       "PDF URL              0\n",
       "Author               0\n",
       "DOI                 47\n",
       "Published Date       0\n",
       "Abstract             0\n",
       "Journal Ref         44\n",
       "Primary Category     0\n",
       "Category             0\n",
       "Entry ID             0\n",
       "_merge               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18bd48f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d50a2586",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Summary'] = df['Summary'].map(lambda x: x.lower().replace('natural langauge processing', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e2fa055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = df['Summary'].values.tolist()\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bebe6739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf715a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd9a7962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30802b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dglover\\Anaconda3\\lib\\site-packages\\spacy\\language.py:1895: UserWarning: [W123] Argument disable with value ['parser', 'ner'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "#nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b41c0fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e832d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6284bd38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.023*\"use\" + 0.015*\"model\" + 0.013*\"semantic\" + 0.012*\"task\" + '\n",
      "  '0.010*\"framework\" + 0.009*\"system\" + 0.008*\"word\" + 0.008*\"propose\" + '\n",
      "  '0.008*\"lexical\" + 0.008*\"work\"'),\n",
      " (1,\n",
      "  '0.019*\"model\" + 0.013*\"task\" + 0.013*\"use\" + 0.011*\"propose\" + '\n",
      "  '0.009*\"write\" + 0.009*\"paper\" + 0.007*\"system\" + 0.007*\"neural\" + '\n",
      "  '0.007*\"set\" + 0.007*\"base\"'),\n",
      " (2,\n",
      "  '0.019*\"system\" + 0.010*\"lithium\" + 0.010*\"lm\" + 0.010*\"usage\" + '\n",
      "  '0.010*\"sentence\" + 0.007*\"user\" + 0.007*\"rich\" + 0.007*\"free\" + '\n",
      "  '0.007*\"learn\" + 0.007*\"content\"'),\n",
      " (3,\n",
      "  '0.018*\"use\" + 0.013*\"method\" + 0.010*\"sentence\" + 0.010*\"research\" + '\n",
      "  '0.010*\"classification\" + 0.008*\"task\" + 0.008*\"present\" + '\n",
      "  '0.008*\"application\" + 0.008*\"learn\" + 0.005*\"model\"'),\n",
      " (4,\n",
      "  '0.025*\"model\" + 0.016*\"task\" + 0.015*\"text\" + 0.012*\"work\" + 0.012*\"base\" + '\n",
      "  '0.012*\"project\" + 0.012*\"key\" + 0.010*\"paper\" + 0.010*\"feature\" + '\n",
      "  '0.009*\"method\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91679646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -6.804613259867314\n",
      "\n",
      "Coherence Score:  0.34664226052529784\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15c032eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dglover\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1553221182940891683359494335\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1553221182940891683359494335_data = {\"mdsDat\": {\"x\": [-0.1285375052417719, 0.09044151835615882, 0.007779297288203437, 0.03976416617017218, -0.00944747657276255], \"y\": [-0.03294979753989916, -0.08176211067954609, -0.040785891562225, 0.10049963342775872, 0.054998166353911596], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [28.62935876445726, 24.138521982609717, 20.992728168011595, 15.751975061423867, 10.48741602349756]}, \"tinfo\": {\"Term\": [\"system\", \"sentence\", \"semantic\", \"key\", \"project\", \"use\", \"classification\", \"lithium\", \"usage\", \"lm\", \"feature\", \"research\", \"base\", \"write\", \"present\", \"focus\", \"learn\", \"lexical\", \"free\", \"interpretation\", \"also\", \"rich\", \"content\", \"introduce\", \"method\", \"visual\", \"text\", \"application\", \"new\", \"exceed\", \"key\", \"feature\", \"emotion\", \"organize\", \"tag\", \"topic\", \"project\", \"indian\", \"pos\", \"understand\", \"memory\", \"intent\", \"contextual\", \"mix\", \"hide\", \"imputation\", \"graph\", \"national\", \"tool\", \"discuss\", \"suppose\", \"mention\", \"detection\", \"czech\", \"learning\", \"table\", \"review\", \"clinical\", \"diverse\", \"globally\", \"text\", \"base\", \"ground\", \"model\", \"work\", \"social\", \"arabic\", \"label\", \"improve\", \"classifier\", \"paper\", \"task\", \"present\", \"extraction\", \"method\", \"efficient\", \"sentiment\", \"aim\", \"embed\", \"represent\", \"information\", \"use\", \"available\", \"network\", \"propose\", \"word\", \"lexical\", \"parallel\", \"semantic\", \"confound\", \"lolcode\", \"course\", \"computer\", \"distribute\", \"science\", \"distance\", \"module\", \"tree\", \"language\", \"hybrid\", \"code\", \"type\", \"expression\", \"gain\", \"precision\", \"encode\", \"pattern\", \"re\", \"special\", \"test\", \"source\", \"latent\", \"attention\", \"interpretation\", \"cat\", \"seek\", \"evaluation\", \"representation\", \"framework\", \"also\", \"use\", \"author\", \"model\", \"new\", \"word\", \"task\", \"system\", \"propose\", \"work\", \"input\", \"set\", \"network\", \"learn\", \"method\", \"make\", \"text\", \"stateoftheart\", \"summarization\", \"application\", \"pretraine\", \"term\", \"dependency\", \"write\", \"exceed\", \"worsen\", \"constrain\", \"state\", \"read\", \"counterexample\", \"probability\", \"thesis\", \"path\", \"modification\", \"minimal\", \"retrieval\", \"useful\", \"output\", \"take\", \"widespread\", \"ofdotype\", \"cold\", \"alarm\", \"device\", \"hot\", \"adoption\", \"pm\", \"audio\", \"smart\", \"database\", \"transducer\", \"phrasebase\", \"bryant\", \"context\", \"play\", \"human\", \"error\", \"even\", \"require\", \"vector\", \"speak\", \"consist\", \"plan\", \"rnn\", \"neural\", \"input\", \"propose\", \"model\", \"task\", \"paper\", \"use\", \"set\", \"learn\", \"base\", \"system\", \"need\", \"represent\", \"apply\", \"explain\", \"network\", \"biomedical\", \"offensive\", \"forum\", \"building\", \"strategy\", \"interact\", \"motivate\", \"several\", \"amount\", \"domain\", \"tackle\", \"unsupervise\", \"uniblock\", \"easily\", \"block\", \"supervised\", \"experimental\", \"illegal\", \"characbyorigin\", \"gulfarabic\", \"subdialect\", \"scoring\", \"filter\", \"removal\", \"corlinguistic\", \"yadac\", \"morphologically\", \"annotate\", \"billel\", \"houda\", \"research\", \"classification\", \"interaction\", \"textual\", \"sentence\", \"application\", \"method\", \"robot\", \"explain\", \"use\", \"study\", \"perform\", \"present\", \"property\", \"traditional\", \"novel\", \"learn\", \"also\", \"evaluate\", \"sentiment\", \"task\", \"result\", \"information\", \"large\", \"model\", \"lithium\", \"lm\", \"usage\", \"rich\", \"content\", \"visual\", \"formal\", \"run\", \"jargon\", \"industrial\", \"par\", \"commodity\", \"posrengle\", \"throughput\", \"practical\", \"follow\", \"able\", \"grammatical\", \"narrow\", \"deep\", \"affect\", \"quality\", \"labeling\", \"skill\", \"prune\", \"quantity\", \"pluginandplay\", \"contextualize\", \"readily\", \"shallowand\", \"free\", \"system\", \"focus\", \"sentence\", \"user\", \"introduce\", \"new\", \"learn\", \"model\", \"available\", \"project\", \"machine\", \"paper\", \"high\", \"set\", \"heavy\", \"apply\", \"information\", \"selection\", \"stateoftheart\", \"involve\"], \"Freq\": [11.0, 5.0, 6.0, 5.0, 6.0, 20.0, 4.0, 2.0, 2.0, 2.0, 5.0, 4.0, 8.0, 3.0, 5.0, 3.0, 7.0, 3.0, 2.0, 3.0, 3.0, 1.0, 1.0, 3.0, 10.0, 1.0, 9.0, 5.0, 6.0, 2.0, 5.2365950222250826, 4.459012872626653, 3.058725743235444, 3.0540160145959567, 3.0496635657396247, 3.03875299746282, 5.237577799928197, 2.3301132410007552, 2.3268931768822427, 2.326118898485781, 1.6021637417644774, 1.6021631218458252, 1.602154752944022, 1.6018064619813221, 1.60166945995921, 1.601105643945133, 1.6010699986226378, 1.600807979672354, 1.600695981035876, 1.6007959945784136, 1.600375173133419, 1.5998750021009567, 1.599144944568344, 1.5987715469002344, 1.5984081712503337, 1.5985892908165196, 1.596240419043746, 1.595901530180603, 1.5956925142750729, 1.5944350092893058, 6.631946772587443, 5.238761844553692, 3.0664117013236214, 11.074923589521802, 5.25291334754363, 2.3385090059455766, 2.322960619592927, 2.3198056469328843, 2.3282747689182033, 2.3285828684882928, 4.528989703353807, 7.255401440670152, 3.0529009875803954, 2.327341791346806, 4.150823620972334, 2.3304587423295207, 2.3296084205784022, 2.3034445472273433, 2.32837168286748, 2.311022846109607, 2.693721980314722, 3.797881201986248, 2.332138308597586, 2.336933172731961, 2.334815737255968, 2.3338304798782445, 2.9242085024758873, 2.228591903841451, 5.01047934599905, 2.142270678906659, 1.532140415106001, 1.5321391955253025, 1.532137975944604, 1.532137975944604, 1.5321376274929759, 1.5321306584604133, 1.5321268254925038, 1.532119508008313, 1.5321191595566848, 1.5321175915243581, 1.5312076100974819, 1.5309067221165866, 1.5304561741614073, 1.5301927447305368, 1.53007810414488, 1.5294400892137636, 1.5293146466276348, 1.5286161753390368, 1.5277821563670941, 1.5234461985323882, 1.5167146357542494, 1.4482354408736091, 2.9131028264097614, 2.710509216843777, 0.8356817831121746, 0.8356818702250817, 1.5301077225332715, 1.5297397576139604, 3.5799780894006843, 2.228009989622464, 8.525607102887406, 2.2301480888127045, 5.7042224944548146, 2.8008088882428077, 2.948341565337307, 4.503976679921983, 3.524537693106159, 2.938904449892509, 2.919985442968697, 2.1222430731279336, 2.150716449469411, 2.2274738967925773, 2.2396874748103603, 1.919954358736568, 1.5858770571647565, 1.5888516144883367, 1.5503642610316821, 1.5337500874021737, 1.5330458666617115, 1.5329463837218789, 1.5327909742957304, 1.5327174510021937, 2.8043412359288173, 2.1339058999327887, 1.4711246099287454, 1.4710283945629152, 1.4687522571999223, 1.4678304079310864, 1.4673746349855477, 1.4672078111780218, 1.4676050973027566, 1.4667900698180316, 1.4668232527552234, 1.4664903627414771, 1.466439603453992, 1.4662305054936051, 1.464682271465176, 1.4511072682282382, 0.8024056196908922, 0.8024050893699781, 0.8024051651301087, 0.8024044075288029, 0.8024042560085418, 0.8024037256876277, 0.8024035741673666, 0.802403498407236, 0.8024033468869749, 0.802402437765408, 0.8023697851491302, 0.8023695578687384, 0.8023690275478245, 0.8023680426661269, 2.127400680560842, 1.469768352071193, 1.4591754190939457, 1.4689115049943906, 1.4676475229758785, 1.4619515733187334, 1.4568235216003775, 1.4695445566454748, 1.4680237477843137, 1.4675197913957292, 1.4670253807835973, 2.139232594713512, 2.108200336108455, 3.455900480831319, 6.139128238577312, 4.12703584043386, 2.7838529692566563, 4.121918395133784, 2.138618937655855, 2.1188026633418935, 2.135299280254323, 2.145477199236281, 1.4732740763534173, 1.4705285292213819, 1.4692925784511826, 1.4682826959106188, 1.467578126696272, 1.3190041405186177, 1.3167303778912325, 1.3169482152428036, 1.316807007523257, 1.3150407740907637, 1.3146809331629337, 1.3140996166814185, 1.3143691847192969, 1.313352989391352, 1.3102856439278674, 0.7201292976733406, 0.7201292976733406, 0.7201288428980763, 0.7201285586635362, 0.7201285018166281, 0.720128217582088, 0.720127990194456, 0.720127990194456, 0.7201276491130078, 0.7201275354191917, 0.7201275354191917, 0.7201269101032033, 0.7201268532562953, 0.7201266258686632, 0.7201265121748471, 0.7201256026243187, 0.7201257163181347, 0.7201254889305025, 0.7201230445134573, 0.7201226465851011, 2.5146577670162884, 2.510542050874914, 1.318885557868467, 1.3149893844859046, 2.521269517195446, 1.9164254647724748, 3.111583727313572, 1.3115878928970195, 1.3197569072747515, 4.306271979619076, 1.3185426573192298, 1.3136763346042268, 1.919881984168333, 1.318018642521009, 1.3158732402119555, 1.3151652688193496, 1.9050471013549992, 1.319538387760284, 1.319595234668314, 1.3159302008138016, 1.9209657136230176, 1.31961410784178, 1.3223935805630005, 1.3217352933680127, 1.3386618013154037, 1.6006630055382205, 1.5975049873711036, 1.5974532116178364, 1.1004466287606658, 1.0985445510880072, 1.0948069321273712, 0.600231841589571, 0.6002316523507067, 0.6002315388073881, 0.6002314631118424, 0.6002314252640696, 0.6002312360252053, 0.6002311981774324, 0.6002310089385682, 0.6002310089385682, 0.6002310089385682, 0.6002309332430225, 0.6002294950276539, 0.5991897410117498, 0.5991822093049515, 0.5991297522917728, 0.5990906933901853, 0.5990685145952915, 0.5990658274034188, 0.5990289636726569, 0.5990203722282186, 0.598999215323192, 0.5989843033006866, 0.5990047032502561, 0.598971056580187, 1.0989433152228196, 3.0986595368624887, 1.097434854387865, 1.5954733189241856, 1.10131160176152, 1.0976464991336765, 1.0981071822248816, 1.0986256210174652, 1.096216685970717, 0.6045383504240078, 0.6021991688226336, 0.6014116323650428, 0.6012013122912813, 0.6006279563802834, 0.6003442494749538, 0.6003193077926415, 0.6002643906742273, 0.600260265266986, 0.6002566318807918, 0.6002343016948066, 0.6002320686762082], \"Total\": [11.0, 5.0, 6.0, 5.0, 6.0, 20.0, 4.0, 2.0, 2.0, 2.0, 5.0, 4.0, 8.0, 3.0, 5.0, 3.0, 7.0, 3.0, 2.0, 3.0, 3.0, 1.0, 1.0, 3.0, 10.0, 1.0, 9.0, 5.0, 6.0, 2.0, 5.734744348987854, 5.004599818860409, 3.55185465092414, 3.55147003997888, 3.550510215694317, 3.5507556442705144, 6.2357595947498785, 2.8235085464763165, 2.8230061830286983, 2.8230069765051127, 2.095277303001287, 2.095277088490306, 2.095275252973665, 2.095214390163556, 2.095189783447524, 2.09517797776581, 2.0951694454093763, 2.0949132563179127, 2.0948688340377024, 2.0951643126502737, 2.0947863807322236, 2.0945603877188366, 2.0943317664980112, 2.0942143504458843, 2.0941682717840795, 2.094752325100801, 2.0943612300793535, 2.094312981145026, 2.094271865900002, 2.094102226401405, 9.85063911514948, 8.242981387790762, 4.723075371199083, 25.353152809840047, 9.693197135730344, 3.425029433668747, 3.4214607361118237, 3.42381075318237, 3.491412336801848, 3.5194173713548205, 8.880282690356108, 18.40663084259611, 5.853961320189904, 3.92348999670046, 10.724999301571174, 3.992607033221741, 4.022932309721983, 3.9912588222275565, 4.214738268893353, 4.159774508956826, 5.932347901861541, 20.85379105298391, 4.619423330473386, 6.850817129160652, 9.553443398990655, 7.546221221455171, 3.4244896639984437, 2.7280724210683065, 6.241844931774118, 2.7319604967498052, 2.031618158803529, 2.0316175909535064, 2.031617247212636, 2.0316173079685464, 2.031617114612496, 2.0316163204000497, 2.0316155081876177, 2.031614529460989, 2.031616912762842, 2.0316169216833955, 2.0315784380865796, 2.0316716922933935, 2.031693308152007, 2.0315953904760162, 2.0317089506446644, 2.0316796686240157, 2.031745588398286, 2.031776525909722, 2.031751087862419, 2.031986034668841, 2.0309717153107747, 2.035391629982777, 4.15306057074086, 3.9172477865517292, 1.3351575661250725, 1.3351577318132664, 2.5321714714584207, 2.5320274310759325, 6.676509932584191, 3.9285057722477568, 20.85379105298391, 4.124670125706867, 25.353152809840047, 6.488476918781336, 7.546221221455171, 18.40663084259611, 11.190389728861645, 9.553443398990655, 9.693197135730344, 4.741524079709205, 5.96544034875147, 6.850817129160652, 7.540462184292833, 10.724999301571174, 4.159275310280001, 9.85063911514948, 3.8701414368773666, 2.7002264259732036, 5.002156648156571, 3.3687263145346567, 3.4285147587213363, 2.6318225628653926, 3.313826517233911, 2.64453319144563, 1.9761548093730283, 1.9761297434010014, 1.9759007297238245, 1.9761991162488568, 1.975779695579336, 1.97574909238881, 1.9763119451484659, 1.9757090680329137, 1.976536590907794, 1.9761225772929745, 1.9762139277246404, 1.9765017422739468, 1.9767050202808603, 1.9772166517807934, 1.3074305907916546, 1.3074305652883385, 1.3074306947612049, 1.3074304467607485, 1.3074303867464814, 1.3074302702756064, 1.307430053292029, 1.3074302724883207, 1.3074302407783849, 1.3074302243378204, 1.3074214027266209, 1.307421253558008, 1.307421197356685, 1.3074209714849243, 3.8743961186901275, 2.576608912907036, 2.5764709864767905, 2.672315244551395, 2.672382064208275, 2.673360145442077, 2.6731003355296505, 2.704511937051796, 2.704163320706657, 2.703831777468018, 2.7043238765219875, 4.796843027380916, 4.741524079709205, 9.553443398990655, 25.353152809840047, 18.40663084259611, 8.880282690356108, 20.85379105298391, 5.96544034875147, 7.540462184292833, 8.242981387790762, 11.190389728861645, 3.273883107713838, 4.159774508956826, 4.532622349830509, 3.176533899804721, 6.850817129160652, 1.8392288181901764, 1.8384277291809457, 1.8395913551341183, 1.8396524726556778, 1.8397552876020398, 1.8398221735724785, 1.8393769419880364, 1.8399778444537565, 1.8401508701507563, 1.840896534488112, 1.2388688243796904, 1.238868835512964, 1.2388686047281332, 1.2388685552430383, 1.238868775202969, 1.2388687504876386, 1.2388687153042386, 1.23886889555396, 1.2388687051399634, 1.238868742730839, 1.2388687843585784, 1.2388686696497677, 1.238868590500558, 1.23886855820876, 1.238868674705858, 1.2388684121710372, 1.2388686518280696, 1.2388686098030497, 1.238868573654589, 1.2388686462721354, 4.43668525865729, 4.439328938493201, 2.339958797571492, 2.3404857687178855, 5.998431620247918, 5.002156648156571, 10.724999301571174, 3.0097325976970275, 3.176533899804721, 20.85379105298391, 3.204386041749288, 3.205097098577432, 5.853961320189904, 3.263970767059303, 3.263844969804994, 3.7666461014729475, 7.540462184292833, 3.9285057722477568, 3.960278667770058, 4.022932309721983, 18.40663084259611, 4.660880381283817, 5.932347901861541, 5.830251872305886, 25.353152809840047, 2.139394225317638, 2.1400244914385667, 2.14047851889478, 1.6391761368700501, 1.640043086484861, 1.641746066731703, 1.13895555834053, 1.1389555613990112, 1.1389555160257652, 1.1389555798539792, 1.1389555190712777, 1.1389556118224506, 1.1389557615038883, 1.1389556708905912, 1.1389557832257036, 1.1389557987250867, 1.1389556764311308, 1.1389556776071053, 1.139163182332005, 1.1391645811542745, 1.139175047008419, 1.1391827860014074, 1.139187282485676, 1.1392240938669218, 1.1391951367049398, 1.1392393461276926, 1.139201071497624, 1.1392038588111255, 1.13925427367341, 1.139206690141083, 2.3677061093750584, 11.190389728861645, 3.0967564237887153, 5.998431620247918, 3.6363503913666473, 3.767081214081699, 6.488476918781336, 7.540462184292833, 25.353152809840047, 4.619423330473386, 6.2357595947498785, 3.2697886148404427, 8.880282690356108, 3.1332375615114065, 5.96544034875147, 1.8353450564945377, 4.532622349830509, 5.932347901861541, 1.8668786734897558, 3.8701414368773666, 1.8353483993701678], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.4396, -4.6003, -4.9773, -4.9788, -4.9802, -4.9838, -4.4394, -5.2494, -5.2507, -5.2511, -5.6239, -5.6239, -5.6239, -5.6241, -5.6242, -5.6246, -5.6246, -5.6248, -5.6248, -5.6248, -5.625, -5.6253, -5.6258, -5.626, -5.6263, -5.6262, -5.6276, -5.6278, -5.628, -5.6288, -4.2034, -4.4392, -4.9748, -3.6906, -4.4365, -5.2458, -5.2524, -5.2538, -5.2501, -5.25, -4.5848, -4.1135, -4.9792, -5.2505, -4.672, -5.2492, -5.2496, -5.2609, -5.2501, -5.2576, -5.1043, -4.7608, -5.2485, -5.2464, -5.2473, -5.2478, -4.8516, -5.1233, -4.3131, -5.1628, -5.498, -5.498, -5.498, -5.498, -5.498, -5.498, -5.498, -5.498, -5.498, -5.498, -5.4986, -5.4988, -5.4991, -5.4993, -5.4993, -5.4997, -5.4998, -5.5003, -5.5008, -5.5037, -5.5081, -5.5543, -4.8554, -4.9275, -6.1042, -6.1042, -5.4993, -5.4996, -4.6493, -5.1235, -3.7816, -5.1226, -4.1834, -4.8947, -4.8434, -4.4197, -4.6649, -4.8466, -4.8531, -5.1722, -5.1588, -5.1238, -5.1183, -5.2723, -5.4635, -5.4616, -5.4862, -5.4969, -5.4974, -5.4975, -5.4976, -5.4976, -4.7538, -5.0271, -5.399, -5.3991, -5.4006, -5.4012, -5.4015, -5.4017, -5.4014, -5.4019, -5.4019, -5.4021, -5.4022, -5.4023, -5.4034, -5.4127, -6.0052, -6.0052, -6.0052, -6.0052, -6.0052, -6.0052, -6.0052, -6.0052, -6.0052, -6.0052, -6.0052, -6.0052, -6.0052, -6.0052, -5.0301, -5.3999, -5.4071, -5.4005, -5.4014, -5.4052, -5.4088, -5.4001, -5.4011, -5.4014, -5.4018, -5.0246, -5.0392, -4.5449, -3.9703, -4.3675, -4.7612, -4.3687, -5.0249, -5.0342, -5.0264, -5.0217, -5.3975, -5.3994, -5.4002, -5.4009, -5.4014, -5.2209, -5.2227, -5.2225, -5.2226, -5.2239, -5.2242, -5.2247, -5.2244, -5.2252, -5.2276, -5.8261, -5.8261, -5.8261, -5.8261, -5.8261, -5.8261, -5.8261, -5.8261, -5.8261, -5.8261, -5.8261, -5.8261, -5.8261, -5.8261, -5.8261, -5.8261, -5.8261, -5.8261, -5.8261, -5.8261, -4.5757, -4.5773, -5.221, -5.224, -4.573, -4.8473, -4.3627, -5.2266, -5.2204, -4.0377, -5.2213, -5.225, -4.8455, -5.2217, -5.2233, -5.2238, -4.8533, -5.2205, -5.2205, -5.2233, -4.845, -5.2205, -5.2184, -5.2189, -5.2061, -4.6206, -4.6226, -4.6226, -4.9953, -4.997, -5.0004, -5.6015, -5.6015, -5.6015, -5.6015, -5.6015, -5.6015, -5.6015, -5.6015, -5.6015, -5.6015, -5.6015, -5.6015, -5.6032, -5.6032, -5.6033, -5.6034, -5.6034, -5.6034, -5.6035, -5.6035, -5.6035, -5.6035, -5.6035, -5.6036, -4.9967, -3.96, -4.998, -4.6238, -4.9945, -4.9978, -4.9974, -4.997, -4.9992, -5.5943, -5.5982, -5.5995, -5.5998, -5.6008, -5.6013, -5.6013, -5.6014, -5.6014, -5.6014, -5.6015, -5.6015], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.1599, 1.1353, 1.1013, 1.0998, 1.0987, 1.095, 1.0763, 1.0587, 1.0575, 1.0571, 0.9824, 0.9824, 0.9824, 0.9822, 0.9821, 0.9818, 0.9818, 0.9817, 0.9817, 0.9816, 0.9815, 0.9813, 0.981, 0.9808, 0.9806, 0.9804, 0.9791, 0.979, 0.9788, 0.9781, 0.8551, 0.7975, 0.8188, 0.4225, 0.6381, 0.8691, 0.8635, 0.8615, 0.8456, 0.8377, 0.5774, 0.3198, 0.5997, 0.7285, 0.3015, 0.7124, 0.7044, 0.701, 0.6573, 0.663, 0.4612, -0.4524, 0.5673, 0.1752, -0.1582, 0.0772, 1.2634, 1.2191, 1.2016, 1.1782, 1.1392, 1.1392, 1.1392, 1.1392, 1.1392, 1.1392, 1.1392, 1.1392, 1.1392, 1.1392, 1.1386, 1.1384, 1.1381, 1.1379, 1.1378, 1.1374, 1.1373, 1.1368, 1.1363, 1.1333, 1.1294, 1.081, 1.0667, 1.0531, 0.9528, 0.9528, 0.9176, 0.9174, 0.7981, 0.8542, 0.5269, 0.8064, -0.0703, 0.5812, 0.4816, 0.0136, 0.2661, 0.2425, 0.2215, 0.6175, 0.4012, 0.2979, 0.2074, -0.2989, 0.4572, -0.4032, 0.5066, 0.8557, 0.2387, 0.634, 0.6163, 0.8807, 1.3941, 1.3465, 1.2659, 1.2658, 1.2644, 1.2636, 1.2635, 1.2634, 1.2634, 1.2631, 1.2627, 1.2627, 1.2626, 1.2624, 1.2612, 1.2516, 1.0728, 1.0728, 1.0728, 1.0728, 1.0728, 1.0728, 1.0728, 1.0728, 1.0728, 1.0728, 1.0728, 1.0728, 1.0728, 1.0727, 0.9615, 0.9996, 0.9924, 0.9626, 0.9617, 0.9574, 0.954, 0.951, 0.9501, 0.9499, 0.9494, 0.7535, 0.7505, 0.5442, 0.1428, 0.0658, 0.401, -0.0602, 0.5352, 0.2916, 0.2102, -0.0907, 0.7625, 0.5212, 0.4345, 0.7893, 0.0202, 1.5157, 1.5144, 1.514, 1.5138, 1.5124, 1.5121, 1.5119, 1.5118, 1.5109, 1.5082, 1.3057, 1.3057, 1.3057, 1.3057, 1.3057, 1.3057, 1.3057, 1.3057, 1.3057, 1.3057, 1.3057, 1.3057, 1.3057, 1.3057, 1.3057, 1.3057, 1.3057, 1.3057, 1.3057, 1.3057, 1.2804, 1.2782, 1.2749, 1.2717, 0.9815, 0.8888, 0.6108, 1.0176, 0.9699, 0.2707, 0.9602, 0.9563, 0.7333, 0.9414, 0.9398, 0.796, 0.4724, 0.7572, 0.7492, 0.7307, -0.4117, 0.5863, 0.3472, 0.3641, -1.093, 1.9649, 1.9626, 1.9624, 1.8565, 1.8543, 1.8498, 1.6144, 1.6144, 1.6144, 1.6144, 1.6144, 1.6144, 1.6144, 1.6144, 1.6144, 1.6144, 1.6144, 1.6144, 1.6125, 1.6125, 1.6124, 1.6123, 1.6123, 1.6123, 1.6122, 1.6122, 1.6122, 1.6121, 1.6121, 1.6121, 1.4874, 0.9709, 1.2176, 0.9307, 1.0605, 1.0219, 0.4786, 0.3288, -0.886, 0.2214, -0.0825, 0.5618, -0.4377, 0.6031, -0.0412, 1.1375, 0.2333, -0.0358, 1.1203, 0.3913, 1.1373]}, \"token.table\": {\"Topic\": [5, 3, 5, 1, 3, 5, 3, 2, 4, 4, 4, 2, 3, 4, 5, 1, 3, 4, 5, 1, 4, 1, 2, 3, 1, 2, 3, 1, 2, 4, 5, 1, 3, 5, 4, 4, 4, 3, 4, 2, 4, 2, 4, 1, 2, 1, 2, 3, 5, 2, 2, 1, 3, 3, 5, 1, 3, 5, 1, 5, 4, 3, 2, 1, 3, 5, 2, 4, 1, 3, 1, 2, 2, 1, 4, 4, 1, 3, 5, 1, 2, 1, 2, 2, 3, 1, 2, 4, 2, 5, 2, 3, 3, 4, 3, 4, 2, 1, 4, 5, 1, 4, 1, 5, 5, 5, 4, 1, 2, 4, 5, 1, 5, 2, 1, 5, 1, 1, 3, 5, 4, 2, 5, 1, 2, 4, 5, 3, 4, 3, 4, 2, 4, 1, 3, 1, 1, 5, 1, 2, 4, 5, 2, 3, 1, 4, 4, 5, 2, 5, 1, 2, 5, 2, 5, 5, 1, 1, 4, 5, 2, 1, 2, 3, 4, 5, 2, 2, 3, 4, 5, 1, 2, 5, 5, 2, 1, 3, 5, 1, 2, 3, 1, 1, 1, 2, 3, 4, 3, 1, 1, 2, 3, 4, 5, 3, 2, 4, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 1, 2, 4, 5, 1, 2, 4, 5, 3, 4, 1, 3, 1, 2, 3, 5, 5, 2, 3, 2, 2, 3, 4, 3, 1, 3, 3, 4, 5, 3, 1, 5, 5, 2, 1, 4, 5, 2, 3, 3, 1, 5, 1, 2, 4, 1, 2, 3, 4, 5, 5, 5, 2, 3, 5, 4, 1, 3, 2, 5, 2, 3, 1, 3, 4, 1, 2, 3, 4, 3, 1, 5, 1, 3, 3, 4, 5, 5, 2, 4, 2, 1, 5, 1, 2, 1, 4, 5, 1, 4, 1, 2, 3, 5, 4, 5, 5, 3, 1, 4, 2, 1, 3, 2, 3, 2, 3, 5, 4, 2, 3, 4, 4, 2, 3, 4, 1, 1, 2, 3, 4, 5, 1, 4, 1, 3, 1, 2, 3, 4, 5, 1, 2, 3, 2, 1, 2, 3, 4, 4, 5, 3, 5, 1, 1, 1, 2, 4, 3, 2, 2, 1, 4, 4, 5, 1, 2, 3, 4, 3, 1, 3, 4, 5, 2, 3, 5, 3, 1, 2, 3, 4, 1, 2, 3, 5, 3, 3, 4], \"Freq\": [0.8779972923384143, 0.7648592729546495, 0.8778282166784589, 0.5010950402068344, 0.2505475201034172, 0.2505475201034172, 0.7648590427716984, 0.5090994174244698, 0.2545497087122349, 0.5434337022149026, 0.8071881005678043, 0.39982754253348973, 0.19991377126674487, 0.39982754253348973, 0.19991377126674487, 0.4412456731752177, 0.22062283658760884, 0.22062283658760884, 0.22062283658760884, 0.584545652940275, 0.2922728264701375, 0.24078627868931154, 0.7223588360679347, 0.764859163273327, 0.24244363052636228, 0.48488726105272456, 0.24244363052636228, 0.43295447438350393, 0.21647723719175196, 0.21647723719175196, 0.21647723719175196, 0.6065766456061442, 0.2426306582424577, 0.12131532912122885, 0.8071881241204296, 0.5437061392850576, 0.8071879928010663, 0.7648645859368723, 0.5435809289329654, 0.7489752710627442, 0.8071880384507922, 0.2252592709067016, 0.6757778127201047, 0.5682758789219956, 0.2841379394609978, 0.9549671028188622, 0.984456205335433, 0.7648588976891387, 0.8779973421439078, 0.9844373996844068, 0.7320750070798557, 0.36980014940025074, 0.36980014940025074, 0.5060396481250055, 0.6097400783191137, 0.2581047392588459, 0.5162094785176918, 0.2581047392588459, 0.9545285265797665, 0.8778060153725262, 0.8071880582802111, 0.506129302896182, 0.9844372331218755, 0.9550120786700631, 0.764864333652872, 0.8778362815553271, 0.7599296503570147, 0.3799648251785073, 0.9549585371300814, 0.7648590778806077, 0.9545790694907857, 0.9844378487795254, 0.984437370244615, 0.954985850960907, 0.5432135816791379, 0.8071881361165247, 0.5009258320086029, 0.25046291600430143, 0.25046291600430143, 0.4745253138874344, 0.4745253138874344, 0.8446291571135786, 0.9844071537884359, 0.37420734774421094, 0.37420734774421094, 0.2525074834097665, 0.505014966819533, 0.2525074834097665, 0.7898359264146069, 0.39491796320730344, 0.3741979911454996, 0.3741979911454996, 0.7562771405061106, 0.8071880318282331, 0.31480854023357835, 0.31480854023357835, 0.9844005450897336, 0.509750248294743, 0.2548751241473715, 0.2548751241473715, 0.7992647054267039, 0.8071881131443936, 0.645837039244148, 0.322918519622074, 0.877997198064552, 0.877997383372017, 0.543598988552049, 0.29955770607621723, 0.5991154121524345, 0.14977885303810862, 0.14977885303810862, 0.4223497147895369, 0.4223497147895369, 0.9844479906657924, 0.9550632126669794, 0.87799729143188, 0.9545767309570605, 0.6351793617975585, 0.2117264539325195, 0.2117264539325195, 0.807188013958363, 0.5448566722978918, 0.5448566722978918, 0.9545674648666459, 0.638317382814485, 0.3191586914072425, 0.3191586914072425, 0.7648591460171714, 0.8071880768062762, 0.3881277938889021, 0.3881277938889021, 0.9844375574223916, 0.8071879143860903, 0.5728340874890792, 0.2864170437445396, 0.9545728435599046, 0.7083385678063419, 0.8779973667877424, 0.5057019665112046, 0.16856732217040155, 0.16856732217040155, 0.16856732217040155, 0.421805302763043, 0.421805302763043, 0.9545276903882172, 0.543530790292764, 0.4273579522160143, 0.4273579522160143, 0.765843817769015, 0.2552812725896717, 0.26545751025008624, 0.26545751025008624, 0.26545751025008624, 0.5448556799042447, 0.5448556799042447, 0.877997415991599, 0.8718784475340158, 0.5841444355944722, 0.2920722177972361, 0.8778187883365648, 0.984437561744923, 0.34303835302556024, 0.17151917651278012, 0.17151917651278012, 0.17151917651278012, 0.17151917651278012, 0.49130594096452185, 0.265235731062494, 0.265235731062494, 0.265235731062494, 0.132617865531247, 0.9550330921097114, 0.8760429419714445, 0.9348440676954047, 0.9345687434892676, 0.984436957965492, 0.6116603351429784, 0.3058301675714892, 0.3058301675714892, 0.4808529974096283, 0.4808529974096283, 0.24042649870481414, 0.954527592665271, 0.9548543034264955, 0.3729603972481392, 0.1864801986240696, 0.0932400993120348, 0.27972029793610437, 0.5060414832008383, 0.9545562541902343, 0.433871088243143, 0.23665695722353255, 0.23665695722353255, 0.03944282620392209, 0.03944282620392209, 0.505935485636881, 0.9844382423444771, 0.807188073186293, 0.5436623549924353, 0.8778373594842478, 0.9546934671248702, 0.3054476800481441, 0.3054476800481441, 0.3054476800481441, 0.291935978189661, 0.291935978189661, 0.1459679890948305, 0.1459679890948305, 0.4169408897860064, 0.2084704448930032, 0.4169408897860064, 0.3082387477114798, 0.4623581215672197, 0.1541193738557399, 0.1541193738557399, 0.2654881751723237, 0.2654881751723237, 0.2654881751723237, 0.2654881751723237, 0.7648589734319556, 0.5439430574981149, 0.8447206272977149, 0.5058923763232589, 0.5630451388028387, 0.11260902776056773, 0.3378270832817032, 0.11260902776056773, 0.877997413643876, 0.7331183675896715, 0.5061473959805406, 0.9843752148007308, 0.3120030280654666, 0.3120030280654666, 0.3120030280654666, 0.7648644537978868, 0.36984549421060586, 0.36984549421060586, 0.388107017324472, 0.388107017324472, 0.8778081631238053, 0.7648591447227125, 0.7084646190375234, 0.8779972267575961, 0.8779972100127024, 0.9843929660128715, 0.5124734920357621, 0.3416489946905081, 0.17082449734525404, 0.5936961965033578, 0.2968480982516789, 0.5061371425411789, 0.8018269344779887, 0.16036538689559773, 0.306375292968986, 0.306375292968986, 0.306375292968986, 0.20934859992066368, 0.3140228998809955, 0.3140228998809955, 0.10467429996033184, 0.8778127361853438, 0.8778222531873515, 0.8777786717067215, 0.9843602258887729, 0.5060218840185298, 0.8777671702521698, 0.8071881341842009, 0.48079529207499117, 0.24039764603749558, 0.789880858103556, 0.394940429051778, 0.3740610862718746, 0.3740610862718746, 0.22539349575197004, 0.22539349575197004, 0.6761804872559101, 0.4291034818295658, 0.2145517409147829, 0.2145517409147829, 0.2145517409147829, 0.5060180914479098, 0.9549451027243384, 0.6100625658871933, 0.3697781943507792, 0.3697781943507792, 0.33225543052069645, 0.33225543052069645, 0.33225543052069645, 0.8779973810142968, 0.9844374639369354, 0.8071880615745197, 0.7489751781176509, 0.5356534488289484, 0.5356534488289484, 0.16020904250752835, 0.8010452125376417, 0.3334204883238027, 0.500130732485704, 0.3334204883238027, 0.4971498016923422, 0.2485749008461711, 0.16763221850157195, 0.3352644370031439, 0.3352644370031439, 0.16763221850157195, 0.5434848050014837, 0.8778038337153348, 0.8777904236607682, 0.7648591728912142, 0.5839365876215797, 0.29196829381078987, 0.9847502970734207, 0.3697524815106218, 0.3697524815106218, 0.9843725503325194, 0.5060982998572868, 0.5167769789865109, 0.25838848949325544, 0.25838848949325544, 0.5435505508471252, 0.3120722618845561, 0.3120722618845561, 0.3120722618845561, 0.8071879868357066, 0.740678626341185, 0.3703393131705925, 0.8071880089044009, 0.9547512903444162, 0.1787247851468219, 0.3574495702936438, 0.1787247851468219, 0.08936239257341096, 0.2680871777202328, 0.9547668123026232, 0.807187960759854, 0.8449489841598266, 0.5057614698416298, 0.3802977339992497, 0.2716412385708926, 0.2173129908567141, 0.10865649542835705, 0.054328247714178525, 0.2916714876190149, 0.5833429752380298, 0.2916714876190149, 0.9842587330212366, 0.7106137904529027, 0.20303251155797222, 0.10151625577898611, 0.10151625577898611, 0.4272617306055223, 0.4272617306055223, 0.5059929949089476, 0.8779972966095013, 0.9547137116671645, 0.8448905812036906, 0.30638710148654746, 0.30638710148654746, 0.30638710148654746, 0.7648644209191232, 0.9844387165958216, 0.9844110185648933, 0.7084644199058988, 0.8071881038743796, 0.8071879535059429, 0.9343705075034741, 0.19181164661317784, 0.43157620487965015, 0.19181164661317784, 0.19181164661317784, 0.5059444060239022, 0.27500100165654573, 0.27500100165654573, 0.27500100165654573, 0.27500100165654573, 0.3740974428488331, 0.3740974428488331, 0.6091075960308189, 0.7648589585122801, 0.26503331154852244, 0.3975499673227837, 0.13251665577426122, 0.13251665577426122, 0.5158256795964018, 0.30949540775784107, 0.10316513591928037, 0.10316513591928037, 0.5060332294094249, 0.9052978435648872, 0.8071882293354823], \"Term\": [\"able\", \"adoption\", \"affect\", \"aim\", \"aim\", \"aim\", \"alarm\", \"also\", \"also\", \"amount\", \"annotate\", \"application\", \"application\", \"application\", \"application\", \"apply\", \"apply\", \"apply\", \"apply\", \"arabic\", \"arabic\", \"attention\", \"attention\", \"audio\", \"author\", \"author\", \"author\", \"available\", \"available\", \"available\", \"available\", \"base\", \"base\", \"base\", \"billel\", \"biomedical\", \"block\", \"bryant\", \"building\", \"cat\", \"characbyorigin\", \"classification\", \"classification\", \"classifier\", \"classifier\", \"clinical\", \"code\", \"cold\", \"commodity\", \"computer\", \"confound\", \"consist\", \"consist\", \"constrain\", \"content\", \"context\", \"context\", \"context\", \"contextual\", \"contextualize\", \"corlinguistic\", \"counterexample\", \"course\", \"czech\", \"database\", \"deep\", \"dependency\", \"dependency\", \"detection\", \"device\", \"discuss\", \"distance\", \"distribute\", \"diverse\", \"domain\", \"easily\", \"efficient\", \"efficient\", \"efficient\", \"embed\", \"embed\", \"emotion\", \"encode\", \"error\", \"error\", \"evaluate\", \"evaluate\", \"evaluate\", \"evaluation\", \"evaluation\", \"even\", \"even\", \"exceed\", \"experimental\", \"explain\", \"explain\", \"expression\", \"extraction\", \"extraction\", \"extraction\", \"feature\", \"filter\", \"focus\", \"focus\", \"follow\", \"formal\", \"forum\", \"framework\", \"framework\", \"framework\", \"framework\", \"free\", \"free\", \"gain\", \"globally\", \"grammatical\", \"graph\", \"ground\", \"ground\", \"ground\", \"gulfarabic\", \"heavy\", \"heavy\", \"hide\", \"high\", \"high\", \"high\", \"hot\", \"houda\", \"human\", \"human\", \"hybrid\", \"illegal\", \"improve\", \"improve\", \"imputation\", \"indian\", \"industrial\", \"information\", \"information\", \"information\", \"information\", \"input\", \"input\", \"intent\", \"interact\", \"interaction\", \"interaction\", \"interpretation\", \"interpretation\", \"introduce\", \"introduce\", \"introduce\", \"involve\", \"involve\", \"jargon\", \"key\", \"label\", \"label\", \"labeling\", \"language\", \"large\", \"large\", \"large\", \"large\", \"large\", \"latent\", \"learn\", \"learn\", \"learn\", \"learn\", \"learning\", \"lexical\", \"lithium\", \"lm\", \"lolcode\", \"machine\", \"machine\", \"machine\", \"make\", \"make\", \"make\", \"memory\", \"mention\", \"method\", \"method\", \"method\", \"method\", \"minimal\", \"mix\", \"model\", \"model\", \"model\", \"model\", \"model\", \"modification\", \"module\", \"morphologically\", \"motivate\", \"narrow\", \"national\", \"need\", \"need\", \"need\", \"network\", \"network\", \"network\", \"network\", \"neural\", \"neural\", \"neural\", \"new\", \"new\", \"new\", \"new\", \"novel\", \"novel\", \"novel\", \"novel\", \"ofdotype\", \"offensive\", \"organize\", \"output\", \"paper\", \"paper\", \"paper\", \"paper\", \"par\", \"parallel\", \"path\", \"pattern\", \"perform\", \"perform\", \"perform\", \"phrasebase\", \"plan\", \"plan\", \"play\", \"play\", \"pluginandplay\", \"pm\", \"pos\", \"posrengle\", \"practical\", \"precision\", \"present\", \"present\", \"present\", \"pretraine\", \"pretraine\", \"probability\", \"project\", \"project\", \"property\", \"property\", \"property\", \"propose\", \"propose\", \"propose\", \"propose\", \"prune\", \"quality\", \"quantity\", \"re\", \"read\", \"readily\", \"removal\", \"represent\", \"represent\", \"representation\", \"representation\", \"require\", \"require\", \"research\", \"research\", \"research\", \"result\", \"result\", \"result\", \"result\", \"retrieval\", \"review\", \"rich\", \"rnn\", \"rnn\", \"robot\", \"robot\", \"robot\", \"run\", \"science\", \"scoring\", \"seek\", \"selection\", \"selection\", \"semantic\", \"semantic\", \"sentence\", \"sentence\", \"sentence\", \"sentiment\", \"sentiment\", \"set\", \"set\", \"set\", \"set\", \"several\", \"shallowand\", \"skill\", \"smart\", \"social\", \"social\", \"source\", \"speak\", \"speak\", \"special\", \"state\", \"stateoftheart\", \"stateoftheart\", \"stateoftheart\", \"strategy\", \"study\", \"study\", \"study\", \"subdialect\", \"summarization\", \"summarization\", \"supervised\", \"suppose\", \"system\", \"system\", \"system\", \"system\", \"system\", \"table\", \"tackle\", \"tag\", \"take\", \"task\", \"task\", \"task\", \"task\", \"task\", \"term\", \"term\", \"term\", \"test\", \"text\", \"text\", \"text\", \"text\", \"textual\", \"textual\", \"thesis\", \"throughput\", \"tool\", \"topic\", \"traditional\", \"traditional\", \"traditional\", \"transducer\", \"tree\", \"type\", \"understand\", \"uniblock\", \"unsupervise\", \"usage\", \"use\", \"use\", \"use\", \"use\", \"useful\", \"user\", \"user\", \"user\", \"user\", \"vector\", \"vector\", \"visual\", \"widespread\", \"word\", \"word\", \"word\", \"word\", \"work\", \"work\", \"work\", \"work\", \"worsen\", \"write\", \"yadac\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [5, 1, 2, 4, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1553221182940891683359494335\", ldavis_el1553221182940891683359494335_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1553221182940891683359494335\", ldavis_el1553221182940891683359494335_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1553221182940891683359494335\", ldavis_el1553221182940891683359494335_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "4     -0.128538 -0.032950       1        1  28.629359\n",
       "0      0.090442 -0.081762       2        1  24.138522\n",
       "1      0.007779 -0.040786       3        1  20.992728\n",
       "3      0.039764  0.100500       4        1  15.751975\n",
       "2     -0.009447  0.054998       5        1  10.487416, topic_info=              Term       Freq      Total Category  logprob  loglift\n",
       "15          system  11.000000  11.000000  Default  30.0000  30.0000\n",
       "287       sentence   5.000000   5.000000  Default  29.0000  29.0000\n",
       "84        semantic   6.000000   6.000000  Default  28.0000  28.0000\n",
       "95             key   5.000000   5.000000  Default  27.0000  27.0000\n",
       "101        project   6.000000   6.000000  Default  26.0000  26.0000\n",
       "..             ...        ...        ...      ...      ...      ...\n",
       "19           apply   0.600264   4.532622   Topic5  -5.6014   0.2333\n",
       "201    information   0.600260   5.932348   Topic5  -5.6014  -0.0358\n",
       "286      selection   0.600257   1.866879   Topic5  -5.6014   1.1203\n",
       "171  stateoftheart   0.600234   3.870141   Topic5  -5.6015   0.3913\n",
       "161        involve   0.600232   1.835348   Topic5  -5.6015   1.1373\n",
       "\n",
       "[305 rows x 6 columns], token_table=      Topic      Freq      Term\n",
       "term                           \n",
       "290       5  0.877997      able\n",
       "0         3  0.764859  adoption\n",
       "332       5  0.877828    affect\n",
       "91        1  0.501095       aim\n",
       "91        3  0.250548       aim\n",
       "...     ...       ...       ...\n",
       "90        3  0.103165      work\n",
       "90        5  0.103165      work\n",
       "17        3  0.506033    worsen\n",
       "376       3  0.905298     write\n",
       "435       4  0.807188     yadac\n",
       "\n",
       "[390 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[5, 1, 2, 4, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84f0b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e031b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dglover\\AppData\\Local\\Temp\\ipykernel_15532\\1533496723.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "199dffc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>model, task, use, propose, write, paper, syste...</td>\n",
       "      <td>[paper, audio, play, hot, cold, set, alarm, pm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>model, task, use, propose, write, paper, syste...</td>\n",
       "      <td>[state, transducer, efficient, way, represent,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9957</td>\n",
       "      <td>use, model, semantic, task, framework, system,...</td>\n",
       "      <td>[quantification, semantic, similarity, word, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>model, task, text, work, base, project, key, p...</td>\n",
       "      <td>[paper, focus, aim, semantic, role, label, lab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9883</td>\n",
       "      <td>use, model, semantic, task, framework, system,...</td>\n",
       "      <td>[also, new, level, incorporating, advance, wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>use, method, sentence, research, classificatio...</td>\n",
       "      <td>[arabic, popular, msa, arabicdialectad, berevi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>model, task, text, work, base, project, key, p...</td>\n",
       "      <td>[efficient, encoding, context, speak, computat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9937</td>\n",
       "      <td>use, model, semantic, task, framework, system,...</td>\n",
       "      <td>[recent, work, rer, seek, make, image, refer, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9923</td>\n",
       "      <td>use, model, semantic, task, framework, system,...</td>\n",
       "      <td>[hybrid, wordcharact, model, abstractive, summ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>use, model, semantic, task, framework, system,...</td>\n",
       "      <td>[regular, expression, re, widely, use, network...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>0.9952</td>\n",
       "      <td>use, method, sentence, research, classificatio...</td>\n",
       "      <td>[robot, perform, simple, fetchandcarry, task, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9919</td>\n",
       "      <td>model, task, use, propose, write, paper, syste...</td>\n",
       "      <td>[drop, network, unit, timestep, oppose, dropou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>0.9927</td>\n",
       "      <td>system, lithium, lm, usage, sentence, user, ri...</td>\n",
       "      <td>[candidate, sentence, selection, comprehensive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>0.9937</td>\n",
       "      <td>system, lithium, lm, usage, sentence, user, ri...</td>\n",
       "      <td>[lithium, extract, rich, set, develop, content...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>0.9934</td>\n",
       "      <td>use, method, sentence, research, classificatio...</td>\n",
       "      <td>[building, sentiment, large, amount, datum, av...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>0.9927</td>\n",
       "      <td>system, lithium, lm, usage, sentence, user, ri...</td>\n",
       "      <td>[efficient, contextualize, representation, mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>model, task, use, propose, write, paper, syste...</td>\n",
       "      <td>[majority, human, acquire, ability, communicat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9957</td>\n",
       "      <td>model, task, text, work, base, project, key, p...</td>\n",
       "      <td>[plan, base, framework, essay, generation, aim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9959</td>\n",
       "      <td>model, task, use, propose, write, paper, syste...</td>\n",
       "      <td>[automate, technique, model, check, violation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>0.9891</td>\n",
       "      <td>use, method, sentence, research, classificatio...</td>\n",
       "      <td>[large, scale, word, forum, novel, annotate, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>model, task, text, work, base, project, key, p...</td>\n",
       "      <td>[paper, present, hmm, hide, model, base, syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>0.9949</td>\n",
       "      <td>use, method, sentence, research, classificatio...</td>\n",
       "      <td>[sentence, simplification, aid, proteinprotein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9936</td>\n",
       "      <td>model, task, text, work, base, project, key, p...</td>\n",
       "      <td>[paper, introduce, neural, model, generate, ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>use, model, semantic, task, framework, system,...</td>\n",
       "      <td>[semantic, type, lexical, sort, classifier, ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>model, task, text, work, base, project, key, p...</td>\n",
       "      <td>[robustness, mention, detection, recurrent, ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9957</td>\n",
       "      <td>model, task, text, work, base, project, key, p...</td>\n",
       "      <td>[grow, body, research, arabic, recent, year, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>model, task, text, work, base, project, key, p...</td>\n",
       "      <td>[regression, method, property, brazilian, use,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9959</td>\n",
       "      <td>use, model, semantic, task, framework, system,...</td>\n",
       "      <td>[novel, teaching, parallel, distribute, comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9927</td>\n",
       "      <td>model, task, use, propose, write, paper, syste...</td>\n",
       "      <td>[nujnetwork, community, train, propose, learn,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>system, lithium, lm, usage, sentence, user, ri...</td>\n",
       "      <td>[work, focus, problem, ground, present, novel,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9931</td>\n",
       "      <td>model, task, text, work, base, project, key, p...</td>\n",
       "      <td>[new, analogy, corpus, explore, embedding, cze...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>model, task, text, work, base, project, key, p...</td>\n",
       "      <td>[problem, tag, find, way, tag, text, meticulou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>use, method, sentence, research, classificatio...</td>\n",
       "      <td>[complex, network, stylometry, boost, performa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>system, lithium, lm, usage, sentence, user, ri...</td>\n",
       "      <td>[generation, textual, entailment, nlml, system...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>model, task, use, propose, write, paper, syste...</td>\n",
       "      <td>[enrich, conversation, context, retrievalbase,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>0.5533</td>\n",
       "      <td>model, task, use, propose, write, paper, syste...</td>\n",
       "      <td>[base, hindienglish, primary, target, machine,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9923</td>\n",
       "      <td>use, model, semantic, task, framework, system,...</td>\n",
       "      <td>[current, stateoftheart, neural, network, arch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>0.7343</td>\n",
       "      <td>use, model, semantic, task, framework, system,...</td>\n",
       "      <td>[topic, avoid, demote, latent, confound, text,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9877</td>\n",
       "      <td>model, task, use, propose, write, paper, syste...</td>\n",
       "      <td>[paper, take, still, lack, generalization, sys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>model, task, text, work, base, project, key, p...</td>\n",
       "      <td>[chinese, model, achieve, excellent, result, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3</td>\n",
       "      <td>0.9919</td>\n",
       "      <td>use, method, sentence, research, classificatio...</td>\n",
       "      <td>[uniblock, scoring, filter, information, train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3</td>\n",
       "      <td>0.9886</td>\n",
       "      <td>use, method, sentence, research, classificatio...</td>\n",
       "      <td>[troll, public, commurum, fix, offensive, adve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>model, task, text, work, base, project, key, p...</td>\n",
       "      <td>[base, contextual, emotion, classifier, contex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9889</td>\n",
       "      <td>use, model, semantic, task, framework, system,...</td>\n",
       "      <td>[program, synthesis, semantic, parsing, learn,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9932</td>\n",
       "      <td>model, task, use, propose, write, paper, syste...</td>\n",
       "      <td>[unilateral, contract, term, service, play, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9943</td>\n",
       "      <td>model, task, text, work, base, project, key, p...</td>\n",
       "      <td>[outofvocabulary, embed, imputation, ground, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>use, model, semantic, task, framework, system,...</td>\n",
       "      <td>[evaluation, basic, module, isolate, spelling,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>use, model, semantic, task, framework, system,...</td>\n",
       "      <td>[pretraining, framework, understanding, framew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>use, model, semantic, task, framework, system,...</td>\n",
       "      <td>[new, model, achieve, considerable, deal, spec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>model, task, use, propose, write, paper, syste...</td>\n",
       "      <td>[aim, build, neural, network, model, task, err...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dominant_Topic  Perc_Contribution  \\\n",
       "0                1             0.9900   \n",
       "1                1             0.9945   \n",
       "2                0             0.9957   \n",
       "3                4             0.9950   \n",
       "4                0             0.9883   \n",
       "5                3             0.9804   \n",
       "6                4             0.9938   \n",
       "7                0             0.9937   \n",
       "8                0             0.9923   \n",
       "9                0             0.9933   \n",
       "10               3             0.9952   \n",
       "11               1             0.9919   \n",
       "12               2             0.9927   \n",
       "13               2             0.9937   \n",
       "14               3             0.9934   \n",
       "15               2             0.9927   \n",
       "16               1             0.9938   \n",
       "17               4             0.9957   \n",
       "18               1             0.9959   \n",
       "19               3             0.9891   \n",
       "20               4             0.9956   \n",
       "21               3             0.9949   \n",
       "22               4             0.9936   \n",
       "23               0             0.9926   \n",
       "24               4             0.9896   \n",
       "25               4             0.9957   \n",
       "26               4             0.9804   \n",
       "27               0             0.9959   \n",
       "28               1             0.9927   \n",
       "29               2             0.9925   \n",
       "30               4             0.9931   \n",
       "31               4             0.9954   \n",
       "32               3             0.9951   \n",
       "33               2             0.9933   \n",
       "34               1             0.9946   \n",
       "35               1             0.5533   \n",
       "36               0             0.9923   \n",
       "37               0             0.7343   \n",
       "38               1             0.9877   \n",
       "39               4             0.9945   \n",
       "40               3             0.9919   \n",
       "41               3             0.9886   \n",
       "42               4             0.9905   \n",
       "43               0             0.9889   \n",
       "44               1             0.9932   \n",
       "45               4             0.9943   \n",
       "46               0             0.9905   \n",
       "47               0             0.9913   \n",
       "48               0             0.9900   \n",
       "49               1             0.9930   \n",
       "\n",
       "                                       Topic_Keywords  \\\n",
       "0   model, task, use, propose, write, paper, syste...   \n",
       "1   model, task, use, propose, write, paper, syste...   \n",
       "2   use, model, semantic, task, framework, system,...   \n",
       "3   model, task, text, work, base, project, key, p...   \n",
       "4   use, model, semantic, task, framework, system,...   \n",
       "5   use, method, sentence, research, classificatio...   \n",
       "6   model, task, text, work, base, project, key, p...   \n",
       "7   use, model, semantic, task, framework, system,...   \n",
       "8   use, model, semantic, task, framework, system,...   \n",
       "9   use, model, semantic, task, framework, system,...   \n",
       "10  use, method, sentence, research, classificatio...   \n",
       "11  model, task, use, propose, write, paper, syste...   \n",
       "12  system, lithium, lm, usage, sentence, user, ri...   \n",
       "13  system, lithium, lm, usage, sentence, user, ri...   \n",
       "14  use, method, sentence, research, classificatio...   \n",
       "15  system, lithium, lm, usage, sentence, user, ri...   \n",
       "16  model, task, use, propose, write, paper, syste...   \n",
       "17  model, task, text, work, base, project, key, p...   \n",
       "18  model, task, use, propose, write, paper, syste...   \n",
       "19  use, method, sentence, research, classificatio...   \n",
       "20  model, task, text, work, base, project, key, p...   \n",
       "21  use, method, sentence, research, classificatio...   \n",
       "22  model, task, text, work, base, project, key, p...   \n",
       "23  use, model, semantic, task, framework, system,...   \n",
       "24  model, task, text, work, base, project, key, p...   \n",
       "25  model, task, text, work, base, project, key, p...   \n",
       "26  model, task, text, work, base, project, key, p...   \n",
       "27  use, model, semantic, task, framework, system,...   \n",
       "28  model, task, use, propose, write, paper, syste...   \n",
       "29  system, lithium, lm, usage, sentence, user, ri...   \n",
       "30  model, task, text, work, base, project, key, p...   \n",
       "31  model, task, text, work, base, project, key, p...   \n",
       "32  use, method, sentence, research, classificatio...   \n",
       "33  system, lithium, lm, usage, sentence, user, ri...   \n",
       "34  model, task, use, propose, write, paper, syste...   \n",
       "35  model, task, use, propose, write, paper, syste...   \n",
       "36  use, model, semantic, task, framework, system,...   \n",
       "37  use, model, semantic, task, framework, system,...   \n",
       "38  model, task, use, propose, write, paper, syste...   \n",
       "39  model, task, text, work, base, project, key, p...   \n",
       "40  use, method, sentence, research, classificatio...   \n",
       "41  use, method, sentence, research, classificatio...   \n",
       "42  model, task, text, work, base, project, key, p...   \n",
       "43  use, model, semantic, task, framework, system,...   \n",
       "44  model, task, use, propose, write, paper, syste...   \n",
       "45  model, task, text, work, base, project, key, p...   \n",
       "46  use, model, semantic, task, framework, system,...   \n",
       "47  use, model, semantic, task, framework, system,...   \n",
       "48  use, model, semantic, task, framework, system,...   \n",
       "49  model, task, use, propose, write, paper, syste...   \n",
       "\n",
       "                                                    0  \n",
       "0   [paper, audio, play, hot, cold, set, alarm, pm...  \n",
       "1   [state, transducer, efficient, way, represent,...  \n",
       "2   [quantification, semantic, similarity, word, u...  \n",
       "3   [paper, focus, aim, semantic, role, label, lab...  \n",
       "4   [also, new, level, incorporating, advance, wor...  \n",
       "5   [arabic, popular, msa, arabicdialectad, berevi...  \n",
       "6   [efficient, encoding, context, speak, computat...  \n",
       "7   [recent, work, rer, seek, make, image, refer, ...  \n",
       "8   [hybrid, wordcharact, model, abstractive, summ...  \n",
       "9   [regular, expression, re, widely, use, network...  \n",
       "10  [robot, perform, simple, fetchandcarry, task, ...  \n",
       "11  [drop, network, unit, timestep, oppose, dropou...  \n",
       "12  [candidate, sentence, selection, comprehensive...  \n",
       "13  [lithium, extract, rich, set, develop, content...  \n",
       "14  [building, sentiment, large, amount, datum, av...  \n",
       "15  [efficient, contextualize, representation, mod...  \n",
       "16  [majority, human, acquire, ability, communicat...  \n",
       "17  [plan, base, framework, essay, generation, aim...  \n",
       "18  [automate, technique, model, check, violation,...  \n",
       "19  [large, scale, word, forum, novel, annotate, s...  \n",
       "20  [paper, present, hmm, hide, model, base, syste...  \n",
       "21  [sentence, simplification, aid, proteinprotein...  \n",
       "22  [paper, introduce, neural, model, generate, ge...  \n",
       "23  [semantic, type, lexical, sort, classifier, ne...  \n",
       "24  [robustness, mention, detection, recurrent, ne...  \n",
       "25  [grow, body, research, arabic, recent, year, e...  \n",
       "26  [regression, method, property, brazilian, use,...  \n",
       "27  [novel, teaching, parallel, distribute, comput...  \n",
       "28  [nujnetwork, community, train, propose, learn,...  \n",
       "29  [work, focus, problem, ground, present, novel,...  \n",
       "30  [new, analogy, corpus, explore, embedding, cze...  \n",
       "31  [problem, tag, find, way, tag, text, meticulou...  \n",
       "32  [complex, network, stylometry, boost, performa...  \n",
       "33  [generation, textual, entailment, nlml, system...  \n",
       "34  [enrich, conversation, context, retrievalbase,...  \n",
       "35  [base, hindienglish, primary, target, machine,...  \n",
       "36  [current, stateoftheart, neural, network, arch...  \n",
       "37  [topic, avoid, demote, latent, confound, text,...  \n",
       "38  [paper, take, still, lack, generalization, sys...  \n",
       "39  [chinese, model, achieve, excellent, result, t...  \n",
       "40  [uniblock, scoring, filter, information, train...  \n",
       "41  [troll, public, commurum, fix, offensive, adve...  \n",
       "42  [base, contextual, emotion, classifier, contex...  \n",
       "43  [program, synthesis, semantic, parsing, learn,...  \n",
       "44  [unilateral, contract, term, service, play, su...  \n",
       "45  [outofvocabulary, embed, imputation, ground, g...  \n",
       "46  [evaluation, basic, module, isolate, spelling,...  \n",
       "47  [pretraining, framework, understanding, framew...  \n",
       "48  [new, model, achieve, considerable, deal, spec...  \n",
       "49  [aim, build, neural, network, model, task, err...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_sents_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3725cf6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Title</th>\n",
       "      <th>PDF URL</th>\n",
       "      <th>Author</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Journal Ref</th>\n",
       "      <th>Primary Category</th>\n",
       "      <th>Category</th>\n",
       "      <th>Entry ID</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1809.09190v1.pdf</td>\n",
       "      <td>in this paper we formulate audio play hot n c...</td>\n",
       "      <td>From Audio to Semantics: Approaches to end-to-...</td>\n",
       "      <td>http://arxiv.org/pdf/1809.09190v1</td>\n",
       "      <td>[arxiv.Result.Author('Parisa Haghani'), arxiv....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-09-24 19:46:24+00:00</td>\n",
       "      <td>Conventional spoken language understanding sys...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eess.AS</td>\n",
       "      <td>['eess.AS', 'cs.CL', 'cs.SD']</td>\n",
       "      <td>http://arxiv.org/abs/1809.09190v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1903.10625v2.pdf</td>\n",
       "      <td>finite state transducers fst are an efficient...</td>\n",
       "      <td>Neural Grammatical Error Correction with Finit...</td>\n",
       "      <td>http://arxiv.org/pdf/1903.10625v2</td>\n",
       "      <td>[arxiv.Result.Author('Felix Stahlberg'), arxiv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-25 23:05:11+00:00</td>\n",
       "      <td>Grammatical error correction (GEC) is one of t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1903.10625v2</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1904.04307v1.pdf</td>\n",
       "      <td>the quantification of semantic similarity bet...</td>\n",
       "      <td>Word Similarity Datasets for Thai: Constructio...</td>\n",
       "      <td>http://arxiv.org/pdf/1904.04307v1</td>\n",
       "      <td>[arxiv.Result.Author('Ponrudee Netisopakul'), ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-08 19:18:09+00:00</td>\n",
       "      <td>Distributional semantics in the form of word e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1904.04307v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1809.02794v3.pdf</td>\n",
       "      <td>this paper focuses on the aim of semantic rol...</td>\n",
       "      <td>Explicit Contextual Semantics for Text Compreh...</td>\n",
       "      <td>http://arxiv.org/pdf/1809.02794v3</td>\n",
       "      <td>[arxiv.Result.Author('Zhuosheng Zhang'), arxiv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-09-08 12:34:59+00:00</td>\n",
       "      <td>Who did what to whom is a major focus in natur...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1809.02794v3</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1805.01083v1.pdf</td>\n",
       "      <td>the mainconstructusedinextractionlanguagesand...</td>\n",
       "      <td>Scalable Semantic Querying of Text</td>\n",
       "      <td>http://arxiv.org/pdf/1805.01083v1</td>\n",
       "      <td>[arxiv.Result.Author('Xiaolan Wang'), arxiv.Re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-05-03 01:57:31+00:00</td>\n",
       "      <td>We present the KOKO system that takes declarat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.DB</td>\n",
       "      <td>['cs.DB', 'cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1805.01083v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1903.02784v1.pdf</td>\n",
       "      <td>arabic isrecognisedasthe4th most popular lang...</td>\n",
       "      <td>Arabic natural language processing: An overview</td>\n",
       "      <td>http://arxiv.org/pdf/1903.02784v1</td>\n",
       "      <td>[arxiv.Result.Author('Imane Guellil'), arxiv.R...</td>\n",
       "      <td>10.1016/j.jksuci.2019.02.006</td>\n",
       "      <td>2019-03-07 09:22:35+00:00</td>\n",
       "      <td>Arabic is recognised as the 4th most used lang...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1903.02784v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1807.00267v1.pdf</td>\n",
       "      <td>an efficient approach to encoding context for...</td>\n",
       "      <td>An Efficient Approach to Encoding Context for ...</td>\n",
       "      <td>http://arxiv.org/pdf/1807.00267v1</td>\n",
       "      <td>[arxiv.Result.Author('Raghav Gupta'), arxiv.Re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-07-01 04:11:18+00:00</td>\n",
       "      <td>In task-oriented dialogue systems, spoken lang...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1807.00267v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1805.11818v1.pdf</td>\n",
       "      <td>recent work on rer has sought to make an imag...</td>\n",
       "      <td>Visual Referring Expression Recognition: What ...</td>\n",
       "      <td>http://arxiv.org/pdf/1805.11818v1</td>\n",
       "      <td>[arxiv.Result.Author('Volkan Cirik'), arxiv.Re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-05-30 06:03:21+00:00</td>\n",
       "      <td>We present an empirical analysis of the state-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL', 'cs.AI', 'cs.CV', 'cs.NE']</td>\n",
       "      <td>http://arxiv.org/abs/1805.11818v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1802.09968v2.pdf</td>\n",
       "      <td>a hybrid wordcharacter model for abstractive ...</td>\n",
       "      <td>A Hybrid Word-Character Approach to Abstractiv...</td>\n",
       "      <td>http://arxiv.org/pdf/1802.09968v2</td>\n",
       "      <td>[arxiv.Result.Author('Chieh-Teng Chang'), arxi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-02-27 15:31:11+00:00</td>\n",
       "      <td>Automatic abstractive text summarization is an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1802.09968v2</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1805.05588v1.pdf</td>\n",
       "      <td>regular expressions res are widely used in ne...</td>\n",
       "      <td>Marrying up Regular Expressions with Neural Ne...</td>\n",
       "      <td>http://arxiv.org/pdf/1805.05588v1</td>\n",
       "      <td>[arxiv.Result.Author('Bingfeng Luo'), arxiv.Re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-05-15 06:40:44+00:00</td>\n",
       "      <td>The success of many natural language processin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1805.05588v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1807.11838v1.pdf</td>\n",
       "      <td>a robot that could perform simple fetchandcar...</td>\n",
       "      <td>Extensible Grounding of Speech for Robot Instr...</td>\n",
       "      <td>http://arxiv.org/pdf/1807.11838v1</td>\n",
       "      <td>[arxiv.Result.Author('Jonathan Connell')]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-07-31 14:31:17+00:00</td>\n",
       "      <td>Spoken language is a convenient interface for ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>['cs.RO', 'cs.AI', 'cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1807.11838v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1708.01009v1.pdf</td>\n",
       "      <td>variational rnns drop the same network units ...</td>\n",
       "      <td>Revisiting Activation Regularization for Langu...</td>\n",
       "      <td>http://arxiv.org/pdf/1708.01009v1</td>\n",
       "      <td>[arxiv.Result.Author('Stephen Merity'), arxiv....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-03 05:53:53+00:00</td>\n",
       "      <td>Recurrent neural networks (RNNs) serve as a fu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL', 'cs.NE']</td>\n",
       "      <td>http://arxiv.org/abs/1708.01009v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1706.03530v1.pdf</td>\n",
       "      <td>candidate sentence selection for language fro...</td>\n",
       "      <td>Candidate sentence selection for language lear...</td>\n",
       "      <td>http://arxiv.org/pdf/1706.03530v1</td>\n",
       "      <td>[arxiv.Result.Author('Ildik Piln'), arxiv.Re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-06-12 09:21:45+00:00</td>\n",
       "      <td>We present a framework and its implementation ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1706.03530v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1707.04244v1.pdf</td>\n",
       "      <td>lithium nlp extracts a rich set developed for...</td>\n",
       "      <td>Lithium NLP: A System for Rich Information Ext...</td>\n",
       "      <td>http://arxiv.org/pdf/1707.04244v1</td>\n",
       "      <td>[arxiv.Result.Author('Preeti Bhargava'), arxiv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-07-13 17:52:51+00:00</td>\n",
       "      <td>In this paper, we describe the Lithium Natural...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>['cs.AI', 'cs.CL', 'cs.IR']</td>\n",
       "      <td>http://arxiv.org/abs/1707.04244v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1712.08917v1.pdf</td>\n",
       "      <td>building a sentiment corpus of tweets in braz...</td>\n",
       "      <td>Building a Sentiment Corpus of Tweets in Brazi...</td>\n",
       "      <td>http://arxiv.org/pdf/1712.08917v1</td>\n",
       "      <td>[arxiv.Result.Author('Henrico Bertini Brum'), ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-12-24 13:23:58+00:00</td>\n",
       "      <td>The large amount of data available in social m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1712.08917v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1804.07827v2.pdf</td>\n",
       "      <td>efficient contextualized representation langu...</td>\n",
       "      <td>Efficient Contextualized Representation: Langu...</td>\n",
       "      <td>http://arxiv.org/pdf/1804.07827v2</td>\n",
       "      <td>[arxiv.Result.Author('Liyuan Liu'), arxiv.Resu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-04-20 21:10:17+00:00</td>\n",
       "      <td>Many efforts have been made to facilitate natu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1804.07827v2</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1804.03052v1.pdf</td>\n",
       "      <td>the majority of humans acquire the ability to...</td>\n",
       "      <td>Vision as an Interlingua: Learning Multilingua...</td>\n",
       "      <td>http://arxiv.org/pdf/1804.03052v1</td>\n",
       "      <td>[arxiv.Result.Author('David Harwath'), arxiv.R...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-04-09 15:15:37+00:00</td>\n",
       "      <td>In this paper, we explore the learning of neur...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL', 'cs.SD', 'eess.AS']</td>\n",
       "      <td>http://arxiv.org/abs/1804.03052v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1512.05919v2.pdf</td>\n",
       "      <td>a planning based framework for essay generati...</td>\n",
       "      <td>A Planning based Framework for Essay Generation</td>\n",
       "      <td>http://arxiv.org/pdf/1512.05919v2</td>\n",
       "      <td>[arxiv.Result.Author('Bing Qin'), arxiv.Result...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-12-18 12:10:42+00:00</td>\n",
       "      <td>Generating an article automatically with compu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1512.05919v2</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1803.08966v1.pdf</td>\n",
       "      <td>automated techniques such as model checking v...</td>\n",
       "      <td>Counterexamples for Robotic Planning Explained...</td>\n",
       "      <td>http://arxiv.org/pdf/1803.08966v1</td>\n",
       "      <td>[arxiv.Result.Author('Lu Feng'), arxiv.Result....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-03-23 20:14:51+00:00</td>\n",
       "      <td>Automated techniques such as model checking ha...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>['cs.RO', 'cs.CL', 'cs.FL']</td>\n",
       "      <td>http://arxiv.org/abs/1803.08966v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1609.02960v1.pdf</td>\n",
       "      <td>a large scale corpus of gulf arabic has 110m ...</td>\n",
       "      <td>A Large Scale Corpus of Gulf Arabic</td>\n",
       "      <td>http://arxiv.org/pdf/1609.02960v1</td>\n",
       "      <td>[arxiv.Result.Author('Salam Khalifa'), arxiv.R...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-09-09 22:22:53+00:00</td>\n",
       "      <td>Most Arabic natural language processing tools ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1609.02960v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1601.01195v1.pdf</td>\n",
       "      <td>this paper presents a description of hmm hidd...</td>\n",
       "      <td>Part-of-Speech Tagging for Code-mixed Indian S...</td>\n",
       "      <td>http://arxiv.org/pdf/1601.01195v1</td>\n",
       "      <td>[arxiv.Result.Author('Kamal Sarkar')]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-06 14:40:38+00:00</td>\n",
       "      <td>This paper discusses the experiments carried o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL', '68T50']</td>\n",
       "      <td>http://arxiv.org/abs/1601.01195v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1001.4273v1.pdf</td>\n",
       "      <td>sentence simplification aids proteinprotein i...</td>\n",
       "      <td>Sentence Simplification Aids Protein-Protein I...</td>\n",
       "      <td>http://arxiv.org/pdf/1001.4273v1</td>\n",
       "      <td>[arxiv.Result.Author('Siddhartha Jonnalagadda'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-01-24 20:23:10+00:00</td>\n",
       "      <td>Accurate systems for extracting Protein-Protei...</td>\n",
       "      <td>The 3rd International Symposium on Languages i...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1001.4273v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1603.07771v3.pdf</td>\n",
       "      <td>this paper introduces a neural model for pare...</td>\n",
       "      <td>Neural Text Generation from Structured Data wi...</td>\n",
       "      <td>http://arxiv.org/pdf/1603.07771v3</td>\n",
       "      <td>[arxiv.Result.Author('Remi Lebret'), arxiv.Res...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-24 22:40:00+00:00</td>\n",
       "      <td>This paper introduces a neural model for conce...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1603.07771v3</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1312.3168v1.pdf</td>\n",
       "      <td>semantic types lexical sorts and classifiers ...</td>\n",
       "      <td>Semantic Types, Lexical Sorts and Classifiers</td>\n",
       "      <td>http://arxiv.org/pdf/1312.3168v1</td>\n",
       "      <td>[arxiv.Result.Author('Bruno Mery'), arxiv.Resu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-12-11 14:04:52+00:00</td>\n",
       "      <td>We propose a cognitively and linguistically mo...</td>\n",
       "      <td>NLPCS '10- 10th International Workshop on Natu...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1312.3168v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1602.07749v1.pdf</td>\n",
       "      <td>thien huu nguyen avirupsil georgianadinu and ...</td>\n",
       "      <td>Toward Mention Detection Robustness with Recur...</td>\n",
       "      <td>http://arxiv.org/pdf/1602.07749v1</td>\n",
       "      <td>[arxiv.Result.Author('Thien Huu Nguyen'), arxi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-02-24 23:14:01+00:00</td>\n",
       "      <td>One of the key challenges in natural language ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1602.07749v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1803.00124v2.pdf</td>\n",
       "      <td>there is a growing body of research in nlp fo...</td>\n",
       "      <td>Improving Sentiment Analysis in Arabic Using W...</td>\n",
       "      <td>http://arxiv.org/pdf/1803.00124v2</td>\n",
       "      <td>[arxiv.Result.Author('Abdulaziz M. Alayba'), a...</td>\n",
       "      <td>10.1109/ASAR.2018.8480191</td>\n",
       "      <td>2018-02-28 22:46:19+00:00</td>\n",
       "      <td>The complexities of Arabic language in morphol...</td>\n",
       "      <td>Proc. 2nd International Workshop on Arabic and...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL', 'I.2.7; I.2.6']</td>\n",
       "      <td>http://arxiv.org/abs/1803.00124v2</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1705.07008v1.pdf</td>\n",
       "      <td>a lightweight regression method to inferative...</td>\n",
       "      <td>A Lightweight Regression Method to Infer Psych...</td>\n",
       "      <td>http://arxiv.org/pdf/1705.07008v1</td>\n",
       "      <td>[arxiv.Result.Author('Leandro B. dos Santos'),...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-19 14:17:31+00:00</td>\n",
       "      <td>Psycholinguistic properties of words have been...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1705.07008v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1703.10242v1.pdf</td>\n",
       "      <td>a novel approach to teaching parallel and dis...</td>\n",
       "      <td>I CAN HAS SUPERCOMPUTER? A Novel Approach to T...</td>\n",
       "      <td>http://arxiv.org/pdf/1703.10242v1</td>\n",
       "      <td>[arxiv.Result.Author('David Richie'), arxiv.Re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-03-29 20:42:28+00:00</td>\n",
       "      <td>A novel approach is presented to teach the par...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.DC</td>\n",
       "      <td>['cs.DC', 'cs.PL']</td>\n",
       "      <td>http://arxiv.org/abs/1703.10242v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1607.04606v2.pdf</td>\n",
       "      <td>in the neural nujnetwork community collobert ...</td>\n",
       "      <td>Enriching Word Vectors with Subword Information</td>\n",
       "      <td>http://arxiv.org/pdf/1607.04606v2</td>\n",
       "      <td>[arxiv.Result.Author('Piotr Bojanowski'), arxi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-07-15 18:27:55+00:00</td>\n",
       "      <td>Continuous word representations, trained on la...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL', 'cs.LG']</td>\n",
       "      <td>http://arxiv.org/abs/1607.04606v2</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1603.08079v1.pdf</td>\n",
       "      <td>in this work we focus on the problem of groun...</td>\n",
       "      <td>Do You See What I Mean? Visual Resolution of L...</td>\n",
       "      <td>http://arxiv.org/pdf/1603.08079v1</td>\n",
       "      <td>[arxiv.Result.Author('Yevgeni Berzak'), arxiv....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-26 06:49:33+00:00</td>\n",
       "      <td>Understanding language goes hand in hand with ...</td>\n",
       "      <td>Conference on Empirical Methods in Natural Lan...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>['cs.CV', 'cs.AI', 'cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1603.08079v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1608.00789v1.pdf</td>\n",
       "      <td>new word analogy corpus for exploring embeddi...</td>\n",
       "      <td>New word analogy corpus for exploring embeddin...</td>\n",
       "      <td>http://arxiv.org/pdf/1608.00789v1</td>\n",
       "      <td>[arxiv.Result.Author('Luk Svoboda'), arxiv.R...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-02 12:31:06+00:00</td>\n",
       "      <td>The word embedding methods have been proven to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1608.00789v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1111.5293v1.pdf</td>\n",
       "      <td>the problem of tagging in natural language pr...</td>\n",
       "      <td>Rule based Part of speech Tagger for Homoeopat...</td>\n",
       "      <td>http://arxiv.org/pdf/1111.5293v1</td>\n",
       "      <td>[arxiv.Result.Author('Sanjay K. Dwivedi'), arx...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-11-13 18:19:15+00:00</td>\n",
       "      <td>A tagger is a mandatory segment of most text s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1111.5293v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1506.09107v2.pdf</td>\n",
       "      <td>a complex network approach to stylometry coul...</td>\n",
       "      <td>A complex network approach to stylometry</td>\n",
       "      <td>http://arxiv.org/pdf/1506.09107v2</td>\n",
       "      <td>[arxiv.Result.Author('Diego R. Amancio')]</td>\n",
       "      <td>10.1371/journal.pone.0136076</td>\n",
       "      <td>2015-06-30 14:32:30+00:00</td>\n",
       "      <td>Statistical methods have been widely employed ...</td>\n",
       "      <td>PLoS ONE 10(8): e0136076, 2015</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1506.09107v2</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0802.4326v1.pdf</td>\n",
       "      <td>the generation of textual entailment with nlm...</td>\n",
       "      <td>The Generation of Textual Entailment with NLML...</td>\n",
       "      <td>http://arxiv.org/pdf/0802.4326v1</td>\n",
       "      <td>[arxiv.Result.Author('Jiyou Jia')]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-02-29 06:16:29+00:00</td>\n",
       "      <td>This research report introduces the generation...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL', 'cs.AI', 'cs.CY']</td>\n",
       "      <td>http://arxiv.org/abs/0802.4326v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1911.02290v1.pdf</td>\n",
       "      <td>enriching conversation context in retrievalba...</td>\n",
       "      <td>Enriching Conversation Context in Retrieval-ba...</td>\n",
       "      <td>http://arxiv.org/pdf/1911.02290v1</td>\n",
       "      <td>[arxiv.Result.Author('Amir Vakili Tahami'), ar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-06 10:24:45+00:00</td>\n",
       "      <td>Work on retrieval-based chatbots, like most se...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1911.02290v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1909.09779v1.pdf</td>\n",
       "      <td>selfattention based endtoend hindienglish the...</td>\n",
       "      <td>Self-attention based end-to-end Hindi-English ...</td>\n",
       "      <td>http://arxiv.org/pdf/1909.09779v1</td>\n",
       "      <td>[arxiv.Result.Author('Siddhant Srivastava'), a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-21 06:16:52+00:00</td>\n",
       "      <td>Machine Translation (MT) is a zone of concentr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1909.09779v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1909.09482v1.pdf</td>\n",
       "      <td>the current stateoftheart natural language pr...</td>\n",
       "      <td>Language models and Automated Essay Scoring</td>\n",
       "      <td>http://arxiv.org/pdf/1909.09482v1</td>\n",
       "      <td>[arxiv.Result.Author('Pedro Uria Rodriguez'), ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-18 18:50:18+00:00</td>\n",
       "      <td>In this paper, we present a new comparative st...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL', 'cs.LG', 'stat.ML']</td>\n",
       "      <td>http://arxiv.org/abs/1909.09482v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1909.00453v2.pdf</td>\n",
       "      <td>topics to avoid demoting latent confounds in ...</td>\n",
       "      <td>Topics to Avoid: Demoting Latent Confounds in ...</td>\n",
       "      <td>http://arxiv.org/pdf/1909.00453v2</td>\n",
       "      <td>[arxiv.Result.Author('Sachin Kumar'), arxiv.Re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-01 19:18:44+00:00</td>\n",
       "      <td>Despite impressive performance on many text cl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>['cs.LG', 'cs.CL', 'stat.ML']</td>\n",
       "      <td>http://arxiv.org/abs/1909.00453v2</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1909.01792v2.pdf</td>\n",
       "      <td>in this paper we take a languageagnosticappro...</td>\n",
       "      <td>Mogrifier LSTM</td>\n",
       "      <td>http://arxiv.org/pdf/1909.01792v2</td>\n",
       "      <td>[arxiv.Result.Author('Gbor Melis'), arxiv.Res...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-04 13:32:23+00:00</td>\n",
       "      <td>Many advances in Natural Language Processing h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1909.01792v2</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1908.07721v2.pdf</td>\n",
       "      <td>chinese language model has achieved excellent...</td>\n",
       "      <td>Fine-tuning BERT for Joint Entity and Relation...</td>\n",
       "      <td>http://arxiv.org/pdf/1908.07721v2</td>\n",
       "      <td>[arxiv.Result.Author('Kui Xue'), arxiv.Result....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-08-21 06:56:08+00:00</td>\n",
       "      <td>Entity and relation extraction is the necessar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1908.07721v2</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1908.09716v1.pdf</td>\n",
       "      <td>uniblock scoring and filtering corpuswith uni...</td>\n",
       "      <td>uniblock: Scoring and Filtering Corpus with Un...</td>\n",
       "      <td>http://arxiv.org/pdf/1908.09716v1</td>\n",
       "      <td>[arxiv.Result.Author('Yingbo Gao'), arxiv.Resu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-08-26 14:55:03+00:00</td>\n",
       "      <td>The preprocessing pipelines in Natural Languag...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1908.09716v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1908.06083v1.pdf</td>\n",
       "      <td>the detection of trolls in public fo vant to ...</td>\n",
       "      <td>Build it Break it Fix it for Dialogue Safety: ...</td>\n",
       "      <td>http://arxiv.org/pdf/1908.06083v1</td>\n",
       "      <td>[arxiv.Result.Author('Emily Dinan'), arxiv.Res...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-08-17 18:34:11+00:00</td>\n",
       "      <td>The detection of offensive language in the con...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1908.06083v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1906.11565v2.pdf</td>\n",
       "      <td>emotionxku bertmax based contextual emotion c...</td>\n",
       "      <td>EmotionX-KU: BERT-Max based Contextual Emotion...</td>\n",
       "      <td>http://arxiv.org/pdf/1906.11565v2</td>\n",
       "      <td>[arxiv.Result.Author('Kisu Yang'), arxiv.Resul...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-06-27 11:46:48+00:00</td>\n",
       "      <td>We propose a contextual emotion classifier bas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1906.11565v2</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1906.10816v4.pdf</td>\n",
       "      <td>program synthesis and semantic parsing with l...</td>\n",
       "      <td>Program Synthesis and Semantic Parsing with Le...</td>\n",
       "      <td>http://arxiv.org/pdf/1906.10816v4</td>\n",
       "      <td>[arxiv.Result.Author('Richard Shin'), arxiv.Re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-06-26 02:28:10+00:00</td>\n",
       "      <td>Program synthesis of general-purpose source co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>['cs.LG', 'cs.AI', 'cs.CL', 'cs.PL', 'stat.ML']</td>\n",
       "      <td>http://arxiv.org/abs/1906.10816v4</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1906.00424v1.pdf</td>\n",
       "      <td>unilateral contracts such as terms of service...</td>\n",
       "      <td>Plain English Summarization of Contracts</td>\n",
       "      <td>http://arxiv.org/pdf/1906.00424v1</td>\n",
       "      <td>[arxiv.Result.Author('Laura Manor'), arxiv.Res...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-06-02 15:27:51+00:00</td>\n",
       "      <td>Unilateral contracts, such as terms of service...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1906.00424v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1906.03753v2.pdf</td>\n",
       "      <td>outofvocabulary embedding imputation with gro...</td>\n",
       "      <td>Out-of-Vocabulary Embedding Imputation with Gr...</td>\n",
       "      <td>http://arxiv.org/pdf/1906.03753v2</td>\n",
       "      <td>[arxiv.Result.Author('Ziyi Yang'), arxiv.Resul...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-06-10 01:10:34+00:00</td>\n",
       "      <td>Due to the ubiquitous use of embeddings as inp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1906.03753v2</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1905.10810v1.pdf</td>\n",
       "      <td>evaluation of basic modules for isolated spel...</td>\n",
       "      <td>Evaluation of basic modules for isolated spell...</td>\n",
       "      <td>http://arxiv.org/pdf/1905.10810v1</td>\n",
       "      <td>[arxiv.Result.Author('Szymon Rutkowski')]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-05-26 14:54:52+00:00</td>\n",
       "      <td>Spelling error correction is an important prob...</td>\n",
       "      <td>Human Language Technologies as a Challenge for...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1905.10810v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1907.12412v2.pdf</td>\n",
       "      <td>ernie 20 a continual pretraining framework for...</td>\n",
       "      <td>ERNIE 2.0: A Continual Pre-training Framework ...</td>\n",
       "      <td>http://arxiv.org/pdf/1907.12412v2</td>\n",
       "      <td>[arxiv.Result.Author('Yu Sun'), arxiv.Result.A...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-07-29 13:25:37+00:00</td>\n",
       "      <td>Recently, pre-trained models have achieved sta...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1907.12412v2</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1812.06624v1.pdf</td>\n",
       "      <td>the new models achieved considerable improvem...</td>\n",
       "      <td>Feature Fusion Effects of Tensor Product Repre...</td>\n",
       "      <td>http://arxiv.org/pdf/1812.06624v1</td>\n",
       "      <td>[arxiv.Result.Author('Chiranjib Sur')]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-17 06:24:03+00:00</td>\n",
       "      <td>Progress in image captioning is gradually gett...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>['cs.CV', 'cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1812.06624v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1810.00660v1.pdf</td>\n",
       "      <td>aims to build neural network models for the t...</td>\n",
       "      <td>Attention-based Encoder-Decoder Networks for S...</td>\n",
       "      <td>http://arxiv.org/pdf/1810.00660v1</td>\n",
       "      <td>[arxiv.Result.Author('Sina Ahmadi')]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-09-21 23:47:42+00:00</td>\n",
       "      <td>Automatic spelling and grammatical correction ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL', 'cs.AI']</td>\n",
       "      <td>http://arxiv.org/abs/1810.00660v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Filename                                            Summary  \\\n",
       "0   1809.09190v1.pdf   in this paper we formulate audio play hot n c...   \n",
       "1   1903.10625v2.pdf   finite state transducers fst are an efficient...   \n",
       "2   1904.04307v1.pdf   the quantification of semantic similarity bet...   \n",
       "3   1809.02794v3.pdf   this paper focuses on the aim of semantic rol...   \n",
       "4   1805.01083v1.pdf   the mainconstructusedinextractionlanguagesand...   \n",
       "5   1903.02784v1.pdf   arabic isrecognisedasthe4th most popular lang...   \n",
       "6   1807.00267v1.pdf   an efficient approach to encoding context for...   \n",
       "7   1805.11818v1.pdf   recent work on rer has sought to make an imag...   \n",
       "8   1802.09968v2.pdf   a hybrid wordcharacter model for abstractive ...   \n",
       "9   1805.05588v1.pdf   regular expressions res are widely used in ne...   \n",
       "10  1807.11838v1.pdf   a robot that could perform simple fetchandcar...   \n",
       "11  1708.01009v1.pdf   variational rnns drop the same network units ...   \n",
       "12  1706.03530v1.pdf   candidate sentence selection for language fro...   \n",
       "13  1707.04244v1.pdf   lithium nlp extracts a rich set developed for...   \n",
       "14  1712.08917v1.pdf   building a sentiment corpus of tweets in braz...   \n",
       "15  1804.07827v2.pdf   efficient contextualized representation langu...   \n",
       "16  1804.03052v1.pdf   the majority of humans acquire the ability to...   \n",
       "17  1512.05919v2.pdf   a planning based framework for essay generati...   \n",
       "18  1803.08966v1.pdf   automated techniques such as model checking v...   \n",
       "19  1609.02960v1.pdf   a large scale corpus of gulf arabic has 110m ...   \n",
       "20  1601.01195v1.pdf   this paper presents a description of hmm hidd...   \n",
       "21   1001.4273v1.pdf   sentence simplification aids proteinprotein i...   \n",
       "22  1603.07771v3.pdf   this paper introduces a neural model for pare...   \n",
       "23   1312.3168v1.pdf   semantic types lexical sorts and classifiers ...   \n",
       "24  1602.07749v1.pdf   thien huu nguyen avirupsil georgianadinu and ...   \n",
       "25  1803.00124v2.pdf   there is a growing body of research in nlp fo...   \n",
       "26  1705.07008v1.pdf   a lightweight regression method to inferative...   \n",
       "27  1703.10242v1.pdf   a novel approach to teaching parallel and dis...   \n",
       "28  1607.04606v2.pdf   in the neural nujnetwork community collobert ...   \n",
       "29  1603.08079v1.pdf   in this work we focus on the problem of groun...   \n",
       "30  1608.00789v1.pdf   new word analogy corpus for exploring embeddi...   \n",
       "31   1111.5293v1.pdf   the problem of tagging in natural language pr...   \n",
       "32  1506.09107v2.pdf   a complex network approach to stylometry coul...   \n",
       "33   0802.4326v1.pdf   the generation of textual entailment with nlm...   \n",
       "34  1911.02290v1.pdf   enriching conversation context in retrievalba...   \n",
       "35  1909.09779v1.pdf   selfattention based endtoend hindienglish the...   \n",
       "36  1909.09482v1.pdf   the current stateoftheart natural language pr...   \n",
       "37  1909.00453v2.pdf   topics to avoid demoting latent confounds in ...   \n",
       "38  1909.01792v2.pdf   in this paper we take a languageagnosticappro...   \n",
       "39  1908.07721v2.pdf   chinese language model has achieved excellent...   \n",
       "40  1908.09716v1.pdf   uniblock scoring and filtering corpuswith uni...   \n",
       "41  1908.06083v1.pdf   the detection of trolls in public fo vant to ...   \n",
       "42  1906.11565v2.pdf   emotionxku bertmax based contextual emotion c...   \n",
       "43  1906.10816v4.pdf   program synthesis and semantic parsing with l...   \n",
       "44  1906.00424v1.pdf   unilateral contracts such as terms of service...   \n",
       "45  1906.03753v2.pdf   outofvocabulary embedding imputation with gro...   \n",
       "46  1905.10810v1.pdf   evaluation of basic modules for isolated spel...   \n",
       "47  1907.12412v2.pdf  ernie 20 a continual pretraining framework for...   \n",
       "48  1812.06624v1.pdf   the new models achieved considerable improvem...   \n",
       "49  1810.00660v1.pdf   aims to build neural network models for the t...   \n",
       "\n",
       "                                                Title  \\\n",
       "0   From Audio to Semantics: Approaches to end-to-...   \n",
       "1   Neural Grammatical Error Correction with Finit...   \n",
       "2   Word Similarity Datasets for Thai: Constructio...   \n",
       "3   Explicit Contextual Semantics for Text Compreh...   \n",
       "4                  Scalable Semantic Querying of Text   \n",
       "5     Arabic natural language processing: An overview   \n",
       "6   An Efficient Approach to Encoding Context for ...   \n",
       "7   Visual Referring Expression Recognition: What ...   \n",
       "8   A Hybrid Word-Character Approach to Abstractiv...   \n",
       "9   Marrying up Regular Expressions with Neural Ne...   \n",
       "10  Extensible Grounding of Speech for Robot Instr...   \n",
       "11  Revisiting Activation Regularization for Langu...   \n",
       "12  Candidate sentence selection for language lear...   \n",
       "13  Lithium NLP: A System for Rich Information Ext...   \n",
       "14  Building a Sentiment Corpus of Tweets in Brazi...   \n",
       "15  Efficient Contextualized Representation: Langu...   \n",
       "16  Vision as an Interlingua: Learning Multilingua...   \n",
       "17    A Planning based Framework for Essay Generation   \n",
       "18  Counterexamples for Robotic Planning Explained...   \n",
       "19                A Large Scale Corpus of Gulf Arabic   \n",
       "20  Part-of-Speech Tagging for Code-mixed Indian S...   \n",
       "21  Sentence Simplification Aids Protein-Protein I...   \n",
       "22  Neural Text Generation from Structured Data wi...   \n",
       "23      Semantic Types, Lexical Sorts and Classifiers   \n",
       "24  Toward Mention Detection Robustness with Recur...   \n",
       "25  Improving Sentiment Analysis in Arabic Using W...   \n",
       "26  A Lightweight Regression Method to Infer Psych...   \n",
       "27  I CAN HAS SUPERCOMPUTER? A Novel Approach to T...   \n",
       "28    Enriching Word Vectors with Subword Information   \n",
       "29  Do You See What I Mean? Visual Resolution of L...   \n",
       "30  New word analogy corpus for exploring embeddin...   \n",
       "31  Rule based Part of speech Tagger for Homoeopat...   \n",
       "32           A complex network approach to stylometry   \n",
       "33  The Generation of Textual Entailment with NLML...   \n",
       "34  Enriching Conversation Context in Retrieval-ba...   \n",
       "35  Self-attention based end-to-end Hindi-English ...   \n",
       "36        Language models and Automated Essay Scoring   \n",
       "37  Topics to Avoid: Demoting Latent Confounds in ...   \n",
       "38                                     Mogrifier LSTM   \n",
       "39  Fine-tuning BERT for Joint Entity and Relation...   \n",
       "40  uniblock: Scoring and Filtering Corpus with Un...   \n",
       "41  Build it Break it Fix it for Dialogue Safety: ...   \n",
       "42  EmotionX-KU: BERT-Max based Contextual Emotion...   \n",
       "43  Program Synthesis and Semantic Parsing with Le...   \n",
       "44           Plain English Summarization of Contracts   \n",
       "45  Out-of-Vocabulary Embedding Imputation with Gr...   \n",
       "46  Evaluation of basic modules for isolated spell...   \n",
       "47  ERNIE 2.0: A Continual Pre-training Framework ...   \n",
       "48  Feature Fusion Effects of Tensor Product Repre...   \n",
       "49  Attention-based Encoder-Decoder Networks for S...   \n",
       "\n",
       "                              PDF URL  \\\n",
       "0   http://arxiv.org/pdf/1809.09190v1   \n",
       "1   http://arxiv.org/pdf/1903.10625v2   \n",
       "2   http://arxiv.org/pdf/1904.04307v1   \n",
       "3   http://arxiv.org/pdf/1809.02794v3   \n",
       "4   http://arxiv.org/pdf/1805.01083v1   \n",
       "5   http://arxiv.org/pdf/1903.02784v1   \n",
       "6   http://arxiv.org/pdf/1807.00267v1   \n",
       "7   http://arxiv.org/pdf/1805.11818v1   \n",
       "8   http://arxiv.org/pdf/1802.09968v2   \n",
       "9   http://arxiv.org/pdf/1805.05588v1   \n",
       "10  http://arxiv.org/pdf/1807.11838v1   \n",
       "11  http://arxiv.org/pdf/1708.01009v1   \n",
       "12  http://arxiv.org/pdf/1706.03530v1   \n",
       "13  http://arxiv.org/pdf/1707.04244v1   \n",
       "14  http://arxiv.org/pdf/1712.08917v1   \n",
       "15  http://arxiv.org/pdf/1804.07827v2   \n",
       "16  http://arxiv.org/pdf/1804.03052v1   \n",
       "17  http://arxiv.org/pdf/1512.05919v2   \n",
       "18  http://arxiv.org/pdf/1803.08966v1   \n",
       "19  http://arxiv.org/pdf/1609.02960v1   \n",
       "20  http://arxiv.org/pdf/1601.01195v1   \n",
       "21   http://arxiv.org/pdf/1001.4273v1   \n",
       "22  http://arxiv.org/pdf/1603.07771v3   \n",
       "23   http://arxiv.org/pdf/1312.3168v1   \n",
       "24  http://arxiv.org/pdf/1602.07749v1   \n",
       "25  http://arxiv.org/pdf/1803.00124v2   \n",
       "26  http://arxiv.org/pdf/1705.07008v1   \n",
       "27  http://arxiv.org/pdf/1703.10242v1   \n",
       "28  http://arxiv.org/pdf/1607.04606v2   \n",
       "29  http://arxiv.org/pdf/1603.08079v1   \n",
       "30  http://arxiv.org/pdf/1608.00789v1   \n",
       "31   http://arxiv.org/pdf/1111.5293v1   \n",
       "32  http://arxiv.org/pdf/1506.09107v2   \n",
       "33   http://arxiv.org/pdf/0802.4326v1   \n",
       "34  http://arxiv.org/pdf/1911.02290v1   \n",
       "35  http://arxiv.org/pdf/1909.09779v1   \n",
       "36  http://arxiv.org/pdf/1909.09482v1   \n",
       "37  http://arxiv.org/pdf/1909.00453v2   \n",
       "38  http://arxiv.org/pdf/1909.01792v2   \n",
       "39  http://arxiv.org/pdf/1908.07721v2   \n",
       "40  http://arxiv.org/pdf/1908.09716v1   \n",
       "41  http://arxiv.org/pdf/1908.06083v1   \n",
       "42  http://arxiv.org/pdf/1906.11565v2   \n",
       "43  http://arxiv.org/pdf/1906.10816v4   \n",
       "44  http://arxiv.org/pdf/1906.00424v1   \n",
       "45  http://arxiv.org/pdf/1906.03753v2   \n",
       "46  http://arxiv.org/pdf/1905.10810v1   \n",
       "47  http://arxiv.org/pdf/1907.12412v2   \n",
       "48  http://arxiv.org/pdf/1812.06624v1   \n",
       "49  http://arxiv.org/pdf/1810.00660v1   \n",
       "\n",
       "                                               Author  \\\n",
       "0   [arxiv.Result.Author('Parisa Haghani'), arxiv....   \n",
       "1   [arxiv.Result.Author('Felix Stahlberg'), arxiv...   \n",
       "2   [arxiv.Result.Author('Ponrudee Netisopakul'), ...   \n",
       "3   [arxiv.Result.Author('Zhuosheng Zhang'), arxiv...   \n",
       "4   [arxiv.Result.Author('Xiaolan Wang'), arxiv.Re...   \n",
       "5   [arxiv.Result.Author('Imane Guellil'), arxiv.R...   \n",
       "6   [arxiv.Result.Author('Raghav Gupta'), arxiv.Re...   \n",
       "7   [arxiv.Result.Author('Volkan Cirik'), arxiv.Re...   \n",
       "8   [arxiv.Result.Author('Chieh-Teng Chang'), arxi...   \n",
       "9   [arxiv.Result.Author('Bingfeng Luo'), arxiv.Re...   \n",
       "10          [arxiv.Result.Author('Jonathan Connell')]   \n",
       "11  [arxiv.Result.Author('Stephen Merity'), arxiv....   \n",
       "12  [arxiv.Result.Author('Ildik Piln'), arxiv.Re...   \n",
       "13  [arxiv.Result.Author('Preeti Bhargava'), arxiv...   \n",
       "14  [arxiv.Result.Author('Henrico Bertini Brum'), ...   \n",
       "15  [arxiv.Result.Author('Liyuan Liu'), arxiv.Resu...   \n",
       "16  [arxiv.Result.Author('David Harwath'), arxiv.R...   \n",
       "17  [arxiv.Result.Author('Bing Qin'), arxiv.Result...   \n",
       "18  [arxiv.Result.Author('Lu Feng'), arxiv.Result....   \n",
       "19  [arxiv.Result.Author('Salam Khalifa'), arxiv.R...   \n",
       "20              [arxiv.Result.Author('Kamal Sarkar')]   \n",
       "21  [arxiv.Result.Author('Siddhartha Jonnalagadda'...   \n",
       "22  [arxiv.Result.Author('Remi Lebret'), arxiv.Res...   \n",
       "23  [arxiv.Result.Author('Bruno Mery'), arxiv.Resu...   \n",
       "24  [arxiv.Result.Author('Thien Huu Nguyen'), arxi...   \n",
       "25  [arxiv.Result.Author('Abdulaziz M. Alayba'), a...   \n",
       "26  [arxiv.Result.Author('Leandro B. dos Santos'),...   \n",
       "27  [arxiv.Result.Author('David Richie'), arxiv.Re...   \n",
       "28  [arxiv.Result.Author('Piotr Bojanowski'), arxi...   \n",
       "29  [arxiv.Result.Author('Yevgeni Berzak'), arxiv....   \n",
       "30  [arxiv.Result.Author('Luk Svoboda'), arxiv.R...   \n",
       "31  [arxiv.Result.Author('Sanjay K. Dwivedi'), arx...   \n",
       "32          [arxiv.Result.Author('Diego R. Amancio')]   \n",
       "33                 [arxiv.Result.Author('Jiyou Jia')]   \n",
       "34  [arxiv.Result.Author('Amir Vakili Tahami'), ar...   \n",
       "35  [arxiv.Result.Author('Siddhant Srivastava'), a...   \n",
       "36  [arxiv.Result.Author('Pedro Uria Rodriguez'), ...   \n",
       "37  [arxiv.Result.Author('Sachin Kumar'), arxiv.Re...   \n",
       "38  [arxiv.Result.Author('Gbor Melis'), arxiv.Res...   \n",
       "39  [arxiv.Result.Author('Kui Xue'), arxiv.Result....   \n",
       "40  [arxiv.Result.Author('Yingbo Gao'), arxiv.Resu...   \n",
       "41  [arxiv.Result.Author('Emily Dinan'), arxiv.Res...   \n",
       "42  [arxiv.Result.Author('Kisu Yang'), arxiv.Resul...   \n",
       "43  [arxiv.Result.Author('Richard Shin'), arxiv.Re...   \n",
       "44  [arxiv.Result.Author('Laura Manor'), arxiv.Res...   \n",
       "45  [arxiv.Result.Author('Ziyi Yang'), arxiv.Resul...   \n",
       "46          [arxiv.Result.Author('Szymon Rutkowski')]   \n",
       "47  [arxiv.Result.Author('Yu Sun'), arxiv.Result.A...   \n",
       "48             [arxiv.Result.Author('Chiranjib Sur')]   \n",
       "49               [arxiv.Result.Author('Sina Ahmadi')]   \n",
       "\n",
       "                             DOI             Published Date  \\\n",
       "0                            NaN  2018-09-24 19:46:24+00:00   \n",
       "1                            NaN  2019-03-25 23:05:11+00:00   \n",
       "2                            NaN  2019-04-08 19:18:09+00:00   \n",
       "3                            NaN  2018-09-08 12:34:59+00:00   \n",
       "4                            NaN  2018-05-03 01:57:31+00:00   \n",
       "5   10.1016/j.jksuci.2019.02.006  2019-03-07 09:22:35+00:00   \n",
       "6                            NaN  2018-07-01 04:11:18+00:00   \n",
       "7                            NaN  2018-05-30 06:03:21+00:00   \n",
       "8                            NaN  2018-02-27 15:31:11+00:00   \n",
       "9                            NaN  2018-05-15 06:40:44+00:00   \n",
       "10                           NaN  2018-07-31 14:31:17+00:00   \n",
       "11                           NaN  2017-08-03 05:53:53+00:00   \n",
       "12                           NaN  2017-06-12 09:21:45+00:00   \n",
       "13                           NaN  2017-07-13 17:52:51+00:00   \n",
       "14                           NaN  2017-12-24 13:23:58+00:00   \n",
       "15                           NaN  2018-04-20 21:10:17+00:00   \n",
       "16                           NaN  2018-04-09 15:15:37+00:00   \n",
       "17                           NaN  2015-12-18 12:10:42+00:00   \n",
       "18                           NaN  2018-03-23 20:14:51+00:00   \n",
       "19                           NaN  2016-09-09 22:22:53+00:00   \n",
       "20                           NaN  2016-01-06 14:40:38+00:00   \n",
       "21                           NaN  2010-01-24 20:23:10+00:00   \n",
       "22                           NaN  2016-03-24 22:40:00+00:00   \n",
       "23                           NaN  2013-12-11 14:04:52+00:00   \n",
       "24                           NaN  2016-02-24 23:14:01+00:00   \n",
       "25     10.1109/ASAR.2018.8480191  2018-02-28 22:46:19+00:00   \n",
       "26                           NaN  2017-05-19 14:17:31+00:00   \n",
       "27                           NaN  2017-03-29 20:42:28+00:00   \n",
       "28                           NaN  2016-07-15 18:27:55+00:00   \n",
       "29                           NaN  2016-03-26 06:49:33+00:00   \n",
       "30                           NaN  2016-08-02 12:31:06+00:00   \n",
       "31                           NaN  2011-11-13 18:19:15+00:00   \n",
       "32  10.1371/journal.pone.0136076  2015-06-30 14:32:30+00:00   \n",
       "33                           NaN  2008-02-29 06:16:29+00:00   \n",
       "34                           NaN  2019-11-06 10:24:45+00:00   \n",
       "35                           NaN  2019-09-21 06:16:52+00:00   \n",
       "36                           NaN  2019-09-18 18:50:18+00:00   \n",
       "37                           NaN  2019-09-01 19:18:44+00:00   \n",
       "38                           NaN  2019-09-04 13:32:23+00:00   \n",
       "39                           NaN  2019-08-21 06:56:08+00:00   \n",
       "40                           NaN  2019-08-26 14:55:03+00:00   \n",
       "41                           NaN  2019-08-17 18:34:11+00:00   \n",
       "42                           NaN  2019-06-27 11:46:48+00:00   \n",
       "43                           NaN  2019-06-26 02:28:10+00:00   \n",
       "44                           NaN  2019-06-02 15:27:51+00:00   \n",
       "45                           NaN  2019-06-10 01:10:34+00:00   \n",
       "46                           NaN  2019-05-26 14:54:52+00:00   \n",
       "47                           NaN  2019-07-29 13:25:37+00:00   \n",
       "48                           NaN  2018-12-17 06:24:03+00:00   \n",
       "49                           NaN  2018-09-21 23:47:42+00:00   \n",
       "\n",
       "                                             Abstract  \\\n",
       "0   Conventional spoken language understanding sys...   \n",
       "1   Grammatical error correction (GEC) is one of t...   \n",
       "2   Distributional semantics in the form of word e...   \n",
       "3   Who did what to whom is a major focus in natur...   \n",
       "4   We present the KOKO system that takes declarat...   \n",
       "5   Arabic is recognised as the 4th most used lang...   \n",
       "6   In task-oriented dialogue systems, spoken lang...   \n",
       "7   We present an empirical analysis of the state-...   \n",
       "8   Automatic abstractive text summarization is an...   \n",
       "9   The success of many natural language processin...   \n",
       "10  Spoken language is a convenient interface for ...   \n",
       "11  Recurrent neural networks (RNNs) serve as a fu...   \n",
       "12  We present a framework and its implementation ...   \n",
       "13  In this paper, we describe the Lithium Natural...   \n",
       "14  The large amount of data available in social m...   \n",
       "15  Many efforts have been made to facilitate natu...   \n",
       "16  In this paper, we explore the learning of neur...   \n",
       "17  Generating an article automatically with compu...   \n",
       "18  Automated techniques such as model checking ha...   \n",
       "19  Most Arabic natural language processing tools ...   \n",
       "20  This paper discusses the experiments carried o...   \n",
       "21  Accurate systems for extracting Protein-Protei...   \n",
       "22  This paper introduces a neural model for conce...   \n",
       "23  We propose a cognitively and linguistically mo...   \n",
       "24  One of the key challenges in natural language ...   \n",
       "25  The complexities of Arabic language in morphol...   \n",
       "26  Psycholinguistic properties of words have been...   \n",
       "27  A novel approach is presented to teach the par...   \n",
       "28  Continuous word representations, trained on la...   \n",
       "29  Understanding language goes hand in hand with ...   \n",
       "30  The word embedding methods have been proven to...   \n",
       "31  A tagger is a mandatory segment of most text s...   \n",
       "32  Statistical methods have been widely employed ...   \n",
       "33  This research report introduces the generation...   \n",
       "34  Work on retrieval-based chatbots, like most se...   \n",
       "35  Machine Translation (MT) is a zone of concentr...   \n",
       "36  In this paper, we present a new comparative st...   \n",
       "37  Despite impressive performance on many text cl...   \n",
       "38  Many advances in Natural Language Processing h...   \n",
       "39  Entity and relation extraction is the necessar...   \n",
       "40  The preprocessing pipelines in Natural Languag...   \n",
       "41  The detection of offensive language in the con...   \n",
       "42  We propose a contextual emotion classifier bas...   \n",
       "43  Program synthesis of general-purpose source co...   \n",
       "44  Unilateral contracts, such as terms of service...   \n",
       "45  Due to the ubiquitous use of embeddings as inp...   \n",
       "46  Spelling error correction is an important prob...   \n",
       "47  Recently, pre-trained models have achieved sta...   \n",
       "48  Progress in image captioning is gradually gett...   \n",
       "49  Automatic spelling and grammatical correction ...   \n",
       "\n",
       "                                          Journal Ref Primary Category  \\\n",
       "0                                                 NaN          eess.AS   \n",
       "1                                                 NaN            cs.CL   \n",
       "2                                                 NaN            cs.CL   \n",
       "3                                                 NaN            cs.CL   \n",
       "4                                                 NaN            cs.DB   \n",
       "5                                                 NaN            cs.CL   \n",
       "6                                                 NaN            cs.CL   \n",
       "7                                                 NaN            cs.CL   \n",
       "8                                                 NaN            cs.CL   \n",
       "9                                                 NaN            cs.CL   \n",
       "10                                                NaN            cs.RO   \n",
       "11                                                NaN            cs.CL   \n",
       "12                                                NaN            cs.CL   \n",
       "13                                                NaN            cs.AI   \n",
       "14                                                NaN            cs.CL   \n",
       "15                                                NaN            cs.CL   \n",
       "16                                                NaN            cs.CL   \n",
       "17                                                NaN            cs.CL   \n",
       "18                                                NaN            cs.RO   \n",
       "19                                                NaN            cs.CL   \n",
       "20                                                NaN            cs.CL   \n",
       "21  The 3rd International Symposium on Languages i...            cs.CL   \n",
       "22                                                NaN            cs.CL   \n",
       "23  NLPCS '10- 10th International Workshop on Natu...            cs.CL   \n",
       "24                                                NaN            cs.CL   \n",
       "25  Proc. 2nd International Workshop on Arabic and...            cs.CL   \n",
       "26                                                NaN            cs.CL   \n",
       "27                                                NaN            cs.DC   \n",
       "28                                                NaN            cs.CL   \n",
       "29  Conference on Empirical Methods in Natural Lan...            cs.CV   \n",
       "30                                                NaN            cs.CL   \n",
       "31                                                NaN            cs.CL   \n",
       "32                     PLoS ONE 10(8): e0136076, 2015            cs.CL   \n",
       "33                                                NaN            cs.CL   \n",
       "34                                                NaN            cs.CL   \n",
       "35                                                NaN            cs.CL   \n",
       "36                                                NaN            cs.CL   \n",
       "37                                                NaN            cs.LG   \n",
       "38                                                NaN            cs.CL   \n",
       "39                                                NaN            cs.CL   \n",
       "40                                                NaN            cs.CL   \n",
       "41                                                NaN            cs.CL   \n",
       "42                                                NaN            cs.CL   \n",
       "43                                                NaN            cs.LG   \n",
       "44                                                NaN            cs.CL   \n",
       "45                                                NaN            cs.CL   \n",
       "46  Human Language Technologies as a Challenge for...            cs.CL   \n",
       "47                                                NaN            cs.CL   \n",
       "48                                                NaN            cs.CV   \n",
       "49                                                NaN            cs.CL   \n",
       "\n",
       "                                           Category  \\\n",
       "0                     ['eess.AS', 'cs.CL', 'cs.SD']   \n",
       "1                                         ['cs.CL']   \n",
       "2                                         ['cs.CL']   \n",
       "3                                         ['cs.CL']   \n",
       "4                                ['cs.DB', 'cs.CL']   \n",
       "5                                         ['cs.CL']   \n",
       "6                                         ['cs.CL']   \n",
       "7              ['cs.CL', 'cs.AI', 'cs.CV', 'cs.NE']   \n",
       "8                                         ['cs.CL']   \n",
       "9                                         ['cs.CL']   \n",
       "10                      ['cs.RO', 'cs.AI', 'cs.CL']   \n",
       "11                               ['cs.CL', 'cs.NE']   \n",
       "12                                        ['cs.CL']   \n",
       "13                      ['cs.AI', 'cs.CL', 'cs.IR']   \n",
       "14                                        ['cs.CL']   \n",
       "15                                        ['cs.CL']   \n",
       "16                    ['cs.CL', 'cs.SD', 'eess.AS']   \n",
       "17                                        ['cs.CL']   \n",
       "18                      ['cs.RO', 'cs.CL', 'cs.FL']   \n",
       "19                                        ['cs.CL']   \n",
       "20                               ['cs.CL', '68T50']   \n",
       "21                                        ['cs.CL']   \n",
       "22                                        ['cs.CL']   \n",
       "23                                        ['cs.CL']   \n",
       "24                                        ['cs.CL']   \n",
       "25                        ['cs.CL', 'I.2.7; I.2.6']   \n",
       "26                                        ['cs.CL']   \n",
       "27                               ['cs.DC', 'cs.PL']   \n",
       "28                               ['cs.CL', 'cs.LG']   \n",
       "29                      ['cs.CV', 'cs.AI', 'cs.CL']   \n",
       "30                                        ['cs.CL']   \n",
       "31                                        ['cs.CL']   \n",
       "32                                        ['cs.CL']   \n",
       "33                      ['cs.CL', 'cs.AI', 'cs.CY']   \n",
       "34                                        ['cs.CL']   \n",
       "35                                        ['cs.CL']   \n",
       "36                    ['cs.CL', 'cs.LG', 'stat.ML']   \n",
       "37                    ['cs.LG', 'cs.CL', 'stat.ML']   \n",
       "38                                        ['cs.CL']   \n",
       "39                                        ['cs.CL']   \n",
       "40                                        ['cs.CL']   \n",
       "41                                        ['cs.CL']   \n",
       "42                                        ['cs.CL']   \n",
       "43  ['cs.LG', 'cs.AI', 'cs.CL', 'cs.PL', 'stat.ML']   \n",
       "44                                        ['cs.CL']   \n",
       "45                                        ['cs.CL']   \n",
       "46                                        ['cs.CL']   \n",
       "47                                        ['cs.CL']   \n",
       "48                               ['cs.CV', 'cs.CL']   \n",
       "49                               ['cs.CL', 'cs.AI']   \n",
       "\n",
       "                             Entry ID _merge  \n",
       "0   http://arxiv.org/abs/1809.09190v1   both  \n",
       "1   http://arxiv.org/abs/1903.10625v2   both  \n",
       "2   http://arxiv.org/abs/1904.04307v1   both  \n",
       "3   http://arxiv.org/abs/1809.02794v3   both  \n",
       "4   http://arxiv.org/abs/1805.01083v1   both  \n",
       "5   http://arxiv.org/abs/1903.02784v1   both  \n",
       "6   http://arxiv.org/abs/1807.00267v1   both  \n",
       "7   http://arxiv.org/abs/1805.11818v1   both  \n",
       "8   http://arxiv.org/abs/1802.09968v2   both  \n",
       "9   http://arxiv.org/abs/1805.05588v1   both  \n",
       "10  http://arxiv.org/abs/1807.11838v1   both  \n",
       "11  http://arxiv.org/abs/1708.01009v1   both  \n",
       "12  http://arxiv.org/abs/1706.03530v1   both  \n",
       "13  http://arxiv.org/abs/1707.04244v1   both  \n",
       "14  http://arxiv.org/abs/1712.08917v1   both  \n",
       "15  http://arxiv.org/abs/1804.07827v2   both  \n",
       "16  http://arxiv.org/abs/1804.03052v1   both  \n",
       "17  http://arxiv.org/abs/1512.05919v2   both  \n",
       "18  http://arxiv.org/abs/1803.08966v1   both  \n",
       "19  http://arxiv.org/abs/1609.02960v1   both  \n",
       "20  http://arxiv.org/abs/1601.01195v1   both  \n",
       "21   http://arxiv.org/abs/1001.4273v1   both  \n",
       "22  http://arxiv.org/abs/1603.07771v3   both  \n",
       "23   http://arxiv.org/abs/1312.3168v1   both  \n",
       "24  http://arxiv.org/abs/1602.07749v1   both  \n",
       "25  http://arxiv.org/abs/1803.00124v2   both  \n",
       "26  http://arxiv.org/abs/1705.07008v1   both  \n",
       "27  http://arxiv.org/abs/1703.10242v1   both  \n",
       "28  http://arxiv.org/abs/1607.04606v2   both  \n",
       "29  http://arxiv.org/abs/1603.08079v1   both  \n",
       "30  http://arxiv.org/abs/1608.00789v1   both  \n",
       "31   http://arxiv.org/abs/1111.5293v1   both  \n",
       "32  http://arxiv.org/abs/1506.09107v2   both  \n",
       "33   http://arxiv.org/abs/0802.4326v1   both  \n",
       "34  http://arxiv.org/abs/1911.02290v1   both  \n",
       "35  http://arxiv.org/abs/1909.09779v1   both  \n",
       "36  http://arxiv.org/abs/1909.09482v1   both  \n",
       "37  http://arxiv.org/abs/1909.00453v2   both  \n",
       "38  http://arxiv.org/abs/1909.01792v2   both  \n",
       "39  http://arxiv.org/abs/1908.07721v2   both  \n",
       "40  http://arxiv.org/abs/1908.09716v1   both  \n",
       "41  http://arxiv.org/abs/1908.06083v1   both  \n",
       "42  http://arxiv.org/abs/1906.11565v2   both  \n",
       "43  http://arxiv.org/abs/1906.10816v4   both  \n",
       "44  http://arxiv.org/abs/1906.00424v1   both  \n",
       "45  http://arxiv.org/abs/1906.03753v2   both  \n",
       "46  http://arxiv.org/abs/1905.10810v1   both  \n",
       "47  http://arxiv.org/abs/1907.12412v2   both  \n",
       "48  http://arxiv.org/abs/1812.06624v1   both  \n",
       "49  http://arxiv.org/abs/1810.00660v1   both  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "940cc19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'original_df' is your original dataframe and 'lda_output' is the output from LDA\n",
    "merged_df = df.merge(df_topic_sents_keywords, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7018c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Title</th>\n",
       "      <th>PDF URL</th>\n",
       "      <th>Author</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Journal Ref</th>\n",
       "      <th>Primary Category</th>\n",
       "      <th>Category</th>\n",
       "      <th>Entry ID</th>\n",
       "      <th>_merge</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1809.09190v1.pdf</td>\n",
       "      <td>in this paper we formulate audio play hot n c...</td>\n",
       "      <td>From Audio to Semantics: Approaches to end-to-...</td>\n",
       "      <td>http://arxiv.org/pdf/1809.09190v1</td>\n",
       "      <td>[arxiv.Result.Author('Parisa Haghani'), arxiv....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-09-24 19:46:24+00:00</td>\n",
       "      <td>Conventional spoken language understanding sys...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eess.AS</td>\n",
       "      <td>['eess.AS', 'cs.CL', 'cs.SD']</td>\n",
       "      <td>http://arxiv.org/abs/1809.09190v1</td>\n",
       "      <td>both</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>model, task, use, propose, write, paper, syste...</td>\n",
       "      <td>[paper, audio, play, hot, cold, set, alarm, pm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1903.10625v2.pdf</td>\n",
       "      <td>finite state transducers fst are an efficient...</td>\n",
       "      <td>Neural Grammatical Error Correction with Finit...</td>\n",
       "      <td>http://arxiv.org/pdf/1903.10625v2</td>\n",
       "      <td>[arxiv.Result.Author('Felix Stahlberg'), arxiv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-25 23:05:11+00:00</td>\n",
       "      <td>Grammatical error correction (GEC) is one of t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1903.10625v2</td>\n",
       "      <td>both</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>model, task, use, propose, write, paper, syste...</td>\n",
       "      <td>[state, transducer, efficient, way, represent,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1904.04307v1.pdf</td>\n",
       "      <td>the quantification of semantic similarity bet...</td>\n",
       "      <td>Word Similarity Datasets for Thai: Constructio...</td>\n",
       "      <td>http://arxiv.org/pdf/1904.04307v1</td>\n",
       "      <td>[arxiv.Result.Author('Ponrudee Netisopakul'), ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-08 19:18:09+00:00</td>\n",
       "      <td>Distributional semantics in the form of word e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1904.04307v1</td>\n",
       "      <td>both</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9957</td>\n",
       "      <td>use, model, semantic, task, framework, system,...</td>\n",
       "      <td>[quantification, semantic, similarity, word, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1809.02794v3.pdf</td>\n",
       "      <td>this paper focuses on the aim of semantic rol...</td>\n",
       "      <td>Explicit Contextual Semantics for Text Compreh...</td>\n",
       "      <td>http://arxiv.org/pdf/1809.02794v3</td>\n",
       "      <td>[arxiv.Result.Author('Zhuosheng Zhang'), arxiv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-09-08 12:34:59+00:00</td>\n",
       "      <td>Who did what to whom is a major focus in natur...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1809.02794v3</td>\n",
       "      <td>both</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>model, task, text, work, base, project, key, p...</td>\n",
       "      <td>[paper, focus, aim, semantic, role, label, lab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1805.01083v1.pdf</td>\n",
       "      <td>the mainconstructusedinextractionlanguagesand...</td>\n",
       "      <td>Scalable Semantic Querying of Text</td>\n",
       "      <td>http://arxiv.org/pdf/1805.01083v1</td>\n",
       "      <td>[arxiv.Result.Author('Xiaolan Wang'), arxiv.Re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-05-03 01:57:31+00:00</td>\n",
       "      <td>We present the KOKO system that takes declarat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.DB</td>\n",
       "      <td>['cs.DB', 'cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1805.01083v1</td>\n",
       "      <td>both</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9883</td>\n",
       "      <td>use, model, semantic, task, framework, system,...</td>\n",
       "      <td>[also, new, level, incorporating, advance, wor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Filename                                            Summary  \\\n",
       "0  1809.09190v1.pdf   in this paper we formulate audio play hot n c...   \n",
       "1  1903.10625v2.pdf   finite state transducers fst are an efficient...   \n",
       "2  1904.04307v1.pdf   the quantification of semantic similarity bet...   \n",
       "3  1809.02794v3.pdf   this paper focuses on the aim of semantic rol...   \n",
       "4  1805.01083v1.pdf   the mainconstructusedinextractionlanguagesand...   \n",
       "\n",
       "                                               Title  \\\n",
       "0  From Audio to Semantics: Approaches to end-to-...   \n",
       "1  Neural Grammatical Error Correction with Finit...   \n",
       "2  Word Similarity Datasets for Thai: Constructio...   \n",
       "3  Explicit Contextual Semantics for Text Compreh...   \n",
       "4                 Scalable Semantic Querying of Text   \n",
       "\n",
       "                             PDF URL  \\\n",
       "0  http://arxiv.org/pdf/1809.09190v1   \n",
       "1  http://arxiv.org/pdf/1903.10625v2   \n",
       "2  http://arxiv.org/pdf/1904.04307v1   \n",
       "3  http://arxiv.org/pdf/1809.02794v3   \n",
       "4  http://arxiv.org/pdf/1805.01083v1   \n",
       "\n",
       "                                              Author  DOI  \\\n",
       "0  [arxiv.Result.Author('Parisa Haghani'), arxiv....  NaN   \n",
       "1  [arxiv.Result.Author('Felix Stahlberg'), arxiv...  NaN   \n",
       "2  [arxiv.Result.Author('Ponrudee Netisopakul'), ...  NaN   \n",
       "3  [arxiv.Result.Author('Zhuosheng Zhang'), arxiv...  NaN   \n",
       "4  [arxiv.Result.Author('Xiaolan Wang'), arxiv.Re...  NaN   \n",
       "\n",
       "              Published Date  \\\n",
       "0  2018-09-24 19:46:24+00:00   \n",
       "1  2019-03-25 23:05:11+00:00   \n",
       "2  2019-04-08 19:18:09+00:00   \n",
       "3  2018-09-08 12:34:59+00:00   \n",
       "4  2018-05-03 01:57:31+00:00   \n",
       "\n",
       "                                            Abstract Journal Ref  \\\n",
       "0  Conventional spoken language understanding sys...         NaN   \n",
       "1  Grammatical error correction (GEC) is one of t...         NaN   \n",
       "2  Distributional semantics in the form of word e...         NaN   \n",
       "3  Who did what to whom is a major focus in natur...         NaN   \n",
       "4  We present the KOKO system that takes declarat...         NaN   \n",
       "\n",
       "  Primary Category                       Category  \\\n",
       "0          eess.AS  ['eess.AS', 'cs.CL', 'cs.SD']   \n",
       "1            cs.CL                      ['cs.CL']   \n",
       "2            cs.CL                      ['cs.CL']   \n",
       "3            cs.CL                      ['cs.CL']   \n",
       "4            cs.DB             ['cs.DB', 'cs.CL']   \n",
       "\n",
       "                            Entry ID _merge  Dominant_Topic  \\\n",
       "0  http://arxiv.org/abs/1809.09190v1   both               1   \n",
       "1  http://arxiv.org/abs/1903.10625v2   both               1   \n",
       "2  http://arxiv.org/abs/1904.04307v1   both               0   \n",
       "3  http://arxiv.org/abs/1809.02794v3   both               4   \n",
       "4  http://arxiv.org/abs/1805.01083v1   both               0   \n",
       "\n",
       "   Perc_Contribution                                     Topic_Keywords  \\\n",
       "0             0.9900  model, task, use, propose, write, paper, syste...   \n",
       "1             0.9945  model, task, use, propose, write, paper, syste...   \n",
       "2             0.9957  use, model, semantic, task, framework, system,...   \n",
       "3             0.9950  model, task, text, work, base, project, key, p...   \n",
       "4             0.9883  use, model, semantic, task, framework, system,...   \n",
       "\n",
       "                                                   0  \n",
       "0  [paper, audio, play, hot, cold, set, alarm, pm...  \n",
       "1  [state, transducer, efficient, way, represent,...  \n",
       "2  [quantification, semantic, similarity, word, u...  \n",
       "3  [paper, focus, aim, semantic, role, label, lab...  \n",
       "4  [also, new, level, incorporating, advance, wor...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53de37ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('../data/Dominant_topics_of_summaries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b35a269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaa810f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f71e48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd9f82fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dglover\\AppData\\Local\\Temp\\ipykernel_15532\\3153082663.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=texts)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8bcdeb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.to_csv('../data/Dominant_topics_of_absracts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea71fbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9919</td>\n",
       "      <td>use, method, sentence, research, classificatio...</td>\n",
       "      <td>[uniblock, scoring, filter, information, train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9886</td>\n",
       "      <td>use, method, sentence, research, classificatio...</td>\n",
       "      <td>[troll, public, commurum, fix, offensive, adve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>model, task, text, work, base, project, key, p...</td>\n",
       "      <td>[base, contextual, emotion, classifier, contex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9889</td>\n",
       "      <td>use, model, semantic, task, framework, system,...</td>\n",
       "      <td>[program, synthesis, semantic, parsing, learn,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9932</td>\n",
       "      <td>model, task, use, propose, write, paper, syste...</td>\n",
       "      <td>[unilateral, contract, term, service, play, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9943</td>\n",
       "      <td>model, task, text, work, base, project, key, p...</td>\n",
       "      <td>[outofvocabulary, embed, imputation, ground, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>use, model, semantic, task, framework, system,...</td>\n",
       "      <td>[evaluation, basic, module, isolate, spelling,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>use, model, semantic, task, framework, system,...</td>\n",
       "      <td>[pretraining, framework, understanding, framew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>use, model, semantic, task, framework, system,...</td>\n",
       "      <td>[new, model, achieve, considerable, deal, spec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>model, task, use, propose, write, paper, syste...</td>\n",
       "      <td>[aim, build, neural, network, model, task, err...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "40           40               3              0.9919   \n",
       "41           41               3              0.9886   \n",
       "42           42               4              0.9905   \n",
       "43           43               0              0.9889   \n",
       "44           44               1              0.9932   \n",
       "45           45               4              0.9943   \n",
       "46           46               0              0.9905   \n",
       "47           47               0              0.9913   \n",
       "48           48               0              0.9900   \n",
       "49           49               1              0.9930   \n",
       "\n",
       "                                             Keywords  \\\n",
       "40  use, method, sentence, research, classificatio...   \n",
       "41  use, method, sentence, research, classificatio...   \n",
       "42  model, task, text, work, base, project, key, p...   \n",
       "43  use, model, semantic, task, framework, system,...   \n",
       "44  model, task, use, propose, write, paper, syste...   \n",
       "45  model, task, text, work, base, project, key, p...   \n",
       "46  use, model, semantic, task, framework, system,...   \n",
       "47  use, model, semantic, task, framework, system,...   \n",
       "48  use, model, semantic, task, framework, system,...   \n",
       "49  model, task, use, propose, write, paper, syste...   \n",
       "\n",
       "                                                 Text  \n",
       "40  [uniblock, scoring, filter, information, train...  \n",
       "41  [troll, public, commurum, fix, offensive, adve...  \n",
       "42  [base, contextual, emotion, classifier, contex...  \n",
       "43  [program, synthesis, semantic, parsing, learn,...  \n",
       "44  [unilateral, contract, term, service, play, su...  \n",
       "45  [outofvocabulary, embed, imputation, ground, g...  \n",
       "46  [evaluation, basic, module, isolate, spelling,...  \n",
       "47  [pretraining, framework, understanding, framew...  \n",
       "48  [new, model, achieve, considerable, deal, spec...  \n",
       "49  [aim, build, neural, network, model, task, err...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dominant_topic.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4634e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    13\n",
       "4    13\n",
       "1    11\n",
       "3     8\n",
       "2     5\n",
       "Name: Dominant_Topic, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dominant_topic['Dominant_Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3db13218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9959</td>\n",
       "      <td>use, model, semantic, task, framework, system,...</td>\n",
       "      <td>[novel, teaching, parallel, distribute, comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9959</td>\n",
       "      <td>model, task, use, propose, write, paper, syste...</td>\n",
       "      <td>[automate, technique, model, check, violation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.9937</td>\n",
       "      <td>system, lithium, lm, usage, sentence, user, ri...</td>\n",
       "      <td>[lithium, extract, rich, set, develop, content...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.9952</td>\n",
       "      <td>use, method, sentence, research, classificatio...</td>\n",
       "      <td>[robot, perform, simple, fetchandcarry, task, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9957</td>\n",
       "      <td>model, task, text, work, base, project, key, p...</td>\n",
       "      <td>[plan, base, framework, essay, generation, aim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Num  Topic_Perc_Contrib  \\\n",
       "0          0              0.9959   \n",
       "1          1              0.9959   \n",
       "2          2              0.9937   \n",
       "3          3              0.9952   \n",
       "4          4              0.9957   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  use, model, semantic, task, framework, system,...   \n",
       "1  model, task, use, propose, write, paper, syste...   \n",
       "2  system, lithium, lm, usage, sentence, user, ri...   \n",
       "3  use, method, sentence, research, classificatio...   \n",
       "4  model, task, text, work, base, project, key, p...   \n",
       "\n",
       "                                                Text  \n",
       "0  [novel, teaching, parallel, distribute, comput...  \n",
       "1  [automate, technique, model, check, violation,...  \n",
       "2  [lithium, extract, rich, set, develop, content...  \n",
       "3  [robot, perform, simple, fetchandcarry, task, ...  \n",
       "4  [plan, base, framework, essay, generation, aim...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c0c711e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Representative Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9959</td>\n",
       "      <td>use, model, semantic, task, framework, system, word, propose, lexical, work</td>\n",
       "      <td>[novel, teaching, parallel, distribute, computing, concept, use, memebase, programming, lolcode,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9959</td>\n",
       "      <td>model, task, use, propose, write, paper, system, neural, set, base</td>\n",
       "      <td>[automate, technique, model, check, violation, probability, reach, error, state, exceed, use, ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.9937</td>\n",
       "      <td>system, lithium, lm, usage, sentence, user, rich, free, learn, content</td>\n",
       "      <td>[lithium, extract, rich, set, develop, content, lithium, high, involve, heavy, usage, jargon, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.9952</td>\n",
       "      <td>use, method, sentence, research, classification, task, present, application, learn, model</td>\n",
       "      <td>[robot, perform, simple, fetchandcarry, task, many, potential, application, eldercare, describe,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9957</td>\n",
       "      <td>model, task, text, work, base, project, key, paper, feature, method</td>\n",
       "      <td>[plan, base, framework, essay, generation, aim, understand, represent, mean, topic, argue, gener...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Num  Topic_Perc_Contrib  \\\n",
       "0          0              0.9959   \n",
       "1          1              0.9959   \n",
       "2          2              0.9937   \n",
       "3          3              0.9952   \n",
       "4          4              0.9957   \n",
       "\n",
       "                                                                                    Keywords  \\\n",
       "0                use, model, semantic, task, framework, system, word, propose, lexical, work   \n",
       "1                         model, task, use, propose, write, paper, system, neural, set, base   \n",
       "2                     system, lithium, lm, usage, sentence, user, rich, free, learn, content   \n",
       "3  use, method, sentence, research, classification, task, present, application, learn, model   \n",
       "4                        model, task, text, work, base, project, key, paper, feature, method   \n",
       "\n",
       "                                                                                   Representative Text  \n",
       "0  [novel, teaching, parallel, distribute, computing, concept, use, memebase, programming, lolcode,...  \n",
       "1  [automate, technique, model, check, violation, probability, reach, error, state, exceed, use, ve...  \n",
       "2  [lithium, extract, rich, set, develop, content, lithium, high, involve, heavy, usage, jargon, th...  \n",
       "3  [robot, perform, simple, fetchandcarry, task, many, potential, application, eldercare, describe,...  \n",
       "4  [plan, base, framework, essay, generation, aim, understand, represent, mean, topic, argue, gener...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display setting to show more characters in column\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0e6eaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Num_Documents</th>\n",
       "      <th>Perc_Documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>model, task, use, propose, write, paper, system, neural, set, base</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>model, task, use, propose, write, paper, system, neural, set, base</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>use, model, semantic, task, framework, system, word, propose, lexical, work</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>model, task, text, work, base, project, key, paper, feature, method</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>use, model, semantic, task, framework, system, word, propose, lexical, work</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>use, method, sentence, research, classification, task, present, application, learn, model</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>model, task, text, work, base, project, key, paper, feature, method</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>use, model, semantic, task, framework, system, word, propose, lexical, work</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>use, model, semantic, task, framework, system, word, propose, lexical, work</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>use, model, semantic, task, framework, system, word, propose, lexical, work</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>use, method, sentence, research, classification, task, present, application, learn, model</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>model, task, use, propose, write, paper, system, neural, set, base</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>system, lithium, lm, usage, sentence, user, rich, free, learn, content</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>system, lithium, lm, usage, sentence, user, rich, free, learn, content</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>use, method, sentence, research, classification, task, present, application, learn, model</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>system, lithium, lm, usage, sentence, user, rich, free, learn, content</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>model, task, use, propose, write, paper, system, neural, set, base</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>model, task, text, work, base, project, key, paper, feature, method</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>model, task, use, propose, write, paper, system, neural, set, base</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>use, method, sentence, research, classification, task, present, application, learn, model</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>model, task, text, work, base, project, key, paper, feature, method</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>use, method, sentence, research, classification, task, present, application, learn, model</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>model, task, text, work, base, project, key, paper, feature, method</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>use, model, semantic, task, framework, system, word, propose, lexical, work</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>model, task, text, work, base, project, key, paper, feature, method</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>model, task, text, work, base, project, key, paper, feature, method</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>model, task, text, work, base, project, key, paper, feature, method</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>use, model, semantic, task, framework, system, word, propose, lexical, work</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>model, task, use, propose, write, paper, system, neural, set, base</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>system, lithium, lm, usage, sentence, user, rich, free, learn, content</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>model, task, text, work, base, project, key, paper, feature, method</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>model, task, text, work, base, project, key, paper, feature, method</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>use, method, sentence, research, classification, task, present, application, learn, model</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>system, lithium, lm, usage, sentence, user, rich, free, learn, content</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>model, task, use, propose, write, paper, system, neural, set, base</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>model, task, use, propose, write, paper, system, neural, set, base</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>use, model, semantic, task, framework, system, word, propose, lexical, work</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>use, model, semantic, task, framework, system, word, propose, lexical, work</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>model, task, use, propose, write, paper, system, neural, set, base</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>model, task, text, work, base, project, key, paper, feature, method</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3</td>\n",
       "      <td>use, method, sentence, research, classification, task, present, application, learn, model</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3</td>\n",
       "      <td>use, method, sentence, research, classification, task, present, application, learn, model</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "      <td>model, task, text, work, base, project, key, paper, feature, method</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>use, model, semantic, task, framework, system, word, propose, lexical, work</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>model, task, use, propose, write, paper, system, neural, set, base</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4</td>\n",
       "      <td>model, task, text, work, base, project, key, paper, feature, method</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>use, model, semantic, task, framework, system, word, propose, lexical, work</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>use, model, semantic, task, framework, system, word, propose, lexical, work</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>use, model, semantic, task, framework, system, word, propose, lexical, work</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>model, task, use, propose, write, paper, system, neural, set, base</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dominant_Topic  \\\n",
       "0                1   \n",
       "1                1   \n",
       "2                0   \n",
       "3                4   \n",
       "4                0   \n",
       "5                3   \n",
       "6                4   \n",
       "7                0   \n",
       "8                0   \n",
       "9                0   \n",
       "10               3   \n",
       "11               1   \n",
       "12               2   \n",
       "13               2   \n",
       "14               3   \n",
       "15               2   \n",
       "16               1   \n",
       "17               4   \n",
       "18               1   \n",
       "19               3   \n",
       "20               4   \n",
       "21               3   \n",
       "22               4   \n",
       "23               0   \n",
       "24               4   \n",
       "25               4   \n",
       "26               4   \n",
       "27               0   \n",
       "28               1   \n",
       "29               2   \n",
       "30               4   \n",
       "31               4   \n",
       "32               3   \n",
       "33               2   \n",
       "34               1   \n",
       "35               1   \n",
       "36               0   \n",
       "37               0   \n",
       "38               1   \n",
       "39               4   \n",
       "40               3   \n",
       "41               3   \n",
       "42               4   \n",
       "43               0   \n",
       "44               1   \n",
       "45               4   \n",
       "46               0   \n",
       "47               0   \n",
       "48               0   \n",
       "49               1   \n",
       "\n",
       "                                                                               Topic_Keywords  \\\n",
       "0                          model, task, use, propose, write, paper, system, neural, set, base   \n",
       "1                          model, task, use, propose, write, paper, system, neural, set, base   \n",
       "2                 use, model, semantic, task, framework, system, word, propose, lexical, work   \n",
       "3                         model, task, text, work, base, project, key, paper, feature, method   \n",
       "4                 use, model, semantic, task, framework, system, word, propose, lexical, work   \n",
       "5   use, method, sentence, research, classification, task, present, application, learn, model   \n",
       "6                         model, task, text, work, base, project, key, paper, feature, method   \n",
       "7                 use, model, semantic, task, framework, system, word, propose, lexical, work   \n",
       "8                 use, model, semantic, task, framework, system, word, propose, lexical, work   \n",
       "9                 use, model, semantic, task, framework, system, word, propose, lexical, work   \n",
       "10  use, method, sentence, research, classification, task, present, application, learn, model   \n",
       "11                         model, task, use, propose, write, paper, system, neural, set, base   \n",
       "12                     system, lithium, lm, usage, sentence, user, rich, free, learn, content   \n",
       "13                     system, lithium, lm, usage, sentence, user, rich, free, learn, content   \n",
       "14  use, method, sentence, research, classification, task, present, application, learn, model   \n",
       "15                     system, lithium, lm, usage, sentence, user, rich, free, learn, content   \n",
       "16                         model, task, use, propose, write, paper, system, neural, set, base   \n",
       "17                        model, task, text, work, base, project, key, paper, feature, method   \n",
       "18                         model, task, use, propose, write, paper, system, neural, set, base   \n",
       "19  use, method, sentence, research, classification, task, present, application, learn, model   \n",
       "20                        model, task, text, work, base, project, key, paper, feature, method   \n",
       "21  use, method, sentence, research, classification, task, present, application, learn, model   \n",
       "22                        model, task, text, work, base, project, key, paper, feature, method   \n",
       "23                use, model, semantic, task, framework, system, word, propose, lexical, work   \n",
       "24                        model, task, text, work, base, project, key, paper, feature, method   \n",
       "25                        model, task, text, work, base, project, key, paper, feature, method   \n",
       "26                        model, task, text, work, base, project, key, paper, feature, method   \n",
       "27                use, model, semantic, task, framework, system, word, propose, lexical, work   \n",
       "28                         model, task, use, propose, write, paper, system, neural, set, base   \n",
       "29                     system, lithium, lm, usage, sentence, user, rich, free, learn, content   \n",
       "30                        model, task, text, work, base, project, key, paper, feature, method   \n",
       "31                        model, task, text, work, base, project, key, paper, feature, method   \n",
       "32  use, method, sentence, research, classification, task, present, application, learn, model   \n",
       "33                     system, lithium, lm, usage, sentence, user, rich, free, learn, content   \n",
       "34                         model, task, use, propose, write, paper, system, neural, set, base   \n",
       "35                         model, task, use, propose, write, paper, system, neural, set, base   \n",
       "36                use, model, semantic, task, framework, system, word, propose, lexical, work   \n",
       "37                use, model, semantic, task, framework, system, word, propose, lexical, work   \n",
       "38                         model, task, use, propose, write, paper, system, neural, set, base   \n",
       "39                        model, task, text, work, base, project, key, paper, feature, method   \n",
       "40  use, method, sentence, research, classification, task, present, application, learn, model   \n",
       "41  use, method, sentence, research, classification, task, present, application, learn, model   \n",
       "42                        model, task, text, work, base, project, key, paper, feature, method   \n",
       "43                use, model, semantic, task, framework, system, word, propose, lexical, work   \n",
       "44                         model, task, use, propose, write, paper, system, neural, set, base   \n",
       "45                        model, task, text, work, base, project, key, paper, feature, method   \n",
       "46                use, model, semantic, task, framework, system, word, propose, lexical, work   \n",
       "47                use, model, semantic, task, framework, system, word, propose, lexical, work   \n",
       "48                use, model, semantic, task, framework, system, word, propose, lexical, work   \n",
       "49                         model, task, use, propose, write, paper, system, neural, set, base   \n",
       "\n",
       "    Num_Documents  Perc_Documents  \n",
       "0            13.0            0.26  \n",
       "1            11.0            0.22  \n",
       "2             5.0            0.10  \n",
       "3             8.0            0.16  \n",
       "4            13.0            0.26  \n",
       "5             NaN             NaN  \n",
       "6             NaN             NaN  \n",
       "7             NaN             NaN  \n",
       "8             NaN             NaN  \n",
       "9             NaN             NaN  \n",
       "10            NaN             NaN  \n",
       "11            NaN             NaN  \n",
       "12            NaN             NaN  \n",
       "13            NaN             NaN  \n",
       "14            NaN             NaN  \n",
       "15            NaN             NaN  \n",
       "16            NaN             NaN  \n",
       "17            NaN             NaN  \n",
       "18            NaN             NaN  \n",
       "19            NaN             NaN  \n",
       "20            NaN             NaN  \n",
       "21            NaN             NaN  \n",
       "22            NaN             NaN  \n",
       "23            NaN             NaN  \n",
       "24            NaN             NaN  \n",
       "25            NaN             NaN  \n",
       "26            NaN             NaN  \n",
       "27            NaN             NaN  \n",
       "28            NaN             NaN  \n",
       "29            NaN             NaN  \n",
       "30            NaN             NaN  \n",
       "31            NaN             NaN  \n",
       "32            NaN             NaN  \n",
       "33            NaN             NaN  \n",
       "34            NaN             NaN  \n",
       "35            NaN             NaN  \n",
       "36            NaN             NaN  \n",
       "37            NaN             NaN  \n",
       "38            NaN             NaN  \n",
       "39            NaN             NaN  \n",
       "40            NaN             NaN  \n",
       "41            NaN             NaN  \n",
       "42            NaN             NaN  \n",
       "43            NaN             NaN  \n",
       "44            NaN             NaN  \n",
       "45            NaN             NaN  \n",
       "46            NaN             NaN  \n",
       "47            NaN             NaN  \n",
       "48            NaN             NaN  \n",
       "49            NaN             NaN  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8870b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46af1a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75152243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce302396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20e2173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696279a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79536c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a89ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
