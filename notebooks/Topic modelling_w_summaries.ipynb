{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87fc752e",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd90fd",
   "metadata": {},
   "source": [
    "## 0.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "36bd3d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dglover\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "217c8d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd49c49d",
   "metadata": {},
   "source": [
    "## 1.0 Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "aa4a4912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "feb04083",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PDF URL</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/pdf/1608.04434v1</th>\n",
       "      <td>Hadoop is one of the platform s that can proce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/pdf/2202.07138v2</th>\n",
       "      <td>Integrating AI Planning with Natural Language ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/pdf/1906.11608v2</th>\n",
       "      <td>The tools are machine learning based using nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/pdf/2006.16212v1</th>\n",
       "      <td>The name Tangkhul also known as Hao or Ihao re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/pdf/1511.07916v1</th>\n",
       "      <td>This is a lecture note for the course DS GA at...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             Summary\n",
       "PDF URL                                                                             \n",
       "http://arxiv.org/pdf/1608.04434v1  Hadoop is one of the platform s that can proce...\n",
       "http://arxiv.org/pdf/2202.07138v2  Integrating AI Planning with Natural Language ...\n",
       "http://arxiv.org/pdf/1906.11608v2  The tools are machine learning based using nat...\n",
       "http://arxiv.org/pdf/2006.16212v1  The name Tangkhul also known as Hao or Ihao re...\n",
       "http://arxiv.org/pdf/1511.07916v1  This is a lecture note for the course DS GA at..."
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dataset\n",
    "df = pd.read_csv('..\\\\data\\\\summaries_full.csv', index_col=0)\n",
    "#print(df.target_names.unique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "863d7d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3557, 1)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bacb412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "714afa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = list(df['Summary'])\n",
    "vectorizer = CountVectorizer(decode_error='ignore')\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "01b1a7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aa', 'aaaa', 'aaai', ..., 'zwnj', 'zynq', 'zzy'], dtype=object)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f06b0f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_df = pd.DataFrame(X.toarray())\n",
    "CV_df.columns = vectorizer.get_feature_names_out()\n",
    "#CV_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "631d3066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3556, 24780)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9c1cb703",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CV_stopwords = []\n",
    "for col in CV_df.columns:\n",
    "    if sum(CV_df[col]) > 0.25 * (CV_df.shape[0]):\n",
    "        CV_stopwords.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "59666fe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['also',\n",
       " 'an',\n",
       " 'and',\n",
       " 'are',\n",
       " 'as',\n",
       " 'at',\n",
       " 'based',\n",
       " 'be',\n",
       " 'by',\n",
       " 'can',\n",
       " 'data',\n",
       " 'for',\n",
       " 'from',\n",
       " 'has',\n",
       " 'have',\n",
       " 'in',\n",
       " 'information',\n",
       " 'is',\n",
       " 'it',\n",
       " 'language',\n",
       " 'languages',\n",
       " 'learning',\n",
       " 'model',\n",
       " 'models',\n",
       " 'natural',\n",
       " 'new',\n",
       " 'nlp',\n",
       " 'of',\n",
       " 'on',\n",
       " 'or',\n",
       " 'our',\n",
       " 'paper',\n",
       " 'processing',\n",
       " 'published',\n",
       " 'study',\n",
       " 'such',\n",
       " 'task',\n",
       " 'tasks',\n",
       " 'text',\n",
       " 'that',\n",
       " 'the',\n",
       " 'this',\n",
       " 'to',\n",
       " 'university',\n",
       " 'use',\n",
       " 'used',\n",
       " 'using',\n",
       " 'we',\n",
       " 'with']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d35830de",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stopwords = ['author','article','use', 'show', 'however', 'approach', 'well', 'provide',' present', 'include', 'word', 'nlp', 'natural', 'language', 'processing']\n",
    "full_stopwords = CV_stopwords + extra_stopwords\n",
    "stop_words.extend(full_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f1f6db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "827f680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "df2 = pd.read_csv('..\\\\data\\\\arxiv_papers_full_v2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "05f61cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>PDF URL</th>\n",
       "      <th>Author</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Journal Ref</th>\n",
       "      <th>Primary Category</th>\n",
       "      <th>Category</th>\n",
       "      <th>Entry ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Natural Language Processing using Hadoop and K...</td>\n",
       "      <td>http://arxiv.org/pdf/1608.04434v1</td>\n",
       "      <td>[arxiv.Result.Author('Emre Erturk'), arxiv.Res...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-15 23:09:21+00:00</td>\n",
       "      <td>Natural language processing, as a data analyti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1608.04434v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Integrating AI Planning with Natural Language ...</td>\n",
       "      <td>http://arxiv.org/pdf/2202.07138v2</td>\n",
       "      <td>[arxiv.Result.Author('Kebing Jin'), arxiv.Resu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-15 02:19:09+00:00</td>\n",
       "      <td>Natural language processing (NLP) aims at inve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>['cs.AI', 'cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/2202.07138v2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Natural Language Processing using Hadoop and K...   \n",
       "1  Integrating AI Planning with Natural Language ...   \n",
       "\n",
       "                             PDF URL  \\\n",
       "0  http://arxiv.org/pdf/1608.04434v1   \n",
       "1  http://arxiv.org/pdf/2202.07138v2   \n",
       "\n",
       "                                              Author  DOI  \\\n",
       "0  [arxiv.Result.Author('Emre Erturk'), arxiv.Res...  NaN   \n",
       "1  [arxiv.Result.Author('Kebing Jin'), arxiv.Resu...  NaN   \n",
       "\n",
       "              Published Date  \\\n",
       "0  2016-08-15 23:09:21+00:00   \n",
       "1  2022-02-15 02:19:09+00:00   \n",
       "\n",
       "                                             Summary Journal Ref  \\\n",
       "0  Natural language processing, as a data analyti...         NaN   \n",
       "1  Natural language processing (NLP) aims at inve...         NaN   \n",
       "\n",
       "  Primary Category            Category                           Entry ID  \n",
       "0            cs.CL           ['cs.CL']  http://arxiv.org/abs/1608.04434v1  \n",
       "1            cs.AI  ['cs.AI', 'cs.CL']  http://arxiv.org/abs/2202.07138v2  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "454d6e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.rename(columns = {'Summary' : 'Abstract'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d63e95d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2['Filename'] = df2['PDF URL'].map(lambda x: x.split('/')[-1] + '.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4f9755d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df2, how = 'left', on = 'PDF URL', indicator = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "26ab14fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "both          3568\n",
       "left_only        0\n",
       "right_only       0\n",
       "Name: _merge, dtype: int64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['_merge'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "796e6d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PDF URL                0\n",
       "Summary                0\n",
       "Title                  0\n",
       "Author                 0\n",
       "DOI                 3165\n",
       "Published Date         0\n",
       "Abstract               0\n",
       "Journal Ref         3052\n",
       "Primary Category       0\n",
       "Category               0\n",
       "Entry ID               0\n",
       "_merge                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "18bd48f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3568, 12)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d50a2586",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Summary'] = df['Summary'].map(lambda x: str(x).lower().replace('natural langauge processing', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0e2fa055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = df['Summary'].values.tolist()\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bebe6739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cf715a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "dd9a7962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ']):#,  'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "30802b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dglover\\Anaconda3\\lib\\site-packages\\spacy\\language.py:1895: UserWarning: [W123] Argument disable with value ['parser', 'ner'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_trigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "#nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b41c0fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e832d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6284bd38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.018*\"research\" + 0.018*\"domain\" + 0.013*\"technique\" + 0.012*\"deep\" + '\n",
      "  '0.010*\"technology\" + 0.010*\"understand\" + 0.010*\"researcher\" + '\n",
      "  '0.010*\"human\" + 0.009*\"develop\" + 0.009*\"document\"'),\n",
      " (1,\n",
      "  '0.015*\"plm\" + 0.014*\"embedding\" + 0.013*\"improve\" + 0.013*\"accuracy\" + '\n",
      "  '0.012*\"label\" + 0.011*\"prediction\" + 0.011*\"transfer\" + 0.010*\"arabic\" + '\n",
      "  '0.009*\"multilingual\" + 0.009*\"pair\"'),\n",
      " (2,\n",
      "  '0.020*\"large\" + 0.020*\"result\" + 0.018*\"training\" + 0.017*\"performance\" + '\n",
      "  '0.015*\"train\" + 0.014*\"dataset\" + 0.013*\"pre_traine\" + 0.012*\"state\" + '\n",
      "  '0.011*\"method\" + 0.011*\"work\"'),\n",
      " (3,\n",
      "  '0.021*\"system\" + 0.018*\"propose\" + 0.010*\"speech\" + 0.010*\"process\" + '\n",
      "  '0.010*\"method\" + 0.009*\"set\" + 0.009*\"generate\" + 0.009*\"generation\" + '\n",
      "  '0.009*\"learn\" + 0.009*\"form\"'),\n",
      " (4,\n",
      "  '0.024*\"semantic\" + 0.023*\"sentence\" + 0.016*\"network\" + 0.014*\"structure\" + '\n",
      "  '0.012*\"graph\" + 0.011*\"word\" + 0.010*\"concept\" + 0.009*\"representation\" + '\n",
      "  '0.009*\"compare\" + 0.008*\"query\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "91679646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -8.27452750945754\n",
      "\n",
      "Coherence Score:  0.3509583166428295\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "15c032eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dglover\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el801215164456636321650492014\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el801215164456636321650492014_data = {\"mdsDat\": {\"x\": [0.16838521294580755, 0.18378392378659084, 0.07237546443500345, -0.223403456873894, -0.20114114429350757], \"y\": [0.08258191021970848, -0.2323319750562389, 0.22174001696957713, 0.0779509584326251, -0.14994091056567208], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [31.073843512753207, 24.941186460822898, 20.82154900485885, 13.13324421797806, 10.030176803586992]}, \"tinfo\": {\"Term\": [\"system\", \"large\", \"semantic\", \"result\", \"domain\", \"training\", \"sentence\", \"research\", \"propose\", \"performance\", \"train\", \"pre_traine\", \"network\", \"state\", \"dataset\", \"technique\", \"deep\", \"speech\", \"structure\", \"transformer\", \"plm\", \"set\", \"embedding\", \"generate\", \"technology\", \"researcher\", \"understand\", \"graph\", \"evaluation\", \"art\", \"speech\", \"generate\", \"level\", \"question\", \"system\", \"input\", \"set\", \"predict\", \"extract\", \"context\", \"design\", \"require\", \"entity\", \"recognition\", \"find\", \"layer\", \"score\", \"bias\", \"build\", \"report\", \"textual\", \"increase\", \"possible\", \"often\", \"detection\", \"issue\", \"end\", \"answer\", \"prompt\", \"token\", \"give\", \"sequence\", \"describe\", \"extraction\", \"content\", \"form\", \"propose\", \"generation\", \"reduce\", \"novel\", \"process\", \"learn\", \"feature\", \"framework\", \"high\", \"type\", \"architecture\", \"problem\", \"method\", \"representation\", \"exist\", \"present\", \"knowledge\", \"state\", \"transformer\", \"art\", \"evaluation\", \"test\", \"discuss\", \"pre_traine\", \"pre\", \"image\", \"science\", \"conduct\", \"inference\", \"benchmark\", \"visual\", \"previous\", \"current\", \"classification\", \"classi_cation\", \"small\", \"recent\", \"survey\", \"lm\", \"finding\", \"downstream\", \"single\", \"name\", \"detail\", \"modeling\", \"impact\", \"effective\", \"training\", \"simple\", \"large\", \"evaluate\", \"result\", \"train\", \"achieve\", \"performance\", \"pretraine\", \"well\", \"attention\", \"dataset\", \"llm\", \"author\", \"work\", \"com\", \"method\", \"various\", \"knowledge\", \"present\", \"perform\", \"domain\", \"technology\", \"researcher\", \"document\", \"identify\", \"help\", \"code\", \"version\", \"ai\", \"potential\", \"dialogue\", \"write\", \"outperform\", \"medical\", \"available\", \"open\", \"clinical\", \"development\", \"gpt\", \"agent\", \"area\", \"specific\", \"key\", \"popular\", \"still\", \"translate\", \"automate\", \"academic\", \"know\", \"instruction\", \"traditional\", \"understand\", \"deep\", \"research\", \"technique\", \"topic\", \"hope\", \"future\", \"make\", \"human\", \"create\", \"develop\", \"analysis\", \"tool\", \"machine\", \"application\", \"focus\", \"complex\", \"com\", \"semantic\", \"network\", \"structure\", \"graph\", \"concept\", \"compare\", \"query\", \"mean\", \"retrieval\", \"memory\", \"parse\", \"rule\", \"algorithm\", \"property\", \"order\", \"relationship\", \"module\", \"search\", \"game\", \"space\", \"interaction\", \"tree\", \"syntactic\", \"discourse\", \"match\", \"essential\", \"rely\", \"integrate\", \"prior\", \"action\", \"sentence\", \"similarity\", \"vector\", \"capture\", \"represent\", \"word\", \"combine\", \"linguistic\", \"representation\", \"term\", \"plm\", \"embedding\", \"label\", \"prediction\", \"transfer\", \"arabic\", \"multilingual\", \"pair\", \"sentiment\", \"parameter\", \"adapt\", \"low_resource\", \"attack\", \"biomedical\", \"tag\", \"metric\", \"morphological\", \"augmentation\", \"adversarial\", \"cross\", \"social_media\", \"importance\", \"fine_tune\", \"generalize\", \"loss\", \"twitter\", \"sentiment_analysis\", \"thesis\", \"segmentation\", \"lab\", \"accuracy\", \"annotate\", \"machine_translation\", \"resource\", \"improve\", \"sample\", \"experiment\"], \"Freq\": [958.0, 778.0, 470.0, 784.0, 563.0, 659.0, 468.0, 607.0, 920.0, 699.0, 614.0, 470.0, 313.0, 429.0, 687.0, 473.0, 389.0, 485.0, 276.0, 374.0, 223.0, 435.0, 210.0, 420.0, 309.0, 303.0, 314.0, 228.0, 326.0, 326.0, 485.0304253740163, 420.2514573442139, 323.3428520323912, 306.75438944359576, 955.6406846941508, 276.57102297183343, 434.6138988684186, 255.24474513348775, 253.6364240285758, 250.26726730784353, 229.49843918148449, 212.20452412248758, 197.9785145457621, 194.10054437533284, 185.5271996529395, 178.51318358216628, 175.3034709063836, 161.86345614100594, 166.06679060137296, 144.90497586300677, 140.99026405102535, 141.06238733212155, 138.37518526911745, 136.82198742020046, 139.3452660665616, 134.30064084018414, 206.6197730192266, 130.41785669439287, 137.11758277729425, 130.53052098773054, 279.0792336594382, 267.8653263079852, 194.92908782093406, 203.13229612378967, 167.86096785372516, 396.55262362173806, 855.5637927994588, 417.7415412572657, 159.9292094754862, 321.4786346766219, 458.95577608005755, 396.76871282969165, 300.4220042205488, 390.0088552294322, 240.94054623496328, 223.91029886687002, 263.4247902275388, 330.03995525427695, 450.7460699898299, 292.77434594433515, 235.14784434846385, 259.2075966990892, 235.77227955618278, 429.0930056994438, 373.62807470082447, 325.80618259055086, 326.2270879032536, 284.20104739429223, 257.1002864022491, 469.3902363747055, 222.87471709149744, 222.20255424794078, 216.98717147920124, 203.0893431080367, 195.8154887042027, 178.63732961506977, 163.21501034019866, 161.21801458397255, 160.25716808262973, 161.51536169706708, 153.9838734592722, 152.63521604450466, 149.4819608889997, 142.57114857986375, 144.85007049873545, 139.67127350176324, 136.94460688564337, 132.8849463819535, 129.47155365390356, 125.5928120874188, 117.13450740073247, 115.43160129840491, 114.54331989872519, 652.208272291034, 163.11853410188445, 757.7261084109432, 339.2371373551463, 735.9521422229054, 571.8253032114989, 308.25904693983364, 632.0852834542053, 166.8282256649366, 219.8125495051049, 227.46837565348577, 531.4446261347067, 360.8406760503574, 283.8381005976026, 395.4422393214931, 286.331893535033, 407.56494302687935, 218.07762327195752, 281.45738307885733, 254.3229212456645, 212.44334511214674, 562.5372065249044, 308.4104407132062, 303.03859069189576, 264.47053880208216, 252.3719065045777, 211.24581426371628, 196.20324556844915, 185.92118007245332, 182.28270890135616, 175.92679358451903, 159.38613298318234, 154.90046391269073, 157.2518000689879, 142.61672359697613, 188.55506017809014, 129.18080628864652, 126.21479637085658, 124.73993518114597, 119.98421264922004, 107.32216315524026, 107.21970065254496, 106.74442565416007, 105.57409514409146, 101.5065211082444, 99.55248661500649, 97.91018634857541, 131.21353736491736, 96.73156741978097, 95.76689239528734, 93.18040671191086, 98.03916311222086, 307.3927443992893, 376.9454802480871, 568.0510156062065, 410.1037550068188, 161.54835432897085, 143.17093385013735, 138.05019543280667, 226.9486791858536, 302.5304079228281, 213.3283337121587, 266.89411293938593, 238.43444584113354, 227.04712766377327, 221.3892765690823, 216.8695569965303, 157.67497230253784, 150.3433524336705, 176.92988029474816, 469.757330649587, 312.3547784810366, 275.63348449405856, 227.499787157608, 201.63596838670597, 176.17768454844693, 159.61200954410322, 155.40440214181004, 155.08304344406093, 151.8599950796647, 142.09891851641416, 141.92240024018474, 135.50071881646232, 131.25938982689755, 124.87116540007045, 122.2961171148928, 119.98619751059334, 117.11926954511098, 115.04482625192979, 108.64306209687999, 105.83266523386919, 92.5672898600009, 90.88996022645091, 86.08704272505109, 88.54304358849713, 82.64895537515577, 73.60921148249165, 74.28258486641863, 71.38316702748546, 71.068158498768, 442.82794256660384, 89.9124538906538, 99.1008656414882, 118.73888198139899, 110.7784341670633, 215.20826859710485, 123.18389081789329, 122.55766926378764, 185.65945403617098, 121.27041883538122, 222.78135062142806, 210.16334330682676, 178.17763629016815, 165.2536748808913, 160.3261750661379, 145.48223116513057, 132.85285195742298, 132.52402865124327, 128.1795055919088, 113.01510007082933, 102.64165499843863, 98.88272172143171, 98.79467316671912, 90.24989400569257, 83.24843821760976, 76.24834817159119, 74.96947008987073, 74.40874238756965, 73.7300800707932, 75.8076524733641, 73.20669426503412, 74.85145882660606, 73.65508271448017, 71.14611698947684, 62.669151102782585, 61.94573286111544, 60.84678432828611, 58.85790851110114, 59.22789918521706, 61.271564950621816, 189.32522716661188, 105.77453490042915, 81.51752738491471, 107.39622120834423, 192.95163468688574, 88.05771075887407, 81.87211862044923], \"Total\": [958.0, 778.0, 470.0, 784.0, 563.0, 659.0, 468.0, 607.0, 920.0, 699.0, 614.0, 470.0, 313.0, 429.0, 687.0, 473.0, 389.0, 485.0, 276.0, 374.0, 223.0, 435.0, 210.0, 420.0, 309.0, 303.0, 314.0, 228.0, 326.0, 326.0, 485.7769892750995, 420.99708437825467, 324.0925509603742, 307.4958965972779, 958.2033014023657, 277.3185595040563, 435.8637762210729, 255.99134663385308, 254.38675233347772, 251.0113383944248, 230.24388546234638, 212.95090208734032, 198.7265320609963, 194.84941947297335, 186.27543905416303, 179.2714387489036, 176.05760799484742, 162.60840114449618, 166.91346416935272, 145.65043075170883, 141.73953208261048, 141.8180247558778, 139.12586836624263, 137.5677782714648, 140.11134294861196, 135.0500126466785, 207.77526443591913, 131.15754525466042, 137.8967033193371, 131.27897564454136, 282.2816539500374, 272.681418064572, 197.16791316690117, 206.09822000619528, 169.61056226439146, 412.83221588165884, 920.9111342324481, 451.06908450560366, 162.2342011361104, 344.0243524347169, 508.1028691599622, 436.2735051970638, 321.5032462261942, 430.80487752839923, 252.527452443, 239.9900193624686, 324.13084114996377, 483.36540948983463, 913.4430472742649, 539.4926592495275, 349.479846387746, 525.4165575977228, 517.7928271273674, 429.84886894810785, 374.3834642130738, 326.56063340215167, 326.98269188571413, 284.9600403451937, 257.8587297214172, 470.8637164450327, 223.62965278931406, 222.95895299517736, 217.74831033672447, 203.84692923576702, 196.5783781290654, 179.3899824662282, 163.9798225912358, 161.97733727885804, 161.01196902139674, 162.27956258932372, 154.74550959276297, 153.39251264071154, 150.23854184195443, 143.32013987985505, 145.6134885611835, 140.42520881066724, 137.70889560363545, 133.6440626202354, 130.23459091299472, 126.34789365999934, 117.89437202391987, 116.18948523861982, 115.30180798070074, 659.2196561704989, 164.5129195575864, 778.6865313257866, 350.3598244976472, 784.8304162495557, 614.9631805729603, 323.68610223610824, 699.1743729267321, 170.45151388267797, 232.31738697032915, 245.06178884477148, 687.4559878772594, 516.0155833983757, 393.86449442264615, 681.0087106652298, 463.82032601773625, 913.4430472742649, 290.5841297715086, 517.7928271273674, 525.4165575977228, 310.43138141275705, 563.2811455311938, 309.15289043572795, 303.78439557864795, 265.20918237018174, 253.14384398529023, 211.98441593653237, 196.93912418646386, 186.66464611445858, 183.0213939275565, 176.68015356357628, 160.12762243583282, 155.64113477569728, 158.0040446992056, 143.3640384166004, 189.57493552398864, 129.92196708888454, 126.95392688885248, 125.47635260936852, 120.73757283979683, 108.0597762691416, 107.9641738409382, 107.49244051089326, 106.31532402025378, 102.25668449222766, 100.29611981522591, 98.65469759337854, 132.22051659798007, 97.47795581776624, 96.52406696549696, 93.93740010226679, 98.85097513169117, 314.31579093325445, 389.18278566528176, 607.6998742915262, 473.80683882515814, 176.63291477538314, 158.28218069978016, 152.44548372913226, 314.48192257841504, 485.04376744797355, 292.526642431422, 427.25760241922774, 401.25853465445647, 388.2888915091212, 371.2861486060322, 375.070791514704, 207.045964039232, 197.30925032271037, 463.82032601773625, 470.51114141760894, 313.1142780465773, 276.38648923690334, 228.25574878292772, 202.39212245447402, 176.9512965448767, 160.3629872861626, 156.1653716168806, 155.85261757299813, 152.6377176783679, 142.84936364661152, 142.67604086938607, 136.25809258362082, 132.02474341192658, 125.62934414615262, 123.06071428609157, 120.74436036651218, 117.87265631106978, 115.82176947756703, 109.3961554390353, 106.59143378342439, 93.31350111791929, 91.64450467107424, 86.83632367131838, 89.31703647540508, 83.43454428462712, 74.36896393976293, 75.05800745443281, 72.14001626688665, 71.82545396819883, 468.5661022949713, 92.1107479307144, 105.75584927690349, 133.22835786173957, 131.7727127985931, 382.65493004678194, 173.41550828488872, 185.37906511986827, 539.4926592495275, 210.5020668220628, 223.56887292013806, 210.93144668319553, 178.94291813455402, 166.0268357300653, 161.09139455331618, 146.24247717359046, 133.61327312296845, 133.29374151259697, 128.94999298086805, 113.78456993737127, 103.4096073208964, 99.64123724883913, 99.55574370566427, 91.02523432853822, 84.00872003847707, 77.01319205216441, 75.7296013432787, 75.17566265642066, 74.49107038982582, 76.59218559908086, 73.96722916489466, 75.63419807644182, 74.42781381360768, 71.92939610230718, 63.44297012209163, 62.714667277593456, 61.60567249021205, 59.61288201446845, 59.99662229843384, 62.069039846139184, 192.838234262735, 110.57560187214358, 84.14496380748902, 125.80300696090775, 618.177964442863, 138.98406209142667, 169.13727340750904], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.5599, -4.7033, -4.9654, -5.0181, -3.8818, -5.1217, -4.6697, -5.2019, -5.2083, -5.2216, -5.3083, -5.3866, -5.456, -5.4758, -5.521, -5.5595, -5.5776, -5.6574, -5.6318, -5.7681, -5.7955, -5.795, -5.8142, -5.8255, -5.8072, -5.8441, -5.4133, -5.8734, -5.8233, -5.8726, -5.1127, -5.1537, -5.4715, -5.4303, -5.621, -4.7614, -3.9924, -4.7093, -5.6694, -4.9712, -4.6152, -4.7608, -5.039, -4.778, -5.2596, -5.3329, -5.1704, -4.9449, -4.6333, -5.0648, -5.2839, -5.1865, -5.2813, -4.4626, -4.6011, -4.738, -4.7367, -4.8746, -4.9748, -4.3729, -5.1177, -5.1207, -5.1445, -5.2107, -5.2471, -5.339, -5.4292, -5.4416, -5.4475, -5.4397, -5.4875, -5.4963, -5.5171, -5.5645, -5.5486, -5.585, -5.6047, -5.6348, -5.6609, -5.6913, -5.761, -5.7756, -5.7834, -4.0439, -5.4298, -3.894, -4.6976, -3.9231, -4.1755, -4.7934, -4.0753, -5.4073, -5.1315, -5.0973, -4.2487, -4.6359, -4.8759, -4.5443, -4.8672, -4.5141, -5.1395, -4.8843, -4.9857, -5.1656, -4.0113, -4.6123, -4.6299, -4.7661, -4.8129, -4.9908, -5.0646, -5.1185, -5.1382, -5.1737, -5.2725, -5.301, -5.2859, -5.3836, -5.1044, -5.4826, -5.5058, -5.5175, -5.5564, -5.6679, -5.6689, -5.6733, -5.6844, -5.7237, -5.7431, -5.7597, -5.467, -5.7718, -5.7819, -5.8092, -5.7584, -4.6157, -4.4117, -4.0016, -4.3274, -5.259, -5.3797, -5.4162, -4.9191, -4.6316, -4.9809, -4.7569, -4.8697, -4.9186, -4.9439, -4.9645, -5.2832, -5.3309, -5.168, -3.7307, -4.1388, -4.2639, -4.4558, -4.5765, -4.7114, -4.8102, -4.8369, -4.839, -4.86, -4.9264, -4.9277, -4.974, -5.0058, -5.0557, -5.0765, -5.0956, -5.1197, -5.1376, -5.1949, -5.2211, -5.355, -5.3733, -5.4276, -5.3995, -5.4683, -5.5842, -5.5751, -5.6149, -5.6193, -3.7898, -5.3841, -5.2868, -5.106, -5.1754, -4.5113, -5.0693, -5.0744, -4.659, -5.0849, -4.2072, -4.2655, -4.4306, -4.5059, -4.5362, -4.6333, -4.7241, -4.7266, -4.76, -4.8859, -4.9821, -5.0195, -5.0203, -5.1108, -5.1916, -5.2794, -5.2963, -5.3038, -5.313, -5.2852, -5.3201, -5.2979, -5.314, -5.3487, -5.4755, -5.4871, -5.505, -5.5383, -5.532, -5.4981, -4.3699, -4.9521, -5.2126, -4.9369, -4.351, -5.1354, -5.2082], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.1673, 1.167, 1.1665, 1.1664, 1.1661, 1.1661, 1.1659, 1.1659, 1.1658, 1.1658, 1.1656, 1.1653, 1.165, 1.165, 1.1648, 1.1646, 1.1645, 1.1642, 1.1637, 1.1637, 1.1635, 1.1635, 1.1634, 1.1634, 1.1633, 1.1632, 1.1632, 1.1631, 1.1631, 1.1631, 1.1574, 1.151, 1.1574, 1.1543, 1.1584, 1.1286, 1.0952, 1.092, 1.1545, 1.101, 1.0671, 1.0739, 1.101, 1.0693, 1.1218, 1.0995, 0.9614, 0.7872, 0.4625, 0.5576, 0.7726, 0.4622, 0.3821, 1.3869, 1.3866, 1.3863, 1.3863, 1.386, 1.3857, 1.3855, 1.3853, 1.3853, 1.3851, 1.3849, 1.3848, 1.3844, 1.384, 1.384, 1.384, 1.3839, 1.3837, 1.3837, 1.3836, 1.3834, 1.3834, 1.3833, 1.3831, 1.383, 1.3828, 1.3827, 1.3822, 1.3821, 1.382, 1.378, 1.3801, 1.3614, 1.3564, 1.3243, 1.3159, 1.3398, 1.2878, 1.3672, 1.3333, 1.3142, 1.1313, 1.0309, 1.061, 0.8451, 0.9063, 0.5816, 1.1016, 0.7791, 0.6631, 1.0094, 1.5679, 1.5668, 1.5667, 1.5664, 1.5661, 1.5657, 1.5654, 1.5652, 1.5651, 1.5649, 1.5645, 1.5644, 1.5644, 1.564, 1.5638, 1.5635, 1.5633, 1.5633, 1.5629, 1.5623, 1.5623, 1.5622, 1.5622, 1.5618, 1.5617, 1.5616, 1.5615, 1.5615, 1.5613, 1.5611, 1.5609, 1.5469, 1.5372, 1.5017, 1.4248, 1.4799, 1.4688, 1.47, 1.243, 1.0971, 1.2535, 1.0986, 1.0487, 1.0326, 1.0521, 1.0214, 1.2968, 1.2973, 0.6054, 2.0284, 2.0276, 2.0273, 2.0267, 2.0263, 2.0256, 2.0253, 2.0251, 2.0251, 2.0249, 2.0248, 2.0247, 2.0244, 2.0242, 2.024, 2.0238, 2.0237, 2.0236, 2.0233, 2.0231, 2.0229, 2.022, 2.0218, 2.0214, 2.0213, 2.0206, 2.0198, 2.0196, 2.0195, 2.0194, 1.9735, 2.0059, 1.965, 1.9149, 1.8565, 1.4545, 1.688, 1.6162, 0.9633, 1.4786, 2.296, 2.2959, 2.2953, 2.2949, 2.2948, 2.2944, 2.2939, 2.2938, 2.2936, 2.2928, 2.2921, 2.2919, 2.2919, 2.291, 2.2905, 2.2896, 2.2895, 2.2893, 2.2893, 2.2893, 2.2892, 2.2892, 2.2891, 2.2886, 2.2873, 2.2872, 2.2872, 2.2868, 2.2867, 2.2866, 2.2812, 2.2552, 2.2678, 2.1414, 1.1352, 1.8432, 1.574]}, \"token.table\": {\"Topic\": [3, 3, 5, 2, 5, 4, 5, 5, 3, 3, 4, 1, 2, 3, 4, 1, 5, 1, 1, 2, 3, 5, 1, 2, 3, 2, 5, 1, 2, 4, 5, 2, 3, 3, 3, 2, 1, 5, 1, 2, 4, 2, 2, 3, 3, 2, 3, 2, 4, 4, 3, 4, 4, 2, 1, 3, 1, 1, 3, 5, 2, 1, 2, 1, 3, 1, 4, 1, 2, 1, 1, 3, 3, 3, 4, 2, 3, 3, 2, 2, 5, 1, 2, 1, 4, 1, 2, 2, 1, 2, 3, 1, 5, 1, 1, 2, 1, 4, 1, 2, 5, 1, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 4, 5, 1, 1, 2, 1, 4, 3, 4, 3, 1, 5, 2, 3, 1, 3, 3, 2, 2, 5, 1, 2, 3, 5, 1, 2, 1, 3, 4, 4, 1, 3, 3, 1, 2, 5, 5, 1, 2, 3, 1, 1, 2, 4, 5, 1, 1, 2, 4, 1, 2, 2, 5, 5, 1, 3, 1, 5, 1, 3, 4, 4, 4, 3, 4, 1, 2, 3, 4, 5, 5, 2, 4, 5, 5, 2, 4, 1, 2, 3, 4, 1, 3, 4, 3, 5, 5, 4, 1, 2, 1, 2, 5, 5, 3, 1, 3, 2, 2, 5, 1, 5, 1, 2, 3, 2, 5, 2, 4, 1, 2, 3, 1, 3, 4, 1, 4, 1, 2, 4, 4, 1, 2, 1, 1, 3, 4, 4, 1, 1, 3, 4, 1, 2, 4, 1, 2, 3, 3, 1, 2, 5, 1, 2, 4, 4, 4, 2, 5, 2, 1, 4, 5, 4, 1, 4, 5, 5, 5, 1, 2, 1, 5, 4, 5, 2, 3, 2, 2, 5, 4, 3, 1, 2, 3, 4, 2, 4, 1, 3, 5, 1, 3, 3, 1, 3, 4, 2, 1, 5, 1, 1, 3, 2, 3, 3, 1, 2, 5, 2, 5, 5, 2, 3, 4, 5, 1, 2, 2, 3, 1, 2, 3, 4, 5, 3, 2, 1, 2, 1, 3, 4, 1, 2, 3, 3], \"Freq\": [0.9950967804591658, 0.015557080842757616, 0.9800960930937298, 0.9515391543605223, 0.04634119258249297, 0.9885075008566698, 0.9960389819523701, 0.993407660981968, 0.9901926849589061, 0.9944192648430993, 0.9981058550084837, 0.007476476488116192, 0.28161394771904324, 0.5931338013905513, 0.117131464980487, 0.03617434526492673, 0.9586201495205583, 0.9911743906733472, 0.3972583399476968, 0.023995470198183027, 0.5785574481117464, 0.9915039925635585, 0.81140072653043, 0.18511043190808288, 0.9910695019779552, 0.9982832180464898, 0.9944177635063697, 0.03672543174693316, 0.9262970007282029, 0.03264482821949614, 0.9843611267945339, 0.7210601717636591, 0.2767449250783058, 0.9907690831242849, 0.99696723872067, 0.9978260633014912, 0.9962584888590378, 0.9887368119829516, 0.9945273188481314, 0.10508273332115062, 0.8932032332297803, 0.9951823507207098, 0.9982772778970868, 0.9924860387368117, 0.9952313985839873, 0.6166180823844781, 0.38161328874843575, 0.28832484761316446, 0.7092791251283846, 0.9946239639751074, 0.76022791508592, 0.2331365606263488, 0.9980625606880414, 0.9958452686094306, 0.9905043515988061, 0.005895859235707179, 0.9959709453728515, 0.27006087152735236, 0.7281388055104564, 0.9922683287537891, 0.9937149453699168, 0.22546897944494185, 0.7724130844210588, 0.03083384065789757, 0.968696494002282, 0.9890047364600038, 0.010143638322666706, 0.9945975309622291, 0.9972465416721903, 0.9920681443398943, 0.374481341219078, 0.6249157381593364, 0.9962036463487945, 0.9929579767770256, 0.9903689650141808, 0.996669766727134, 0.9954406466647375, 0.9995008788534389, 0.9948522163326626, 0.9973824523137463, 0.9955841260379041, 0.9962687356552097, 0.004812892442778791, 0.9963440610900748, 0.9947917941141414, 0.031396293840973394, 0.9675766920081801, 0.9969946669652545, 0.6724279022924509, 0.07725767388040924, 0.248941393614652, 0.514375088632223, 0.4848133019292217, 0.998479667946817, 0.9849672646076121, 0.009704110981355785, 0.933116550210303, 0.06531815851472121, 0.9985213345593944, 0.9969719908963031, 0.9942519631883979, 0.23666242530917078, 0.7631155754867139, 0.9616497567956342, 0.012111457894151564, 0.01695604105181219, 0.009689166315321253, 0.9052822294805395, 0.025533601344322907, 0.06267338511788349, 0.006963709457542611, 0.09183610860440733, 0.9052416419577294, 0.9929048789249745, 0.9870790503928981, 0.9976316121530219, 0.9266873176603332, 0.07315952507844735, 0.9883745404488162, 0.010627683230632432, 0.9938911076109217, 0.9944985009594569, 0.9953561872358244, 0.954351685998963, 0.04355962052277425, 0.08844962798784238, 0.9034497715901043, 0.37522387094587617, 0.6246858950362664, 0.9954814465669697, 0.995698970674669, 0.9897625397326018, 0.9916149295877924, 0.24426622863545414, 0.2669134286413903, 0.1763246286176457, 0.3122078286532626, 0.9942318703332251, 0.9970577734206064, 0.9988512867489793, 0.9900210129166203, 0.9859041361433006, 0.9944513948032063, 0.9922250088978105, 0.9970340680126817, 0.9945706083262708, 0.4557807440270863, 0.5426880892864884, 0.9827766008820309, 0.994730620555517, 0.011557924322482704, 0.9734340707157656, 0.015410565763310274, 0.9984858784489158, 0.9099796234948441, 0.05730350273897003, 0.025213541205146812, 0.006876420328676403, 0.9966288920953701, 0.08091528560844141, 0.25353456157311643, 0.6635053419892195, 0.30037852535227894, 0.6995912751753077, 0.9957868699716942, 0.9930178218132101, 0.9935645394763842, 0.4013077260205102, 0.5952282379230386, 0.023768505083390352, 0.9745087084190045, 0.27346563928028705, 0.7218220943793623, 0.0031798330148870586, 0.9964504366925298, 0.9925375798436313, 0.9974607410573735, 0.9958220177288574, 0.4937363104856886, 0.44666167334403756, 0.029558493088943663, 0.0021895180065884196, 0.029558493088943663, 0.9868439156309982, 0.9924137852505935, 0.9938352369895147, 0.9903657046869763, 0.9954100883195636, 0.9905202534569367, 0.9964413055401724, 0.9330734807818988, 0.043601564522518635, 0.002906770968167909, 0.017440625809007456, 0.9958727379434421, 0.9929036858851299, 0.9949904685849473, 0.9936454493863306, 0.9977962842871418, 0.993104777406962, 0.9940541306945355, 0.31246840947122695, 0.6829206475041248, 0.015732842086236924, 0.903923290772885, 0.0800944688026607, 0.9974554913986561, 0.9974898023195033, 0.9919075555145588, 0.9961503680529034, 0.9971843949071133, 0.9960419196044589, 0.0021237567582184625, 0.9961274213097874, 0.993815242424214, 0.4929422117646689, 0.48342595285029305, 0.022839021394502033, 0.9797507584177066, 0.017600313025467784, 0.993966209747136, 0.9841971720290569, 0.6827133127881383, 0.25446587113012425, 0.06206484661710348, 0.9033603781038608, 0.0295215809837863, 0.06691558356324895, 0.9934972824023173, 0.9922382472751391, 0.9295142258361881, 0.05537993635238971, 0.015202335469283451, 0.9977364646773831, 0.9983873066185095, 0.991756164385186, 0.9956406363679663, 0.9862285441635332, 0.012327856802044165, 0.9913805612762362, 0.9950387376639832, 0.9955343025876963, 0.10624354392246735, 0.04553294739534315, 0.8423595268138483, 0.5431028485310324, 0.11306919372147774, 0.3447683611835223, 0.9955346416567405, 0.06417641610584059, 0.934671906361986, 0.9974179201102353, 0.007948935595082729, 0.1351319051164064, 0.850536108673852, 0.05351474551751456, 0.9377822071640647, 0.008919124252919094, 0.9945293342757058, 0.9952617071144765, 0.3597534799861364, 0.6331661247756001, 0.9965634161038159, 0.9939928299214518, 0.9925966179232713, 0.9833886932254875, 0.9989136465162782, 0.02561004720833555, 0.9454375761077207, 0.02774421780903018, 0.9926328574441334, 0.9901685597165053, 0.9828319138949784, 0.014669133043208632, 0.9980182426982994, 0.0022942948108006884, 0.9770846727647668, 0.02171299272810593, 0.9908036428892333, 0.006078549956375664, 0.9951807614374493, 0.9974411225557605, 0.9869235447127751, 0.9963787078490517, 0.9954188358869445, 0.9984005226837547, 0.9980251920862672, 0.9970475446530589, 0.9986016348412311, 0.9977662603446842, 0.9929673396851512, 0.9977005908880285, 0.002087239729891273, 0.987992674593601, 0.13296557761009428, 0.8653315368275978, 0.9962708081619323, 0.20902407593552588, 0.21377462311587875, 0.5748162088226962, 0.9966309650151974, 0.9947824571469627, 0.9897189668783385, 0.9978749404223207, 0.4146397270708889, 0.5846162611496384, 0.08492188457103189, 0.9171563533671444, 0.9913913329579451, 0.04878340841812521, 0.9301369871722539, 0.02113947698118759, 0.9890481782469308, 0.00910167035196562, 0.9932249977949321, 0.9989757447918277, 0.9933637463866446, 0.9966403455645382, 0.9886044635391251, 0.933372148537902, 0.06666943918127871, 0.01908908229581794, 0.9767247108026845, 0.041296130003506425, 0.7502130283970334, 0.20648065001753213, 0.9361184338918743, 0.05673445053890147, 0.9964393572736262, 0.9940247368502265, 0.05165347353675514, 0.9469803481738442, 0.29530522443859547, 0.1411193107936651, 0.5618639226044073, 0.30396086974834163, 0.5800219495197824, 0.11453597990517221, 0.9958806855487063], \"Term\": [\"academic\", \"accuracy\", \"accuracy\", \"achieve\", \"achieve\", \"action\", \"adapt\", \"adversarial\", \"agent\", \"ai\", \"algorithm\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"annotate\", \"annotate\", \"answer\", \"application\", \"application\", \"application\", \"arabic\", \"architecture\", \"architecture\", \"area\", \"art\", \"attack\", \"attention\", \"attention\", \"attention\", \"augmentation\", \"author\", \"author\", \"automate\", \"available\", \"benchmark\", \"bias\", \"biomedical\", \"build\", \"capture\", \"capture\", \"classi_cation\", \"classification\", \"clinical\", \"code\", \"com\", \"com\", \"combine\", \"combine\", \"compare\", \"complex\", \"complex\", \"concept\", \"conduct\", \"content\", \"content\", \"context\", \"create\", \"create\", \"cross\", \"current\", \"dataset\", \"dataset\", \"deep\", \"deep\", \"describe\", \"describe\", \"design\", \"detail\", \"detection\", \"develop\", \"develop\", \"development\", \"dialogue\", \"discourse\", \"discuss\", \"document\", \"domain\", \"downstream\", \"effective\", \"embedding\", \"end\", \"end\", \"entity\", \"essential\", \"evaluate\", \"evaluate\", \"evaluation\", \"exist\", \"exist\", \"exist\", \"experiment\", \"experiment\", \"extract\", \"extraction\", \"extraction\", \"feature\", \"feature\", \"find\", \"finding\", \"fine_tune\", \"focus\", \"focus\", \"form\", \"form\", \"form\", \"form\", \"framework\", \"framework\", \"framework\", \"framework\", \"future\", \"future\", \"game\", \"generalize\", \"generate\", \"generation\", \"generation\", \"give\", \"give\", \"gpt\", \"graph\", \"help\", \"high\", \"high\", \"hope\", \"hope\", \"human\", \"human\", \"identify\", \"image\", \"impact\", \"importance\", \"improve\", \"improve\", \"improve\", \"improve\", \"increase\", \"inference\", \"input\", \"instruction\", \"integrate\", \"interaction\", \"issue\", \"key\", \"know\", \"knowledge\", \"knowledge\", \"lab\", \"label\", \"large\", \"large\", \"large\", \"layer\", \"learn\", \"learn\", \"learn\", \"learn\", \"level\", \"linguistic\", \"linguistic\", \"linguistic\", \"llm\", \"llm\", \"lm\", \"loss\", \"low_resource\", \"machine\", \"machine\", \"machine_translation\", \"machine_translation\", \"make\", \"make\", \"make\", \"match\", \"mean\", \"medical\", \"memory\", \"method\", \"method\", \"method\", \"method\", \"method\", \"metric\", \"modeling\", \"module\", \"morphological\", \"multilingual\", \"name\", \"network\", \"novel\", \"novel\", \"novel\", \"novel\", \"often\", \"open\", \"order\", \"outperform\", \"pair\", \"parameter\", \"parse\", \"perform\", \"perform\", \"performance\", \"performance\", \"performance\", \"plm\", \"popular\", \"possible\", \"potential\", \"pre\", \"pre_traine\", \"pre_traine\", \"predict\", \"prediction\", \"present\", \"present\", \"present\", \"pretraine\", \"pretraine\", \"previous\", \"prior\", \"problem\", \"problem\", \"problem\", \"process\", \"process\", \"process\", \"prompt\", \"property\", \"propose\", \"propose\", \"propose\", \"query\", \"question\", \"recent\", \"recognition\", \"reduce\", \"reduce\", \"relationship\", \"rely\", \"report\", \"represent\", \"represent\", \"represent\", \"representation\", \"representation\", \"representation\", \"require\", \"research\", \"research\", \"researcher\", \"resource\", \"resource\", \"resource\", \"result\", \"result\", \"result\", \"retrieval\", \"rule\", \"sample\", \"sample\", \"science\", \"score\", \"search\", \"segmentation\", \"semantic\", \"sentence\", \"sentence\", \"sentence\", \"sentiment\", \"sentiment_analysis\", \"sequence\", \"sequence\", \"set\", \"set\", \"similarity\", \"similarity\", \"simple\", \"simple\", \"single\", \"small\", \"social_media\", \"space\", \"specific\", \"speech\", \"state\", \"still\", \"structure\", \"survey\", \"syntactic\", \"system\", \"system\", \"tag\", \"technique\", \"technique\", \"technology\", \"term\", \"term\", \"term\", \"test\", \"textual\", \"thesis\", \"token\", \"tool\", \"tool\", \"topic\", \"topic\", \"traditional\", \"train\", \"train\", \"train\", \"training\", \"training\", \"transfer\", \"transformer\", \"translate\", \"tree\", \"twitter\", \"type\", \"type\", \"understand\", \"understand\", \"various\", \"various\", \"various\", \"vector\", \"vector\", \"version\", \"visual\", \"well\", \"well\", \"word\", \"word\", \"word\", \"work\", \"work\", \"work\", \"write\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [4, 3, 1, 5, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el801215164456636321650492014\", ldavis_el801215164456636321650492014_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el801215164456636321650492014\", ldavis_el801215164456636321650492014_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el801215164456636321650492014\", ldavis_el801215164456636321650492014_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "3      0.168385  0.082582       1        1  31.073844\n",
       "2      0.183784 -0.232332       2        1  24.941186\n",
       "0      0.072375  0.221740       3        1  20.821549\n",
       "4     -0.223403  0.077951       4        1  13.133244\n",
       "1     -0.201141 -0.149941       5        1  10.030177, topic_info=                    Term        Freq       Total Category  logprob  loglift\n",
       "246               system  958.000000  958.000000  Default  30.0000  30.0000\n",
       "18                 large  778.000000  778.000000  Default  29.0000  29.0000\n",
       "520             semantic  470.000000  470.000000  Default  28.0000  28.0000\n",
       "29                result  784.000000  784.000000  Default  27.0000  27.0000\n",
       "45                domain  563.000000  563.000000  Default  26.0000  26.0000\n",
       "..                   ...         ...         ...      ...      ...      ...\n",
       "153  machine_translation   81.517527   84.144964   Topic5  -5.2126   2.2678\n",
       "107             resource  107.396221  125.803007   Topic5  -4.9369   2.1414\n",
       "17               improve  192.951635  618.177964   Topic5  -4.3510   1.1352\n",
       "815               sample   88.057711  138.984062   Topic5  -5.1354   1.8432\n",
       "325           experiment   81.872119  169.137273   Topic5  -5.2082   1.5740\n",
       "\n",
       "[260 rows x 6 columns], token_table=      Topic      Freq      Term\n",
       "term                           \n",
       "2818      3  0.995097  academic\n",
       "228       3  0.015557  accuracy\n",
       "228       5  0.980096  accuracy\n",
       "785       2  0.951539   achieve\n",
       "785       5  0.046341   achieve\n",
       "...     ...       ...       ...\n",
       "584       4  0.561864      word\n",
       "77        1  0.303961      work\n",
       "77        2  0.580022      work\n",
       "77        3  0.114536      work\n",
       "374       3  0.995881     write\n",
       "\n",
       "[339 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[4, 3, 1, 5, 2])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "84f0b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1e031b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dglover\\AppData\\Local\\Temp\\ipykernel_8012\\1533496723.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "199dffc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.6313</td>\n",
       "      <td>system, propose, speech, process, method, set,...</td>\n",
       "      <td>[hadoop, platform, process, large, amount, req...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.3173</td>\n",
       "      <td>system, propose, speech, process, method, set,...</td>\n",
       "      <td>[integrating, ai, plan, combination, explicit,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.5041</td>\n",
       "      <td>research, domain, technique, deep, technology,...</td>\n",
       "      <td>[tool, machine, rocesse, train, previously, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.5705</td>\n",
       "      <td>large, result, training, performance, train, d...</td>\n",
       "      <td>[name, know, refer, ethnic, group, manipur, im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.2966</td>\n",
       "      <td>semantic, sentence, network, structure, graph,...</td>\n",
       "      <td>[note, course, introduce, reader, understand, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3563</th>\n",
       "      <td>3</td>\n",
       "      <td>0.3075</td>\n",
       "      <td>system, propose, speech, process, method, set,...</td>\n",
       "      <td>[automate, method, security, directive, presen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3564</th>\n",
       "      <td>3</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>system, propose, speech, process, method, set,...</td>\n",
       "      <td>[modifie, run, driven, augmentation, fully, la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3565</th>\n",
       "      <td>2</td>\n",
       "      <td>0.4774</td>\n",
       "      <td>large, result, training, performance, train, d...</td>\n",
       "      <td>[large, llm, revolutionise, understanding, int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3566</th>\n",
       "      <td>0</td>\n",
       "      <td>0.3765</td>\n",
       "      <td>research, domain, technique, deep, technology,...</td>\n",
       "      <td>[predictive, power, vary, clinical, note, type...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3567</th>\n",
       "      <td>2</td>\n",
       "      <td>0.5932</td>\n",
       "      <td>large, result, training, performance, train, d...</td>\n",
       "      <td>[dataset, strong, baseline, classification, cz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3568 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dominant_Topic  Perc_Contribution  \\\n",
       "0                  3             0.6313   \n",
       "1                  3             0.3173   \n",
       "2                  0             0.5041   \n",
       "3                  2             0.5705   \n",
       "4                  4             0.2966   \n",
       "...              ...                ...   \n",
       "3563               3             0.3075   \n",
       "3564               3             0.4256   \n",
       "3565               2             0.4774   \n",
       "3566               0             0.3765   \n",
       "3567               2             0.5932   \n",
       "\n",
       "                                         Topic_Keywords  \\\n",
       "0     system, propose, speech, process, method, set,...   \n",
       "1     system, propose, speech, process, method, set,...   \n",
       "2     research, domain, technique, deep, technology,...   \n",
       "3     large, result, training, performance, train, d...   \n",
       "4     semantic, sentence, network, structure, graph,...   \n",
       "...                                                 ...   \n",
       "3563  system, propose, speech, process, method, set,...   \n",
       "3564  system, propose, speech, process, method, set,...   \n",
       "3565  large, result, training, performance, train, d...   \n",
       "3566  research, domain, technique, deep, technology,...   \n",
       "3567  large, result, training, performance, train, d...   \n",
       "\n",
       "                                                      0  \n",
       "0     [hadoop, platform, process, large, amount, req...  \n",
       "1     [integrating, ai, plan, combination, explicit,...  \n",
       "2     [tool, machine, rocesse, train, previously, an...  \n",
       "3     [name, know, refer, ethnic, group, manipur, im...  \n",
       "4     [note, course, introduce, reader, understand, ...  \n",
       "...                                                 ...  \n",
       "3563  [automate, method, security, directive, presen...  \n",
       "3564  [modifie, run, driven, augmentation, fully, la...  \n",
       "3565  [large, llm, revolutionise, understanding, int...  \n",
       "3566  [predictive, power, vary, clinical, note, type...  \n",
       "3567  [dataset, strong, baseline, classification, cz...  \n",
       "\n",
       "[3568 rows x 4 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_sents_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3725cf6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PDF URL</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Journal Ref</th>\n",
       "      <th>Primary Category</th>\n",
       "      <th>Category</th>\n",
       "      <th>Entry ID</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/pdf/1608.04434v1</td>\n",
       "      <td>hadoop is one of the platform s that can proce...</td>\n",
       "      <td>Natural Language Processing using Hadoop and K...</td>\n",
       "      <td>[arxiv.Result.Author('Emre Erturk'), arxiv.Res...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-15 23:09:21+00:00</td>\n",
       "      <td>Natural language processing, as a data analyti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1608.04434v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/pdf/2202.07138v2</td>\n",
       "      <td>integrating ai planning with natural language ...</td>\n",
       "      <td>Integrating AI Planning with Natural Language ...</td>\n",
       "      <td>[arxiv.Result.Author('Kebing Jin'), arxiv.Resu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-15 02:19:09+00:00</td>\n",
       "      <td>Natural language processing (NLP) aims at inve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>['cs.AI', 'cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/2202.07138v2</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/pdf/1906.11608v2</td>\n",
       "      <td>the tools are machine learning based using nat...</td>\n",
       "      <td>Simple Natural Language Processing Tools for D...</td>\n",
       "      <td>[arxiv.Result.Author('Leon Derczynski')]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-06-27 13:15:12+00:00</td>\n",
       "      <td>This technical note describes a set of baselin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1906.11608v2</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/pdf/2006.16212v1</td>\n",
       "      <td>the name tangkhul also known as hao or ihao re...</td>\n",
       "      <td>Towards the Study of Morphological Processing ...</td>\n",
       "      <td>[arxiv.Result.Author('Mirinso Shadang'), arxiv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-06-29 17:24:09+00:00</td>\n",
       "      <td>There is no or little work on natural language...</td>\n",
       "      <td>In proceeding of Regional International Confer...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/2006.16212v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/pdf/1511.07916v1</td>\n",
       "      <td>this is a lecture note for the course ds ga at...</td>\n",
       "      <td>Natural Language Understanding with Distribute...</td>\n",
       "      <td>[arxiv.Result.Author('Kyunghyun Cho')]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-11-24 23:23:13+00:00</td>\n",
       "      <td>This is a lecture note for the course DS-GA 30...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL', 'stat.ML']</td>\n",
       "      <td>http://arxiv.org/abs/1511.07916v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3563</th>\n",
       "      <td>http://arxiv.org/pdf/2307.01211v1</td>\n",
       "      <td>an automated method for the ontologicalreprese...</td>\n",
       "      <td>An automated method for the ontological repres...</td>\n",
       "      <td>[arxiv.Result.Author('Giampaolo Bella'), arxiv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-06-30 09:04:47+00:00</td>\n",
       "      <td>Large documents written in juridical language ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>['cs.AI', 'cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/2307.01211v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3564</th>\n",
       "      <td>http://arxiv.org/pdf/2307.01488v1</td>\n",
       "      <td>scat modifies ran driven augmentations of the ...</td>\n",
       "      <td>SCAT: Robust Self-supervised Contrastive Learn...</td>\n",
       "      <td>[arxiv.Result.Author('Junjie Wu'), arxiv.Resul...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-04 05:41:31+00:00</td>\n",
       "      <td>Despite their promising performance across var...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/2307.01488v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3565</th>\n",
       "      <td>http://arxiv.org/pdf/2307.06090v1</td>\n",
       "      <td>large language models llms have revolutionised...</td>\n",
       "      <td>Can Large Language Models Aid in Annotating Sp...</td>\n",
       "      <td>[arxiv.Result.Author('Siddique Latif'), arxiv....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-12 11:27:40+00:00</td>\n",
       "      <td>Despite recent advancements in speech emotion ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>['cs.SD', 'eess.AS']</td>\n",
       "      <td>http://arxiv.org/abs/2307.06090v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3566</th>\n",
       "      <td>http://arxiv.org/pdf/2307.07051v1</td>\n",
       "      <td>predictive power varies with clinical note typ...</td>\n",
       "      <td>Making the Most Out of the Limited Context Len...</td>\n",
       "      <td>[arxiv.Result.Author('Hongyi Zheng'), arxiv.Re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-13 20:04:05+00:00</td>\n",
       "      <td>Recent advances in large language models have ...</td>\n",
       "      <td>Association for Computational Linguistics - St...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL', 'cs.IR', 'cs.LG']</td>\n",
       "      <td>http://arxiv.org/abs/2307.07051v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3567</th>\n",
       "      <td>http://arxiv.org/pdf/2307.10666v1</td>\n",
       "      <td>a dataset and strong baselines for classificat...</td>\n",
       "      <td>A Dataset and Strong Baselines for Classificat...</td>\n",
       "      <td>[arxiv.Result.Author('Hynek Kydlek'), arxiv....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-20 07:47:08+00:00</td>\n",
       "      <td>Pre-trained models for Czech Natural Language ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/2307.10666v1</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3568 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                PDF URL  \\\n",
       "0     http://arxiv.org/pdf/1608.04434v1   \n",
       "1     http://arxiv.org/pdf/2202.07138v2   \n",
       "2     http://arxiv.org/pdf/1906.11608v2   \n",
       "3     http://arxiv.org/pdf/2006.16212v1   \n",
       "4     http://arxiv.org/pdf/1511.07916v1   \n",
       "...                                 ...   \n",
       "3563  http://arxiv.org/pdf/2307.01211v1   \n",
       "3564  http://arxiv.org/pdf/2307.01488v1   \n",
       "3565  http://arxiv.org/pdf/2307.06090v1   \n",
       "3566  http://arxiv.org/pdf/2307.07051v1   \n",
       "3567  http://arxiv.org/pdf/2307.10666v1   \n",
       "\n",
       "                                                Summary  \\\n",
       "0     hadoop is one of the platform s that can proce...   \n",
       "1     integrating ai planning with natural language ...   \n",
       "2     the tools are machine learning based using nat...   \n",
       "3     the name tangkhul also known as hao or ihao re...   \n",
       "4     this is a lecture note for the course ds ga at...   \n",
       "...                                                 ...   \n",
       "3563  an automated method for the ontologicalreprese...   \n",
       "3564  scat modifies ran driven augmentations of the ...   \n",
       "3565  large language models llms have revolutionised...   \n",
       "3566  predictive power varies with clinical note typ...   \n",
       "3567  a dataset and strong baselines for classificat...   \n",
       "\n",
       "                                                  Title  \\\n",
       "0     Natural Language Processing using Hadoop and K...   \n",
       "1     Integrating AI Planning with Natural Language ...   \n",
       "2     Simple Natural Language Processing Tools for D...   \n",
       "3     Towards the Study of Morphological Processing ...   \n",
       "4     Natural Language Understanding with Distribute...   \n",
       "...                                                 ...   \n",
       "3563  An automated method for the ontological repres...   \n",
       "3564  SCAT: Robust Self-supervised Contrastive Learn...   \n",
       "3565  Can Large Language Models Aid in Annotating Sp...   \n",
       "3566  Making the Most Out of the Limited Context Len...   \n",
       "3567  A Dataset and Strong Baselines for Classificat...   \n",
       "\n",
       "                                                 Author  DOI  \\\n",
       "0     [arxiv.Result.Author('Emre Erturk'), arxiv.Res...  NaN   \n",
       "1     [arxiv.Result.Author('Kebing Jin'), arxiv.Resu...  NaN   \n",
       "2              [arxiv.Result.Author('Leon Derczynski')]  NaN   \n",
       "3     [arxiv.Result.Author('Mirinso Shadang'), arxiv...  NaN   \n",
       "4                [arxiv.Result.Author('Kyunghyun Cho')]  NaN   \n",
       "...                                                 ...  ...   \n",
       "3563  [arxiv.Result.Author('Giampaolo Bella'), arxiv...  NaN   \n",
       "3564  [arxiv.Result.Author('Junjie Wu'), arxiv.Resul...  NaN   \n",
       "3565  [arxiv.Result.Author('Siddique Latif'), arxiv....  NaN   \n",
       "3566  [arxiv.Result.Author('Hongyi Zheng'), arxiv.Re...  NaN   \n",
       "3567  [arxiv.Result.Author('Hynek Kydlek'), arxiv....  NaN   \n",
       "\n",
       "                 Published Date  \\\n",
       "0     2016-08-15 23:09:21+00:00   \n",
       "1     2022-02-15 02:19:09+00:00   \n",
       "2     2019-06-27 13:15:12+00:00   \n",
       "3     2020-06-29 17:24:09+00:00   \n",
       "4     2015-11-24 23:23:13+00:00   \n",
       "...                         ...   \n",
       "3563  2023-06-30 09:04:47+00:00   \n",
       "3564  2023-07-04 05:41:31+00:00   \n",
       "3565  2023-07-12 11:27:40+00:00   \n",
       "3566  2023-07-13 20:04:05+00:00   \n",
       "3567  2023-07-20 07:47:08+00:00   \n",
       "\n",
       "                                               Abstract  \\\n",
       "0     Natural language processing, as a data analyti...   \n",
       "1     Natural language processing (NLP) aims at inve...   \n",
       "2     This technical note describes a set of baselin...   \n",
       "3     There is no or little work on natural language...   \n",
       "4     This is a lecture note for the course DS-GA 30...   \n",
       "...                                                 ...   \n",
       "3563  Large documents written in juridical language ...   \n",
       "3564  Despite their promising performance across var...   \n",
       "3565  Despite recent advancements in speech emotion ...   \n",
       "3566  Recent advances in large language models have ...   \n",
       "3567  Pre-trained models for Czech Natural Language ...   \n",
       "\n",
       "                                            Journal Ref Primary Category  \\\n",
       "0                                                   NaN            cs.CL   \n",
       "1                                                   NaN            cs.AI   \n",
       "2                                                   NaN            cs.CL   \n",
       "3     In proceeding of Regional International Confer...            cs.CL   \n",
       "4                                                   NaN            cs.CL   \n",
       "...                                                 ...              ...   \n",
       "3563                                                NaN            cs.AI   \n",
       "3564                                                NaN            cs.CL   \n",
       "3565                                                NaN            cs.SD   \n",
       "3566  Association for Computational Linguistics - St...            cs.CL   \n",
       "3567                                                NaN            cs.CL   \n",
       "\n",
       "                         Category                           Entry ID _merge  \n",
       "0                       ['cs.CL']  http://arxiv.org/abs/1608.04434v1   both  \n",
       "1              ['cs.AI', 'cs.CL']  http://arxiv.org/abs/2202.07138v2   both  \n",
       "2                       ['cs.CL']  http://arxiv.org/abs/1906.11608v2   both  \n",
       "3                       ['cs.CL']  http://arxiv.org/abs/2006.16212v1   both  \n",
       "4            ['cs.CL', 'stat.ML']  http://arxiv.org/abs/1511.07916v1   both  \n",
       "...                           ...                                ...    ...  \n",
       "3563           ['cs.AI', 'cs.CL']  http://arxiv.org/abs/2307.01211v1   both  \n",
       "3564                    ['cs.CL']  http://arxiv.org/abs/2307.01488v1   both  \n",
       "3565         ['cs.SD', 'eess.AS']  http://arxiv.org/abs/2307.06090v1   both  \n",
       "3566  ['cs.CL', 'cs.IR', 'cs.LG']  http://arxiv.org/abs/2307.07051v1   both  \n",
       "3567                    ['cs.CL']  http://arxiv.org/abs/2307.10666v1   both  \n",
       "\n",
       "[3568 rows x 12 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "940cc19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'original_df' is your original dataframe and 'lda_output' is the output from LDA\n",
    "merged_df = df.merge(df_topic_sents_keywords, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f7018c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PDF URL</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Journal Ref</th>\n",
       "      <th>Primary Category</th>\n",
       "      <th>Category</th>\n",
       "      <th>Entry ID</th>\n",
       "      <th>_merge</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/pdf/1608.04434v1</td>\n",
       "      <td>hadoop is one of the platform s that can proce...</td>\n",
       "      <td>Natural Language Processing using Hadoop and K...</td>\n",
       "      <td>[arxiv.Result.Author('Emre Erturk'), arxiv.Res...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-15 23:09:21+00:00</td>\n",
       "      <td>Natural language processing, as a data analyti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1608.04434v1</td>\n",
       "      <td>both</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6313</td>\n",
       "      <td>system, propose, speech, process, method, set,...</td>\n",
       "      <td>[hadoop, platform, process, large, amount, req...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/pdf/2202.07138v2</td>\n",
       "      <td>integrating ai planning with natural language ...</td>\n",
       "      <td>Integrating AI Planning with Natural Language ...</td>\n",
       "      <td>[arxiv.Result.Author('Kebing Jin'), arxiv.Resu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-15 02:19:09+00:00</td>\n",
       "      <td>Natural language processing (NLP) aims at inve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>['cs.AI', 'cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/2202.07138v2</td>\n",
       "      <td>both</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3173</td>\n",
       "      <td>system, propose, speech, process, method, set,...</td>\n",
       "      <td>[integrating, ai, plan, combination, explicit,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/pdf/1906.11608v2</td>\n",
       "      <td>the tools are machine learning based using nat...</td>\n",
       "      <td>Simple Natural Language Processing Tools for D...</td>\n",
       "      <td>[arxiv.Result.Author('Leon Derczynski')]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-06-27 13:15:12+00:00</td>\n",
       "      <td>This technical note describes a set of baselin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/1906.11608v2</td>\n",
       "      <td>both</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5041</td>\n",
       "      <td>research, domain, technique, deep, technology,...</td>\n",
       "      <td>[tool, machine, rocesse, train, previously, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/pdf/2006.16212v1</td>\n",
       "      <td>the name tangkhul also known as hao or ihao re...</td>\n",
       "      <td>Towards the Study of Morphological Processing ...</td>\n",
       "      <td>[arxiv.Result.Author('Mirinso Shadang'), arxiv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-06-29 17:24:09+00:00</td>\n",
       "      <td>There is no or little work on natural language...</td>\n",
       "      <td>In proceeding of Regional International Confer...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>http://arxiv.org/abs/2006.16212v1</td>\n",
       "      <td>both</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5705</td>\n",
       "      <td>large, result, training, performance, train, d...</td>\n",
       "      <td>[name, know, refer, ethnic, group, manipur, im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/pdf/1511.07916v1</td>\n",
       "      <td>this is a lecture note for the course ds ga at...</td>\n",
       "      <td>Natural Language Understanding with Distribute...</td>\n",
       "      <td>[arxiv.Result.Author('Kyunghyun Cho')]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-11-24 23:23:13+00:00</td>\n",
       "      <td>This is a lecture note for the course DS-GA 30...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>['cs.CL', 'stat.ML']</td>\n",
       "      <td>http://arxiv.org/abs/1511.07916v1</td>\n",
       "      <td>both</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2966</td>\n",
       "      <td>semantic, sentence, network, structure, graph,...</td>\n",
       "      <td>[note, course, introduce, reader, understand, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             PDF URL  \\\n",
       "0  http://arxiv.org/pdf/1608.04434v1   \n",
       "1  http://arxiv.org/pdf/2202.07138v2   \n",
       "2  http://arxiv.org/pdf/1906.11608v2   \n",
       "3  http://arxiv.org/pdf/2006.16212v1   \n",
       "4  http://arxiv.org/pdf/1511.07916v1   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  hadoop is one of the platform s that can proce...   \n",
       "1  integrating ai planning with natural language ...   \n",
       "2  the tools are machine learning based using nat...   \n",
       "3  the name tangkhul also known as hao or ihao re...   \n",
       "4  this is a lecture note for the course ds ga at...   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Natural Language Processing using Hadoop and K...   \n",
       "1  Integrating AI Planning with Natural Language ...   \n",
       "2  Simple Natural Language Processing Tools for D...   \n",
       "3  Towards the Study of Morphological Processing ...   \n",
       "4  Natural Language Understanding with Distribute...   \n",
       "\n",
       "                                              Author  DOI  \\\n",
       "0  [arxiv.Result.Author('Emre Erturk'), arxiv.Res...  NaN   \n",
       "1  [arxiv.Result.Author('Kebing Jin'), arxiv.Resu...  NaN   \n",
       "2           [arxiv.Result.Author('Leon Derczynski')]  NaN   \n",
       "3  [arxiv.Result.Author('Mirinso Shadang'), arxiv...  NaN   \n",
       "4             [arxiv.Result.Author('Kyunghyun Cho')]  NaN   \n",
       "\n",
       "              Published Date  \\\n",
       "0  2016-08-15 23:09:21+00:00   \n",
       "1  2022-02-15 02:19:09+00:00   \n",
       "2  2019-06-27 13:15:12+00:00   \n",
       "3  2020-06-29 17:24:09+00:00   \n",
       "4  2015-11-24 23:23:13+00:00   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Natural language processing, as a data analyti...   \n",
       "1  Natural language processing (NLP) aims at inve...   \n",
       "2  This technical note describes a set of baselin...   \n",
       "3  There is no or little work on natural language...   \n",
       "4  This is a lecture note for the course DS-GA 30...   \n",
       "\n",
       "                                         Journal Ref Primary Category  \\\n",
       "0                                                NaN            cs.CL   \n",
       "1                                                NaN            cs.AI   \n",
       "2                                                NaN            cs.CL   \n",
       "3  In proceeding of Regional International Confer...            cs.CL   \n",
       "4                                                NaN            cs.CL   \n",
       "\n",
       "               Category                           Entry ID _merge  \\\n",
       "0             ['cs.CL']  http://arxiv.org/abs/1608.04434v1   both   \n",
       "1    ['cs.AI', 'cs.CL']  http://arxiv.org/abs/2202.07138v2   both   \n",
       "2             ['cs.CL']  http://arxiv.org/abs/1906.11608v2   both   \n",
       "3             ['cs.CL']  http://arxiv.org/abs/2006.16212v1   both   \n",
       "4  ['cs.CL', 'stat.ML']  http://arxiv.org/abs/1511.07916v1   both   \n",
       "\n",
       "   Dominant_Topic  Perc_Contribution  \\\n",
       "0               3             0.6313   \n",
       "1               3             0.3173   \n",
       "2               0             0.5041   \n",
       "3               2             0.5705   \n",
       "4               4             0.2966   \n",
       "\n",
       "                                      Topic_Keywords  \\\n",
       "0  system, propose, speech, process, method, set,...   \n",
       "1  system, propose, speech, process, method, set,...   \n",
       "2  research, domain, technique, deep, technology,...   \n",
       "3  large, result, training, performance, train, d...   \n",
       "4  semantic, sentence, network, structure, graph,...   \n",
       "\n",
       "                                                   0  \n",
       "0  [hadoop, platform, process, large, amount, req...  \n",
       "1  [integrating, ai, plan, combination, explicit,...  \n",
       "2  [tool, machine, rocesse, train, previously, an...  \n",
       "3  [name, know, refer, ethnic, group, manipur, im...  \n",
       "4  [note, course, introduce, reader, understand, ...  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "53de37ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('../data/Dominant_topics_of_summaries_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b35a269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaa810f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06bf94bd",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6e682120",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [168], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    break\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f82fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=texts)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcdeb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.to_csv('../data/Dominant_topics_of_absracts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea71fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4634e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic['Dominant_Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db13218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0c711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display setting to show more characters in column\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8870b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46af1a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75152243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce302396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20e2173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696279a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79536c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a89ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
