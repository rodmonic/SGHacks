{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.0.0-py3-none-any.whl (11 kB)\n",
      "Collecting requests==2.31.0\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting feedparser==6.0.10\n",
      "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
      "     ---------------------------------------- 81.1/81.1 kB 1.5 MB/s eta 0:00:00\n",
      "Collecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dglover\\anaconda3\\lib\\site-packages (from requests==2.31.0->arxiv) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dglover\\anaconda3\\lib\\site-packages (from requests==2.31.0->arxiv) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dglover\\anaconda3\\lib\\site-packages (from requests==2.31.0->arxiv) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dglover\\anaconda3\\lib\\site-packages (from requests==2.31.0->arxiv) (2.1.1)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6048 sha256=84bdd9227a27b7b18e627889b40988e20c9d549fb3595d63560d089e76d6f665\n",
      "  Stored in directory: c:\\users\\dglover\\appdata\\local\\pip\\cache\\wheels\\83\\63\\2f\\117884c3b19d46b64d3d61690333aa80c88dc14050e269c546\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, requests, feedparser, arxiv\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.1\n",
      "    Uninstalling requests-2.28.1:\n",
      "      Successfully uninstalled requests-2.28.1\n",
      "Successfully installed arxiv-2.0.0 feedparser-6.0.10 requests-2.31.0 sgmllib3k-1.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.24 requires PyYAML==6.0, but you have pyyaml 5.4.1 which is incompatible.\n",
      "conda-repo-cli 1.0.24 requires requests==2.28.1, but you have requests 2.31.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "!pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder = '..\\\\data\\\\PDFs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialogueLLM: Context and Emotion Knowledge-Tuned LLaMA Models for Emotion Recognition in Conversations\n",
      "VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights\n",
      "Utilizing Weak Supervision To Generate Indonesian Conservation Dataset\n",
      "Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations\n",
      "ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing\n",
      "{'Title': 'DialogueLLM: Context and Emotion Knowledge-Tuned LLaMA Models for Emotion Recognition in Conversations', 'PDF URL': 'http://arxiv.org/pdf/2310.11374v1', 'Author': [arxiv.Result.Author('Yazhou Zhang'), arxiv.Result.Author('Mengyao Wang'), arxiv.Result.Author('Prayag Tiwari'), arxiv.Result.Author('Qiuchi Li'), arxiv.Result.Author('Benyou Wang'), arxiv.Result.Author('Jing Qin')], 'DOI': None, 'Published Date': datetime.datetime(2023, 10, 17, 16, 15, 34, tzinfo=datetime.timezone.utc), 'Summary': 'Large language models (LLMs) and their variants have shown extraordinary\\nefficacy across numerous downstream natural language processing (NLP) tasks,\\nwhich has presented a new vision for the development of NLP. Despite their\\nremarkable performance in natural language generating (NLG), LLMs lack a\\ndistinct focus on the emotion understanding domain. As a result, using LLMs for\\nemotion recognition may lead to suboptimal and inadequate precision. Another\\nlimitation of LLMs is that they are typical trained without leveraging\\nmulti-modal information. To overcome these limitations, we propose DialogueLLM,\\na context and emotion knowledge tuned LLM that is obtained by fine-tuning LLaMA\\nmodels with 13,638 multi-modal (i.e., texts and videos) emotional dialogues.\\nThe visual information is considered as the supplementary knowledge to\\nconstruct high-quality instructions. We offer a comprehensive evaluation of our\\nproposed model on three benchmarking emotion recognition in conversations (ERC)\\ndatasets and compare the results against the SOTA baselines and other SOTA\\nLLMs. Additionally, DialogueLLM-7B can be easily trained using LoRA on a 40GB\\nA100 GPU in 5 hours, facilitating reproducibility for other researchers.', 'Journal Ref': None, 'Primary Category': 'cs.CL', 'Category': ['cs.CL'], 'Entry ID': 'http://arxiv.org/abs/2310.11374v1'}\n",
      "{'Title': 'VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights', 'PDF URL': 'http://arxiv.org/pdf/2310.11368v1', 'Author': [arxiv.Result.Author('Shanshan Xu'), arxiv.Result.Author('Leon Staufer'), arxiv.Result.Author('Santosh T. Y. S. S'), arxiv.Result.Author('Oana Ichim'), arxiv.Result.Author('Corina Heri'), arxiv.Result.Author('Matthias Grabmair')], 'DOI': None, 'Published Date': datetime.datetime(2023, 10, 17, 16, 5, 52, tzinfo=datetime.timezone.utc), 'Summary': 'Recognizing vulnerability is crucial for understanding and implementing\\ntargeted support to empower individuals in need. This is especially important\\nat the European Court of Human Rights (ECtHR), where the court adapts\\nConvention standards to meet actual individual needs and thus ensures effective\\nhuman rights protection. However, the concept of vulnerability remains elusive\\nat the ECtHR and no prior NLP research has dealt with it. To enable future\\nresearch in this area, we present VECHR, a novel expert-annotated multi-label\\ndataset comprising of vulnerability type classification and explanation\\nrationale. We benchmark the performance of state-of-the-art models on VECHR\\nfrom both prediction and explainability perspectives. Our results demonstrate\\nthe challenging nature of the task with lower prediction performance and\\nlimited agreement between models and experts. Further, we analyze the\\nrobustness of these models in dealing with out-of-domain (OOD) data and observe\\noverall limited performance. Our dataset poses unique challenges offering\\nsignificant room for improvement regarding performance, explainability, and\\nrobustness.', 'Journal Ref': None, 'Primary Category': 'cs.CL', 'Category': ['cs.CL'], 'Entry ID': 'http://arxiv.org/abs/2310.11368v1'}\n",
      "{'Title': 'Utilizing Weak Supervision To Generate Indonesian Conservation Dataset', 'PDF URL': 'http://arxiv.org/pdf/2310.11258v1', 'Author': [arxiv.Result.Author('Mega Fransiska'), arxiv.Result.Author('Diah Pitaloka'), arxiv.Result.Author('Saripudin'), arxiv.Result.Author('Satrio Putra'), arxiv.Result.Author('Lintang Sutawika')], 'DOI': None, 'Published Date': datetime.datetime(2023, 10, 17, 13, 23, 18, tzinfo=datetime.timezone.utc), 'Summary': 'Weak supervision has emerged as a promising approach for rapid and\\nlarge-scale dataset creation in response to the increasing demand for\\naccelerated NLP development. By leveraging labeling functions, weak supervision\\nallows practitioners to generate datasets quickly by creating learned label\\nmodels that produce soft-labeled datasets. This paper aims to show how such an\\napproach can be utilized to build an Indonesian NLP dataset from conservation\\nnews text. We construct two types of datasets: multi-class classification and\\nsentiment classification. We then provide baseline experiments using various\\npretrained language models. These baseline results demonstrate test\\nperformances of 59.79% accuracy and 55.72% F1-score for sentiment\\nclassification, 66.87% F1-score-macro, 71.5% F1-score-micro, and 83.67% ROC-AUC\\nfor multi-class classification. Additionally, we release the datasets and\\nlabeling functions used in this work for further research and exploration.', 'Journal Ref': None, 'Primary Category': 'cs.CL', 'Category': ['cs.CL'], 'Entry ID': 'http://arxiv.org/abs/2310.11258v1'}\n",
      "{'Title': 'Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations', 'PDF URL': 'http://arxiv.org/pdf/2310.11207v1', 'Author': [arxiv.Result.Author('Shiyuan Huang'), arxiv.Result.Author('Siddarth Mamidanna'), arxiv.Result.Author('Shreedhar Jangam'), arxiv.Result.Author('Yilun Zhou'), arxiv.Result.Author('Leilani H. Gilpin')], 'DOI': None, 'Published Date': datetime.datetime(2023, 10, 17, 12, 34, 32, tzinfo=datetime.timezone.utc), 'Summary': 'Large language models (LLMs) such as ChatGPT have demonstrated superior\\nperformance on a variety of natural language processing (NLP) tasks including\\nsentiment analysis, mathematical reasoning and summarization. Furthermore,\\nsince these models are instruction-tuned on human conversations to produce\\n\"helpful\" responses, they can and often will produce explanations along with\\nthe response, which we call self-explanations. For example, when analyzing the\\nsentiment of a movie review, the model may output not only the positivity of\\nthe sentiment, but also an explanation (e.g., by listing the sentiment-laden\\nwords such as \"fantastic\" and \"memorable\" in the review). How good are these\\nautomatically generated self-explanations? In this paper, we investigate this\\nquestion on the task of sentiment analysis and for feature attribution\\nexplanation, one of the most commonly studied settings in the interpretability\\nliterature (for pre-ChatGPT models). Specifically, we study different ways to\\nelicit the self-explanations, evaluate their faithfulness on a set of\\nevaluation metrics, and compare them to traditional explanation methods such as\\nocclusion or LIME saliency maps. Through an extensive set of experiments, we\\nfind that ChatGPT\\'s self-explanations perform on par with traditional ones, but\\nare quite different from them according to various agreement metrics, meanwhile\\nbeing much cheaper to produce (as they are generated along with the\\nprediction). In addition, we identified several interesting characteristics of\\nthem, which prompt us to rethink many current model interpretability practices\\nin the era of ChatGPT(-like) LLMs.', 'Journal Ref': None, 'Primary Category': 'cs.CL', 'Category': ['cs.CL', 'cs.LG'], 'Entry ID': 'http://arxiv.org/abs/2310.11207v1'}\n",
      "{'Title': 'ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing', 'PDF URL': 'http://arxiv.org/pdf/2310.11166v1', 'Author': [arxiv.Result.Author('Quoc-Nam Nguyen'), arxiv.Result.Author('Thang Chau Phan'), arxiv.Result.Author('Duc-Vu Nguyen'), arxiv.Result.Author('Kiet Van Nguyen')], 'DOI': None, 'Published Date': datetime.datetime(2023, 10, 17, 11, 34, 50, tzinfo=datetime.timezone.utc), 'Summary': 'English and Chinese, known as resource-rich languages, have witnessed the\\nstrong development of transformer-based language models for natural language\\nprocessing tasks. Although Vietnam has approximately 100M people speaking\\nVietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA,\\nperformed well on general Vietnamese NLP tasks, including POS tagging and named\\nentity recognition. These pre-trained language models are still limited to\\nVietnamese social media tasks. In this paper, we present the first monolingual\\npre-trained language model for Vietnamese social media texts, ViSoBERT, which\\nis pre-trained on a large-scale corpus of high-quality and diverse Vietnamese\\nsocial media texts using XLM-R architecture. Moreover, we explored our\\npre-trained model on five important natural language downstream tasks on\\nVietnamese social media texts: emotion recognition, hate speech detection,\\nsentiment analysis, spam reviews detection, and hate speech spans detection.\\nOur experiments demonstrate that ViSoBERT, with far fewer parameters, surpasses\\nthe previous state-of-the-art models on multiple Vietnamese social media tasks.\\nOur ViSoBERT model is\\navailable\\\\footnote{\\\\url{https://huggingface.co/uitnlp/visobert}} only for\\nresearch purposes.', 'Journal Ref': None, 'Primary Category': 'cs.CL', 'Category': ['cs.CL'], 'Entry ID': 'http://arxiv.org/abs/2310.11166v1'}\n"
     ]
    }
   ],
   "source": [
    "search = arxiv.Search(\n",
    "  query=\"NLP\",\n",
    "  max_results=5,\n",
    "  sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    ")\n",
    "\n",
    "# Initialize an empty list to store dictionaries\n",
    "paper_list = []\n",
    "\n",
    "for result in arxiv.Client().results(search):\n",
    "    print(result.title)\n",
    "    # Create a dictionary for each paper\n",
    "    paper_info = {\n",
    "        'Title': result.title,\n",
    "        'PDF URL': result.pdf_url,\n",
    "        'Author' : result.authors,\n",
    "        'DOI' : result.doi,\n",
    "        'Published Date': result.published,\n",
    "        'Summary' : result.summary,\n",
    "        'Journal Ref' : result.journal_ref,\n",
    "        'Primary Category' : result.primary_category,\n",
    "        'Category' : result.categories,\n",
    "        'Entry ID' : result.entry_id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    }\n",
    "    result.download_source(dirpath=pdf_folder, filename = (result.title.replace('.','').replace('?','') + '.pdf'))\n",
    "    paper_list.append(paper_info)\n",
    "\n",
    "# Print the list of dictionaries\n",
    "#for paper in paper_list:\n",
    "#    print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(paper_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>PDF URL</th>\n",
       "      <th>Author</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Journal Ref</th>\n",
       "      <th>Primary Category</th>\n",
       "      <th>Category</th>\n",
       "      <th>Entry ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DialogueLLM: Context and Emotion Knowledge-Tun...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.11374v1</td>\n",
       "      <td>[Yazhou Zhang, Mengyao Wang, Prayag Tiwari, Qi...</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-10-17 16:15:34+00:00</td>\n",
       "      <td>Large language models (LLMs) and their variant...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>http://arxiv.org/abs/2310.11374v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VECHR: A Dataset for Explainable and Robust Cl...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.11368v1</td>\n",
       "      <td>[Shanshan Xu, Leon Staufer, Santosh T. Y. S. S...</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-10-17 16:05:52+00:00</td>\n",
       "      <td>Recognizing vulnerability is crucial for under...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>http://arxiv.org/abs/2310.11368v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Utilizing Weak Supervision To Generate Indones...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.11258v1</td>\n",
       "      <td>[Mega Fransiska, Diah Pitaloka, Saripudin, Sat...</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-10-17 13:23:18+00:00</td>\n",
       "      <td>Weak supervision has emerged as a promising ap...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>http://arxiv.org/abs/2310.11258v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can Large Language Models Explain Themselves? ...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.11207v1</td>\n",
       "      <td>[Shiyuan Huang, Siddarth Mamidanna, Shreedhar ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-10-17 12:34:32+00:00</td>\n",
       "      <td>Large language models (LLMs) such as ChatGPT h...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>[cs.CL, cs.LG]</td>\n",
       "      <td>http://arxiv.org/abs/2310.11207v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ViSoBERT: A Pre-Trained Language Model for Vie...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.11166v1</td>\n",
       "      <td>[Quoc-Nam Nguyen, Thang Chau Phan, Duc-Vu Nguy...</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-10-17 11:34:50+00:00</td>\n",
       "      <td>English and Chinese, known as resource-rich la...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>http://arxiv.org/abs/2310.11166v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  DialogueLLM: Context and Emotion Knowledge-Tun...   \n",
       "1  VECHR: A Dataset for Explainable and Robust Cl...   \n",
       "2  Utilizing Weak Supervision To Generate Indones...   \n",
       "3  Can Large Language Models Explain Themselves? ...   \n",
       "4  ViSoBERT: A Pre-Trained Language Model for Vie...   \n",
       "\n",
       "                             PDF URL  \\\n",
       "0  http://arxiv.org/pdf/2310.11374v1   \n",
       "1  http://arxiv.org/pdf/2310.11368v1   \n",
       "2  http://arxiv.org/pdf/2310.11258v1   \n",
       "3  http://arxiv.org/pdf/2310.11207v1   \n",
       "4  http://arxiv.org/pdf/2310.11166v1   \n",
       "\n",
       "                                              Author   DOI  \\\n",
       "0  [Yazhou Zhang, Mengyao Wang, Prayag Tiwari, Qi...  None   \n",
       "1  [Shanshan Xu, Leon Staufer, Santosh T. Y. S. S...  None   \n",
       "2  [Mega Fransiska, Diah Pitaloka, Saripudin, Sat...  None   \n",
       "3  [Shiyuan Huang, Siddarth Mamidanna, Shreedhar ...  None   \n",
       "4  [Quoc-Nam Nguyen, Thang Chau Phan, Duc-Vu Nguy...  None   \n",
       "\n",
       "             Published Date  \\\n",
       "0 2023-10-17 16:15:34+00:00   \n",
       "1 2023-10-17 16:05:52+00:00   \n",
       "2 2023-10-17 13:23:18+00:00   \n",
       "3 2023-10-17 12:34:32+00:00   \n",
       "4 2023-10-17 11:34:50+00:00   \n",
       "\n",
       "                                             Summary Journal Ref  \\\n",
       "0  Large language models (LLMs) and their variant...        None   \n",
       "1  Recognizing vulnerability is crucial for under...        None   \n",
       "2  Weak supervision has emerged as a promising ap...        None   \n",
       "3  Large language models (LLMs) such as ChatGPT h...        None   \n",
       "4  English and Chinese, known as resource-rich la...        None   \n",
       "\n",
       "  Primary Category        Category                           Entry ID  \n",
       "0            cs.CL         [cs.CL]  http://arxiv.org/abs/2310.11374v1  \n",
       "1            cs.CL         [cs.CL]  http://arxiv.org/abs/2310.11368v1  \n",
       "2            cs.CL         [cs.CL]  http://arxiv.org/abs/2310.11258v1  \n",
       "3            cs.CL  [cs.CL, cs.LG]  http://arxiv.org/abs/2310.11207v1  \n",
       "4            cs.CL         [cs.CL]  http://arxiv.org/abs/2310.11166v1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('..\\\\data\\\\arxiv_papers_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\hdawson\\\\OneDrive - TP Group Plc\\\\Documents\\\\TP group\\\\_Internal Projects\\\\Hackathon\\\\SGHacks\\\\data\\\\PDFs'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = arxiv.Search(\n",
    "  query=\"NLP\",\n",
    "  max_results=5,\n",
    "  sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    ")\n",
    "\n",
    "for result in arxiv.Client().results(search):\n",
    "    title =  result.title\n",
    "    print(title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
