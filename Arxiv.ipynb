{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Obtaining dependency information for arxiv from https://files.pythonhosted.org/packages/e3/fb/ebc79ba50811878e25af80cd08dcdaae6c72edb8be17eecaaf2695e5cce7/arxiv-2.0.0-py3-none-any.whl.metadata\n",
      "  Downloading arxiv-2.0.0-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting feedparser==6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
      "     ---------------------------------------- 0.0/81.1 kB ? eta -:--:--\n",
      "     -------------- ----------------------- 30.7/81.1 kB 660.6 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 81.1/81.1 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting requests==2.31.0 (from arxiv)\n",
      "  Obtaining dependency information for requests==2.31.0 from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting sgmllib3k (from feedparser==6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hdawson\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests==2.31.0->arxiv) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hdawson\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests==2.31.0->arxiv) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hdawson\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests==2.31.0->arxiv) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hdawson\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests==2.31.0->arxiv) (2022.9.24)\n",
      "Downloading arxiv-2.0.0-py3-none-any.whl (11 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6060 sha256=c83badc9d310f56d072bce6261ddabf905bf5ec02022bb5a0b09e77cd83f312b\n",
      "  Stored in directory: c:\\users\\hdawson\\appdata\\local\\pip\\cache\\wheels\\3b\\25\\2a\\105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, requests, feedparser, arxiv\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.1\n",
      "    Uninstalling requests-2.28.1:\n",
      "      Successfully uninstalled requests-2.28.1\n",
      "Successfully installed arxiv-2.0.0 feedparser-6.0.10 requests-2.31.0 sgmllib3k-1.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "!pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "import arxiv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': 'DialogueLLM: Context and Emotion Knowledge-Tuned LLaMA Models for Emotion Recognition in Conversations', 'PDF URL': 'http://arxiv.org/pdf/2310.11374v1', 'Author': [arxiv.Result.Author('Yazhou Zhang'), arxiv.Result.Author('Mengyao Wang'), arxiv.Result.Author('Prayag Tiwari'), arxiv.Result.Author('Qiuchi Li'), arxiv.Result.Author('Benyou Wang'), arxiv.Result.Author('Jing Qin')], 'DOI': None, 'Published Date': datetime.datetime(2023, 10, 17, 16, 15, 34, tzinfo=datetime.timezone.utc), 'Summary': 'Large language models (LLMs) and their variants have shown extraordinary\\nefficacy across numerous downstream natural language processing (NLP) tasks,\\nwhich has presented a new vision for the development of NLP. Despite their\\nremarkable performance in natural language generating (NLG), LLMs lack a\\ndistinct focus on the emotion understanding domain. As a result, using LLMs for\\nemotion recognition may lead to suboptimal and inadequate precision. Another\\nlimitation of LLMs is that they are typical trained without leveraging\\nmulti-modal information. To overcome these limitations, we propose DialogueLLM,\\na context and emotion knowledge tuned LLM that is obtained by fine-tuning LLaMA\\nmodels with 13,638 multi-modal (i.e., texts and videos) emotional dialogues.\\nThe visual information is considered as the supplementary knowledge to\\nconstruct high-quality instructions. We offer a comprehensive evaluation of our\\nproposed model on three benchmarking emotion recognition in conversations (ERC)\\ndatasets and compare the results against the SOTA baselines and other SOTA\\nLLMs. Additionally, DialogueLLM-7B can be easily trained using LoRA on a 40GB\\nA100 GPU in 5 hours, facilitating reproducibility for other researchers.'}\n",
      "{'Title': 'VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights', 'PDF URL': 'http://arxiv.org/pdf/2310.11368v1', 'Author': [arxiv.Result.Author('Shanshan Xu'), arxiv.Result.Author('Leon Staufer'), arxiv.Result.Author('Santosh T. Y. S. S'), arxiv.Result.Author('Oana Ichim'), arxiv.Result.Author('Corina Heri'), arxiv.Result.Author('Matthias Grabmair')], 'DOI': None, 'Published Date': datetime.datetime(2023, 10, 17, 16, 5, 52, tzinfo=datetime.timezone.utc), 'Summary': 'Recognizing vulnerability is crucial for understanding and implementing\\ntargeted support to empower individuals in need. This is especially important\\nat the European Court of Human Rights (ECtHR), where the court adapts\\nConvention standards to meet actual individual needs and thus ensures effective\\nhuman rights protection. However, the concept of vulnerability remains elusive\\nat the ECtHR and no prior NLP research has dealt with it. To enable future\\nresearch in this area, we present VECHR, a novel expert-annotated multi-label\\ndataset comprising of vulnerability type classification and explanation\\nrationale. We benchmark the performance of state-of-the-art models on VECHR\\nfrom both prediction and explainability perspectives. Our results demonstrate\\nthe challenging nature of the task with lower prediction performance and\\nlimited agreement between models and experts. Further, we analyze the\\nrobustness of these models in dealing with out-of-domain (OOD) data and observe\\noverall limited performance. Our dataset poses unique challenges offering\\nsignificant room for improvement regarding performance, explainability, and\\nrobustness.'}\n",
      "{'Title': 'Utilizing Weak Supervision To Generate Indonesian Conservation Dataset', 'PDF URL': 'http://arxiv.org/pdf/2310.11258v1', 'Author': [arxiv.Result.Author('Mega Fransiska'), arxiv.Result.Author('Diah Pitaloka'), arxiv.Result.Author('Saripudin'), arxiv.Result.Author('Satrio Putra'), arxiv.Result.Author('Lintang Sutawika')], 'DOI': None, 'Published Date': datetime.datetime(2023, 10, 17, 13, 23, 18, tzinfo=datetime.timezone.utc), 'Summary': 'Weak supervision has emerged as a promising approach for rapid and\\nlarge-scale dataset creation in response to the increasing demand for\\naccelerated NLP development. By leveraging labeling functions, weak supervision\\nallows practitioners to generate datasets quickly by creating learned label\\nmodels that produce soft-labeled datasets. This paper aims to show how such an\\napproach can be utilized to build an Indonesian NLP dataset from conservation\\nnews text. We construct two types of datasets: multi-class classification and\\nsentiment classification. We then provide baseline experiments using various\\npretrained language models. These baseline results demonstrate test\\nperformances of 59.79% accuracy and 55.72% F1-score for sentiment\\nclassification, 66.87% F1-score-macro, 71.5% F1-score-micro, and 83.67% ROC-AUC\\nfor multi-class classification. Additionally, we release the datasets and\\nlabeling functions used in this work for further research and exploration.'}\n",
      "{'Title': 'Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations', 'PDF URL': 'http://arxiv.org/pdf/2310.11207v1', 'Author': [arxiv.Result.Author('Shiyuan Huang'), arxiv.Result.Author('Siddarth Mamidanna'), arxiv.Result.Author('Shreedhar Jangam'), arxiv.Result.Author('Yilun Zhou'), arxiv.Result.Author('Leilani H. Gilpin')], 'DOI': None, 'Published Date': datetime.datetime(2023, 10, 17, 12, 34, 32, tzinfo=datetime.timezone.utc), 'Summary': 'Large language models (LLMs) such as ChatGPT have demonstrated superior\\nperformance on a variety of natural language processing (NLP) tasks including\\nsentiment analysis, mathematical reasoning and summarization. Furthermore,\\nsince these models are instruction-tuned on human conversations to produce\\n\"helpful\" responses, they can and often will produce explanations along with\\nthe response, which we call self-explanations. For example, when analyzing the\\nsentiment of a movie review, the model may output not only the positivity of\\nthe sentiment, but also an explanation (e.g., by listing the sentiment-laden\\nwords such as \"fantastic\" and \"memorable\" in the review). How good are these\\nautomatically generated self-explanations? In this paper, we investigate this\\nquestion on the task of sentiment analysis and for feature attribution\\nexplanation, one of the most commonly studied settings in the interpretability\\nliterature (for pre-ChatGPT models). Specifically, we study different ways to\\nelicit the self-explanations, evaluate their faithfulness on a set of\\nevaluation metrics, and compare them to traditional explanation methods such as\\nocclusion or LIME saliency maps. Through an extensive set of experiments, we\\nfind that ChatGPT\\'s self-explanations perform on par with traditional ones, but\\nare quite different from them according to various agreement metrics, meanwhile\\nbeing much cheaper to produce (as they are generated along with the\\nprediction). In addition, we identified several interesting characteristics of\\nthem, which prompt us to rethink many current model interpretability practices\\nin the era of ChatGPT(-like) LLMs.'}\n",
      "{'Title': 'ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing', 'PDF URL': 'http://arxiv.org/pdf/2310.11166v1', 'Author': [arxiv.Result.Author('Quoc-Nam Nguyen'), arxiv.Result.Author('Thang Chau Phan'), arxiv.Result.Author('Duc-Vu Nguyen'), arxiv.Result.Author('Kiet Van Nguyen')], 'DOI': None, 'Published Date': datetime.datetime(2023, 10, 17, 11, 34, 50, tzinfo=datetime.timezone.utc), 'Summary': 'English and Chinese, known as resource-rich languages, have witnessed the\\nstrong development of transformer-based language models for natural language\\nprocessing tasks. Although Vietnam has approximately 100M people speaking\\nVietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA,\\nperformed well on general Vietnamese NLP tasks, including POS tagging and named\\nentity recognition. These pre-trained language models are still limited to\\nVietnamese social media tasks. In this paper, we present the first monolingual\\npre-trained language model for Vietnamese social media texts, ViSoBERT, which\\nis pre-trained on a large-scale corpus of high-quality and diverse Vietnamese\\nsocial media texts using XLM-R architecture. Moreover, we explored our\\npre-trained model on five important natural language downstream tasks on\\nVietnamese social media texts: emotion recognition, hate speech detection,\\nsentiment analysis, spam reviews detection, and hate speech spans detection.\\nOur experiments demonstrate that ViSoBERT, with far fewer parameters, surpasses\\nthe previous state-of-the-art models on multiple Vietnamese social media tasks.\\nOur ViSoBERT model is\\navailable\\\\footnote{\\\\url{https://huggingface.co/uitnlp/visobert}} only for\\nresearch purposes.'}\n"
     ]
    }
   ],
   "source": [
    "search = arxiv.Search(\n",
    "  query=\"NLP\",\n",
    "  max_results=5,\n",
    "  sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    ")\n",
    "\n",
    "# Initialize an empty list to store dictionaries\n",
    "paper_list = []\n",
    "\n",
    "for result in arxiv.Client().results(search):\n",
    "    # Create a dictionary for each paper\n",
    "    paper_info = {\n",
    "        'Title': result.title,\n",
    "        'PDF URL': result.pdf_url,\n",
    "        'Author' : result.authors,\n",
    "        'DOI' : result.doi,\n",
    "        'Published Date': result.published,\n",
    "        'Summary' : result.summary,\n",
    "\n",
    "\n",
    "\n",
    "    }\n",
    "    paper_list.append(paper_info)\n",
    "\n",
    "# Print the list of dictionaries\n",
    "for paper in paper_list:\n",
    "    print(paper)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(paper_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'arxiv_papers_5.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPermissionError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43marxiv_papers_5.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[1;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:3720\u001B[0m, in \u001B[0;36mNDFrame.to_csv\u001B[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001B[0m\n\u001B[0;32m   3709\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ABCDataFrame) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_frame()\n\u001B[0;32m   3711\u001B[0m formatter \u001B[38;5;241m=\u001B[39m DataFrameFormatter(\n\u001B[0;32m   3712\u001B[0m     frame\u001B[38;5;241m=\u001B[39mdf,\n\u001B[0;32m   3713\u001B[0m     header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3717\u001B[0m     decimal\u001B[38;5;241m=\u001B[39mdecimal,\n\u001B[0;32m   3718\u001B[0m )\n\u001B[1;32m-> 3720\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDataFrameRenderer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3721\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath_or_buf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3722\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlineterminator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlineterminator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3723\u001B[0m \u001B[43m    \u001B[49m\u001B[43msep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3724\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3725\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3726\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3727\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquoting\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquoting\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3728\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3729\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindex_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex_label\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3730\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3731\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunksize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3732\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquotechar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquotechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3733\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdate_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdate_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3734\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdoublequote\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdoublequote\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3735\u001B[0m \u001B[43m    \u001B[49m\u001B[43mescapechar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mescapechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3736\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3737\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[1;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1189\u001B[0m, in \u001B[0;36mDataFrameRenderer.to_csv\u001B[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001B[0m\n\u001B[0;32m   1168\u001B[0m     created_buffer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   1170\u001B[0m csv_formatter \u001B[38;5;241m=\u001B[39m CSVFormatter(\n\u001B[0;32m   1171\u001B[0m     path_or_buf\u001B[38;5;241m=\u001B[39mpath_or_buf,\n\u001B[0;32m   1172\u001B[0m     lineterminator\u001B[38;5;241m=\u001B[39mlineterminator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1187\u001B[0m     formatter\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfmt,\n\u001B[0;32m   1188\u001B[0m )\n\u001B[1;32m-> 1189\u001B[0m \u001B[43mcsv_formatter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m created_buffer:\n\u001B[0;32m   1192\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path_or_buf, StringIO)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:241\u001B[0m, in \u001B[0;36mCSVFormatter.save\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    237\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    238\u001B[0m \u001B[38;5;124;03mCreate the writer & save.\u001B[39;00m\n\u001B[0;32m    239\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    240\u001B[0m \u001B[38;5;66;03m# apply compression and byte/text conversion\u001B[39;00m\n\u001B[1;32m--> 241\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    242\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    243\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    244\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    245\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    246\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    247\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    248\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handles:\n\u001B[0;32m    249\u001B[0m \n\u001B[0;32m    250\u001B[0m     \u001B[38;5;66;03m# Note: self.encoding is irrelevant here\u001B[39;00m\n\u001B[0;32m    251\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwriter \u001B[38;5;241m=\u001B[39m csvlib\u001B[38;5;241m.\u001B[39mwriter(\n\u001B[0;32m    252\u001B[0m         handles\u001B[38;5;241m.\u001B[39mhandle,\n\u001B[0;32m    253\u001B[0m         lineterminator\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlineterminator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    258\u001B[0m         quotechar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquotechar,\n\u001B[0;32m    259\u001B[0m     )\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:856\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    851\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    852\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    853\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    854\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    855\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 856\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    857\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    858\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    859\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    860\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    861\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    862\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    863\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    864\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    865\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mPermissionError\u001B[0m: [Errno 13] Permission denied: 'arxiv_papers_5.csv'"
     ]
    }
   ],
   "source": [
    "df.to_csv('arxiv_papers_5.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
