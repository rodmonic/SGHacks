,Title,PDF URL,Author,DOI,Published Date,Summary,Journal Ref,Primary Category,Category,Entry ID
0,Natural Language Processing using Hadoop and KOSHIK,http://arxiv.org/pdf/1608.04434v1,"[arxiv.Result.Author('Emre Erturk'), arxiv.Result.Author('Hong Shi')]",,2016-08-15 23:09:21+00:00,"Natural language processing, as a data analytics related technology, is used
widely in many research areas such as artificial intelligence, human language
processing, and translation. At present, due to explosive growth of data, there
are many challenges for natural language processing. Hadoop is one of the
platforms that can process the large amount of data required for natural
language processing. KOSHIK is one of the natural language processing
architectures, and utilizes Hadoop and contains language processing components
such as Stanford CoreNLP and OpenNLP. This study describes how to build a
KOSHIK platform with the relevant tools, and provides the steps to analyze wiki
data. Finally, it evaluates and discusses the advantages and disadvantages of
the KOSHIK architecture, and gives recommendations on improving the processing
performance.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1608.04434v1
1,Integrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge,http://arxiv.org/pdf/2202.07138v2,"[arxiv.Result.Author('Kebing Jin'), arxiv.Result.Author('Hankz Hankui Zhuo')]",,2022-02-15 02:19:09+00:00,"Natural language processing (NLP) aims at investigating the interactions
between agents and humans, processing and analyzing large amounts of natural
language data. Large-scale language models play an important role in current
natural language processing. However, the challenges of explainability and
complexity come along with the developments of language models. One way is to
introduce logical relations and rules into natural language processing models,
such as making use of Automated Planning. Automated planning (AI planning)
focuses on building symbolic domain models and synthesizing plans to transit
initial states to goals based on domain models. Recently, there have been
plenty of works related to these two fields, which have the abilities to
generate explicit knowledge, e.g., preconditions and effects of action models,
and learn from tacit knowledge, e.g., neural models, respectively. Integrating
AI planning and natural language processing effectively improves the
communication between human and intelligent agents. This paper outlines the
commons and relations between AI planning and natural language processing,
argues that each of them can effectively impact on the other one by five areas:
(1) planning-based text understanding, (2) planning-based natural language
processing, (3) planning-based explainability, (4) text-based human-robot
interaction, and (5) applications. We also explore some potential future issues
between AI planning and natural language processing. To the best of our
knowledge, this survey is the first work that addresses the deep connections
between AI planning and Natural language processing.",,cs.AI,"['cs.AI', 'cs.CL']",http://arxiv.org/abs/2202.07138v2
2,Simple Natural Language Processing Tools for Danish,http://arxiv.org/pdf/1906.11608v2,[arxiv.Result.Author('Leon Derczynski')],,2019-06-27 13:15:12+00:00,"This technical note describes a set of baseline tools for automatic
processing of Danish text. The tools are machine-learning based, using natural
language processing models trained over previously annotated documents. They
are maintained at ITU Copenhagen and will always be freely available.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1906.11608v2
3,Towards the Study of Morphological Processing of the Tangkhul Language,http://arxiv.org/pdf/2006.16212v1,"[arxiv.Result.Author('Mirinso Shadang'), arxiv.Result.Author('Navanath Saharia'), arxiv.Result.Author('Thoudam Doren Singh')]",,2020-06-29 17:24:09+00:00,"There is no or little work on natural language processing of Tangkhul
language. The current work is a humble beginning of morphological processing of
this language using an unsupervised approach. We use a small corpus collected
from different sources of text books, short stories and articles of other
topics. Based on the experiments carried out, the morpheme identification task
using morphessor gives reasonable and interesting output despite using a small
corpus.","In proceeding of Regional International Conference on Natural
  Language Processing (regICON) 2017, 3rd and 4th November 2017, IIIT Senapati,
  Manipur, India",cs.CL,['cs.CL'],http://arxiv.org/abs/2006.16212v1
4,Natural Language Understanding with Distributed Representation,http://arxiv.org/pdf/1511.07916v1,[arxiv.Result.Author('Kyunghyun Cho')],,2015-11-24 23:23:13+00:00,"This is a lecture note for the course DS-GA 3001 <Natural Language
Understanding with Distributed Representation> at the Center for Data Science ,
New York University in Fall, 2015. As the name of the course suggests, this
lecture note introduces readers to a neural network based approach to natural
language understanding/processing. In order to make it as self-contained as
possible, I spend much time on describing basics of machine learning and neural
networks, only after which how they are used for natural languages is
introduced. On the language front, I almost solely focus on language modelling
and machine translation, two of which I personally find most fascinating and
most fundamental to natural language understanding.",,cs.CL,"['cs.CL', 'stat.ML']",http://arxiv.org/abs/1511.07916v1
5,A Primer on Neural Network Models for Natural Language Processing,http://arxiv.org/pdf/1510.00726v1,[arxiv.Result.Author('Yoav Goldberg')],,2015-10-02 20:17:33+00:00,"Over the past few years, neural networks have re-emerged as powerful
machine-learning models, yielding state-of-the-art results in fields such as
image recognition and speech processing. More recently, neural network models
started to be applied also to textual natural language signals, again with very
promising results. This tutorial surveys neural network models from the
perspective of natural language processing research, in an attempt to bring
natural-language researchers up to speed with the neural techniques. The
tutorial covers input encoding for natural language tasks, feed-forward
networks, convolutional networks, recurrent networks and recursive networks, as
well as the computation graph abstraction for automatic gradient computation.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1510.00726v1
6,Deploying Technology to Save Endangered Languages,http://arxiv.org/pdf/1908.08971v2,"[arxiv.Result.Author('Hilaria Cruz'), arxiv.Result.Author('Joseph Waring')]",,2019-08-23 18:31:35+00:00,"Computer scientists working on natural language processing, native speakers
of endangered languages, and field linguists to discuss ways to harness
Automatic Speech Recognition, especially neural networks, to automate
annotation, speech tagging, and text parsing on endangered languages.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1908.08971v2
7,Multilingual Text Classification for Dravidian Languages,http://arxiv.org/pdf/2112.01705v1,"[arxiv.Result.Author('Xiaotian Lin'), arxiv.Result.Author('Nankai Lin'), arxiv.Result.Author('Kanoksak Wattanachote'), arxiv.Result.Author('Shengyi Jiang'), arxiv.Result.Author('Lianxi Wang')]",,2021-12-03 04:26:49+00:00,"As the fourth largest language family in the world, the Dravidian languages
have become a research hotspot in natural language processing (NLP). Although
the Dravidian languages contain a large number of languages, there are
relatively few public available resources. Besides, text classification task,
as a basic task of natural language processing, how to combine it to multiple
languages in the Dravidian languages, is still a major difficulty in Dravidian
Natural Language Processing. Hence, to address these problems, we proposed a
multilingual text classification framework for the Dravidian languages. On the
one hand, the framework used the LaBSE pre-trained model as the base model.
Aiming at the problem of text information bias in multi-task learning, we
propose to use the MLM strategy to select language-specific words, and used
adversarial training to perturb them. On the other hand, in view of the problem
that the model cannot well recognize and utilize the correlation among
languages, we further proposed a language-specific representation module to
enrich semantic information for the model. The experimental results
demonstrated that the framework we proposed has a significant performance in
multilingual text classification tasks with each strategy achieving certain
improvements.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2112.01705v1
8,Fence - An Efficient Parser with Ambiguity Support for Model-Driven Language Specification,http://arxiv.org/pdf/1107.4687v2,"[arxiv.Result.Author('Luis Quesada'), arxiv.Result.Author('Fernando Berzal'), arxiv.Result.Author('Francisco J. Cortijo')]",,2011-07-23 12:56:02+00:00,"Model-based language specification has applications in the implementation of
language processors, the design of domain-specific languages, model-driven
software development, data integration, text mining, natural language
processing, and corpus-based induction of models. Model-based language
specification decouples language design from language processing and, unlike
traditional grammar-driven approaches, which constrain language designers to
specific kinds of grammars, it needs general parser generators able to deal
with ambiguities. In this paper, we propose Fence, an efficient bottom-up
parsing algorithm with lexical and syntactic ambiguity support that enables the
use of model-based language specification in practice.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1107.4687v2
9,A Precis of Language Models are not Models of Language,http://arxiv.org/pdf/2205.07634v1,[arxiv.Result.Author('Csaba Veres')],,2022-05-16 12:50:58+00:00,"Natural Language Processing is one of the leading application areas in the
current resurgence of Artificial Intelligence, spearheaded by Artificial Neural
Networks. We show that despite their many successes at performing linguistic
tasks, Large Neural Language Models are ill-suited as comprehensive models of
natural language. The wider implication is that, in spite of the often
overbearing optimism about AI, modern neural models do not represent a
revolution in our understanding of cognition.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2205.07634v1
10,Supporting Undotted Arabic with Pre-trained Language Models,http://arxiv.org/pdf/2111.09791v1,"[arxiv.Result.Author('Aviad Rom'), arxiv.Result.Author('Kfir Bar')]",,2021-11-18 16:47:56+00:00,"We observe a recent behaviour on social media, in which users intentionally
remove consonantal dots from Arabic letters, in order to bypass
content-classification algorithms. Content classification is typically done by
fine-tuning pre-trained language models, which have been recently employed by
many natural-language-processing applications. In this work we study the effect
of applying pre-trained Arabic language models on ""undotted"" Arabic texts. We
suggest several ways of supporting undotted texts with pre-trained models,
without additional training, and measure their performance on two Arabic
natural-language-processing downstream tasks. The results are encouraging; in
one of the tasks our method shows nearly perfect performance.",,cs.CL,"['cs.CL', 'cs.LG']",http://arxiv.org/abs/2111.09791v1
11,Continuous multilinguality with language vectors,http://arxiv.org/pdf/1612.07486v2,"[arxiv.Result.Author('Robert Östling'), arxiv.Result.Author('Jörg Tiedemann')]",,2016-12-22 08:29:25+00:00,"Most existing models for multilingual natural language processing (NLP) treat
language as a discrete category, and make predictions for either one language
or the other. In contrast, we propose using continuous vector representations
of language. We show that these can be learned efficiently with a
character-based neural language model, and used to improve inference about
language varieties not seen during training. In experiments with 1303 Bible
translations into 990 different languages, we empirically explore the capacity
of multilingual language models, and also show that the language vectors
capture genetic relationships between languages.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1612.07486v2
12,Including Signed Languages in Natural Language Processing,http://arxiv.org/pdf/2105.05222v2,"[arxiv.Result.Author('Kayo Yin'), arxiv.Result.Author('Amit Moryossef'), arxiv.Result.Author('Julie Hochgesang'), arxiv.Result.Author('Yoav Goldberg'), arxiv.Result.Author('Malihe Alikhani')]",,2021-05-11 17:37:55+00:00,"Signed languages are the primary means of communication for many deaf and
hard of hearing individuals. Since signed languages exhibit all the fundamental
linguistic properties of natural language, we believe that tools and theories
of Natural Language Processing (NLP) are crucial towards its modeling. However,
existing research in Sign Language Processing (SLP) seldom attempt to explore
and leverage the linguistic organization of signed languages. This position
paper calls on the NLP community to include signed languages as a research area
with high social and scientific impact. We first discuss the linguistic
properties of signed languages to consider during their modeling. Then, we
review the limitations of current SLP models and identify the open challenges
to extend NLP to signed languages. Finally, we urge (1) the adoption of an
efficient tokenization method; (2) the development of linguistically-informed
models; (3) the collection of real-world signed language data; (4) the
inclusion of local signed language communities as an active and leading voice
in the direction of research.",,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2105.05222v2
13,Self-move and Other-move: Quantum Categorical Foundations of Japanese,http://arxiv.org/pdf/2210.04451v1,[arxiv.Result.Author('Ryder Dale Walton')],,2022-10-10 06:26:59+00:00,"The purpose of this work is to contribute toward the larger goal of creating
a Quantum Natural Language Processing (QNLP) translator program. This work
contributes original diagrammatic representations of the Japanese language
based on prior work that accomplished on the English language based on category
theory. The germane differences between the English and Japanese languages are
emphasized to help address English language bias in the current body of
research. Additionally, topological principles of these diagrams and many
potential avenues for further research are proposed. Why is this endeavor
important? Hundreds of languages have developed over the course of millennia
coinciding with the evolution of human interaction across time and geographic
location. These languages are foundational to human survival, experience,
flourishing, and living the good life. They are also, however, the strongest
barrier between people groups. Over the last several decades, advancements in
Natural Language Processing (NLP) have made it easier to bridge the gap between
individuals who do not share a common language or culture. Tools like Google
Translate and DeepL make it easier than ever before to share our experiences
with people globally. Nevertheless, these tools are still inadequate as they
fail to convey our ideas across the language barrier fluently, leaving people
feeling anxious and embarrassed. This is particularly true of languages born
out of substantially different cultures, such as English and Japanese. Quantum
computers offer the best chance to achieve translation fluency in that they are
better suited to simulating the natural world and natural phenomenon such as
natural speech.
  Keywords: category theory, DisCoCat, DisCoCirc, Japanese grammar, English
grammar, translation, topology, Quantum Natural Language Processing, Natural
Language Processing",,cs.CL,['cs.CL'],http://arxiv.org/abs/2210.04451v1
14,Problems and Countermeasures in Natural Language Processing Evaluation,http://arxiv.org/pdf/2104.09712v1,"[arxiv.Result.Author('Qingxiu Dong'), arxiv.Result.Author('Zhifang Sui'), arxiv.Result.Author('Weidong Zhan'), arxiv.Result.Author('Baobao Chang')]",,2021-04-20 01:35:16+00:00,"Evaluation in natural language processing guides and promotes research on
models and methods. In recent years, new evalua-tion data sets and evaluation
tasks have been continuously proposed. At the same time, a series of problems
exposed by ex-isting evaluation have also restricted the progress of natural
language processing technology. Starting from the concept, com-position,
development and meaning of natural language evaluation, this article classifies
and summarizes the tasks and char-acteristics of mainstream natural language
evaluation, and then summarizes the problems and causes of natural language
pro-cessing evaluation. Finally, this article refers to the human language
ability evaluation standard, puts forward the concept of human-like machine
language ability evaluation, and proposes a series of basic principles and
implementation ideas for hu-man-like machine language ability evaluation from
the three aspects of reliability, difficulty and validity.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2104.09712v1
15,A Survey of Resources and Methods for Natural Language Processing of Serbian Language,http://arxiv.org/pdf/2304.05468v1,"[arxiv.Result.Author('Ulfeta A. Marovac'), arxiv.Result.Author('Aldina R. Avdić'), arxiv.Result.Author('Nikola Lj. Milošević')]",,2023-04-11 19:33:41+00:00,"The Serbian language is a Slavic language spoken by over 12 million speakers
and well understood by over 15 million people. In the area of natural language
processing, it can be considered a low-resourced language. Also, Serbian is
considered a high-inflectional language. The combination of many word
inflections and low availability of language resources makes natural language
processing of Serbian challenging. Nevertheless, over the past three decades,
there have been a number of initiatives to develop resources and methods for
natural language processing of Serbian, ranging from developing a corpus of
free text from books and the internet, annotated corpora for classification and
named entity recognition tasks to various methods and models performing these
tasks. In this paper, we review the initiatives, resources, methods, and their
availability.",,cs.CL,"['cs.CL', 'cs.DL', 'cs.HC', 'A.1']",http://arxiv.org/abs/2304.05468v1
16,A natural language interface to a graph-based bibliographic information retrieval system,http://arxiv.org/pdf/1612.03231v1,"[arxiv.Result.Author('Yongjun Zhu'), arxiv.Result.Author('Erjia Yan'), arxiv.Result.Author('Il-Yeol Song')]",,2016-12-10 00:32:28+00:00,"With the ever-increasing scientific literature, there is a need on a natural
language interface to bibliographic information retrieval systems to retrieve
related information effectively. In this paper, we propose a natural language
interface, NLI-GIBIR, to a graph-based bibliographic information retrieval
system. In designing NLI-GIBIR, we developed a novel framework that can be
applicable to graph-based bibliographic information retrieval systems. Our
framework integrates algorithms/heuristics for interpreting and analyzing
natural language bibliographic queries. NLI-GIBIR allows users to search for a
variety of bibliographic data through natural language. A series of text- and
linguistic-based techniques are used to analyze and answer natural language
queries, including tokenization, named entity recognition, and syntactic
analysis. We find that our framework can effectively represents and addresses
complex bibliographic information needs. Thus, the contributions of this paper
are as follows: First, to our knowledge, it is the first attempt to propose a
natural language interface to graph-based bibliographic information retrieval.
Second, we propose a novel customized natural language processing framework
that integrates a few original algorithms/heuristics for interpreting and
analyzing natural language bibliographic queries. Third, we show that the
proposed framework and natural language interface provide a practical solution
in building real-world natural language interface-based bibliographic
information retrieval systems. Our experimental results show that the presented
system can correctly answer 39 out of 40 example natural language queries with
varying lengths and complexities.",,cs.IR,"['cs.IR', 'cs.CL']",http://arxiv.org/abs/1612.03231v1
17,Specifying Logic Programs in Controlled Natural Language,http://arxiv.org/pdf/cmp-lg/9507009v1,"[arxiv.Result.Author('Norbert E. Fuchs'), arxiv.Result.Author('Rolf Schwitter')]",,1995-07-21 17:44:05+00:00,"Writing specifications for computer programs is not easy since one has to
take into account the disparate conceptual worlds of the application domain and
of software development. To bridge this conceptual gap we propose controlled
natural language as a declarative and application-specific specification
language. Controlled natural language is a subset of natural language that can
be accurately and efficiently processed by a computer, but is expressive enough
to allow natural usage by non-specialists. Specifications in controlled natural
language are automatically translated into Prolog clauses, hence become formal
and executable. The translation uses a definite clause grammar (DCG) enhanced
by feature structures. Inter-text references of the specification, e.g.
anaphora, are resolved with the help of discourse representation theory (DRT).
The generated Prolog clauses are added to a knowledge base. We have implemented
a prototypical specification system that successfully processes the
specification of a simple automated teller machine.","Proceedings CLNLP 95, COMPULOGNET/ELSNET/EAGLES",cmp-lg,"['cmp-lg', 'cs.CL']",http://arxiv.org/abs/cmp-lg/9507009v1
18,On Even Linear Indexed Languages with a Reduction to the Learning of Context-Free Languages,http://arxiv.org/pdf/1312.0175v2,[arxiv.Result.Author('Benjamin Caulfield')],,2013-12-01 03:16:22+00:00,"This paper presents a restricted form of linear indexed grammars, called even
linear indexed grammars, which yield the even linear indexed languages. These
languages properly contain the context-free languages and are contained in the
set of linear indexed languages. We show that several patterns found in natural
languages are also generated by these grammars, including crossing
dependencies, copying, and multiple agreements. We discuss the learning problem
for even linear indexed languages and show that it is reducible to that of the
context-free languages. The closure properties for this class of languages are
also presented.",,cs.FL,['cs.FL'],http://arxiv.org/abs/1312.0175v2
19,Natural Language Generation Using Link Grammar for General Conversational Intelligence,http://arxiv.org/pdf/2105.00830v1,"[arxiv.Result.Author('Vignav Ramesh'), arxiv.Result.Author('Anton Kolonin')]",,2021-04-19 06:16:07+00:00,"Many current artificial general intelligence (AGI) and natural language
processing (NLP) architectures do not possess general conversational
intelligence--that is, they either do not deal with language or are unable to
convey knowledge in a form similar to the human language without manual,
labor-intensive methods such as template-based customization. In this paper, we
propose a new technique to automatically generate grammatically valid sentences
using the Link Grammar database. This natural language generation method far
outperforms current state-of-the-art baselines and may serve as the final
component in a proto-AGI question answering pipeline that understandably
handles natural language material.",,cs.CL,"['cs.CL', 'cs.AI']",http://arxiv.org/abs/2105.00830v1
20,Language Tasks and Language Games: On Methodology in Current Natural Language Processing Research,http://arxiv.org/pdf/1908.10747v1,[arxiv.Result.Author('David Schlangen')],,2019-08-28 14:29:13+00:00,"""This paper introduces a new task and a new dataset"", ""we improve the state
of the art in X by Y"" -- it is rare to find a current natural language
processing paper (or AI paper more generally) that does not contain such
statements. What is mostly left implicit, however, is the assumption that this
necessarily constitutes progress, and what it constitutes progress towards.
Here, we make more precise the normally impressionistically used notions of
language task and language game and ask how a research programme built on these
might make progress towards the goal of modelling general language competence.",,cs.CL,"['cs.CL', 'cs.AI']",http://arxiv.org/abs/1908.10747v1
21,ANGLEr: A Next-Generation Natural Language Exploratory Framework,http://arxiv.org/pdf/2206.08266v1,"[arxiv.Result.Author('Timotej Knez'), arxiv.Result.Author('Marko Bajec'), arxiv.Result.Author('Slavko Žitnik')]",,2022-05-10 13:32:13+00:00,"Natural language processing is used for solving a wide variety of problems.
Some scholars and interest groups working with language resources are not well
versed in programming, so there is a need for a good graphical framework that
allows users to quickly design and test natural language processing pipelines
without the need for programming. The existing frameworks do not satisfy all
the requirements for such a tool. We, therefore, propose a new framework that
provides a simple way for its users to build language processing pipelines. It
also allows a simple programming language agnostic way for adding new modules,
which will help the adoption by natural language processing developers and
researchers. The main parts of the proposed framework consist of (a) a
pluggable Docker-based architecture, (b) a general data model, and (c) APIs
description along with the graphical user interface. The proposed design is
being used for implementation of a new natural language processing framework,
called ANGLEr.",,cs.CL,"['cs.CL', 'cs.AI']",http://arxiv.org/abs/2206.08266v1
22,Information Flow in Pregroup Models of Natural Language,http://arxiv.org/pdf/1811.03273v1,[arxiv.Result.Author('Peter M. Hines')],10.4204/EPTCS.283.2,2018-11-08 05:10:34+00:00,"This paper is about pregroup models of natural languages, and how they relate
to the explicitly categorical use of pregroups in Compositional Distributional
Semantics and Natural Language Processing. These categorical interpretations
make certain assumptions about the nature of natural languages that, when
stated formally, may be seen to impose strong restrictions on pregroup grammars
for natural languages.
  We formalize this as a hypothesis about the form that pregroup models of
natural languages must take, and demonstrate by an artificial language example
that these restrictions are not imposed by the pregroup axioms themselves. We
compare and contrast the artificial language examples with natural languages
(using Welsh, a language where the 'noun' type cannot be taken as primitive, as
an illustrative example).
  The hypothesis is simply that there must exist a causal connection, or
information flow, between the words of a sentence in a language whose purpose
is to communicate information. This is not necessarily the case with formal
languages that are simply generated by a series of 'meaning-free' rules. This
imposes restrictions on the types of pregroup grammars that we expect to find
in natural languages; we formalize this in algebraic, categorical, and
graphical terms.
  We take some preliminary steps in providing conditions that ensure pregroup
models satisfy these conjectured properties, and discuss the more general forms
this hypothesis may take.","EPTCS 283, 2018, pp. 13-27",cs.CL,"['cs.CL', 'cs.FL']",http://arxiv.org/abs/1811.03273v1
23,Curriculum learning for language modeling,http://arxiv.org/pdf/2108.02170v1,[arxiv.Result.Author('Daniel Campos')],,2021-08-04 16:53:43+00:00,"Language Models like ELMo and BERT have provided robust representations of
natural language, which serve as the language understanding component for a
diverse range of downstream tasks.Curriculum learning is a method that employs
a structured training regime instead, which has been leveraged in computer
vision and machine translation to improve model training speed and model
performance. While language models have proven transformational for the natural
language processing community, these models have proven expensive,
energy-intensive, and challenging to train. In this work, we explore the effect
of curriculum learning on language model pretraining using various
linguistically motivated curricula and evaluate transfer performance on the
GLUE Benchmark. Despite a broad variety of training methodologies and
experiments we do not find compelling evidence that curriculum learning methods
improve language model training.",,cs.CL,"['cs.CL', 'cs.AI']",http://arxiv.org/abs/2108.02170v1
24,Fuzzy Modeling and Natural Language Processing for Panini's Sanskrit Grammar,http://arxiv.org/pdf/1006.2835v1,[arxiv.Result.Author('P. Venkata Subba Reddy')],,2010-06-14 20:07:32+00:00,"Indian languages have long history in World Natural languages. Panini was the
first to define Grammar for Sanskrit language with about 4000 rules in fifth
century. These rules contain uncertainty information. It is not possible to
Computer processing of Sanskrit language with uncertain information. In this
paper, fuzzy logic and fuzzy reasoning are proposed to deal to eliminate
uncertain information for reasoning with Sanskrit grammar. The Sanskrit
language processing is also discussed in this paper.","Journal of Computer Science and Engineering, Volume 1, Issue 1,
  p99-101, May 2010",cs.CL,['cs.CL'],http://arxiv.org/abs/1006.2835v1
25,Natural Language Understanding Based on Semantic Relations between Sentences,http://arxiv.org/pdf/1212.4674v1,[arxiv.Result.Author('Hyeok Kong')],,2012-12-19 14:40:38+00:00,"In this paper, we define event expression over sentences of natural language
and semantic relations between events. Based on this definition, we formally
consider text understanding process having events as basic unit.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1212.4674v1
26,Thoth: Improved Rapid Serial Visual Presentation using Natural Language Processing,http://arxiv.org/pdf/1908.01699v1,[arxiv.Result.Author('David Awad')],,2019-08-05 15:45:39+00:00,"Thoth is a tool designed to combine many different types of speed reading
technology. The largest insight is using natural language parsing for more
optimal rapid serial visual presentation and more effective reading
information.",,cs.CL,"['cs.CL', 'cs.HC']",http://arxiv.org/abs/1908.01699v1
27,Pretraining with Artificial Language: Studying Transferable Knowledge in Language Models,http://arxiv.org/pdf/2203.10326v2,"[arxiv.Result.Author('Ryokan Ri'), arxiv.Result.Author('Yoshimasa Tsuruoka')]",,2022-03-19 13:29:48+00:00,"We investigate what kind of structural knowledge learned in neural network
encoders is transferable to processing natural language. We design artificial
languages with structural properties that mimic natural language, pretrain
encoders on the data, and see how much performance the encoder exhibits on
downstream tasks in natural language. Our experimental results show that
pretraining with an artificial language with a nesting dependency structure
provides some knowledge transferable to natural language. A follow-up probing
analysis indicates that its success in the transfer is related to the amount of
encoded contextual information and what is transferred is the knowledge of
position-aware context dependence of language. Our results provide insights
into how neural network encoders process human languages and the source of
cross-lingual transferability of recent multilingual language models.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2203.10326v2
28,Categorical Tools for Natural Language Processing,http://arxiv.org/pdf/2212.06636v1,[arxiv.Result.Author('Giovanni de Felice')],,2022-12-13 15:12:37+00:00,"This thesis develops the translation between category theory and
computational linguistics as a foundation for natural language processing. The
three chapters deal with syntax, semantics and pragmatics. First, string
diagrams provide a unified model of syntactic structures in formal grammars.
Second, functors compute semantics by turning diagrams into logical, tensor,
neural or quantum computation. Third, the resulting functorial models can be
composed to form games where equilibria are the solutions of language
processing tasks. This framework is implemented as part of DisCoPy, the Python
library for computing with string diagrams. We describe the correspondence
between categorical, linguistic and computational structures, and demonstrate
their applications in compositional natural language processing.",,cs.CL,"['cs.CL', 'math.CT']",http://arxiv.org/abs/2212.06636v1
29,Unnatural Language Processing: Bridging the Gap Between Synthetic and Natural Language Data,http://arxiv.org/pdf/2004.13645v1,"[arxiv.Result.Author('Alana Marzoev'), arxiv.Result.Author('Samuel Madden'), arxiv.Result.Author('M. Frans Kaashoek'), arxiv.Result.Author('Michael Cafarella'), arxiv.Result.Author('Jacob Andreas')]",,2020-04-28 16:41:00+00:00,"Large, human-annotated datasets are central to the development of natural
language processing models. Collecting these datasets can be the most
challenging part of the development process. We address this problem by
introducing a general purpose technique for ``simulation-to-real'' transfer in
language understanding problems with a delimited set of target behaviors,
making it possible to develop models that can interpret natural utterances
without natural training data. We begin with a synthetic data generation
procedure, and train a model that can accurately interpret utterances produced
by the data generator. To generalize to natural utterances, we automatically
find projections of natural language utterances onto the support of the
synthetic language, using learned sentence embeddings to define a distance
metric. With only synthetic training data, our approach matches or outperforms
state-of-the-art models trained on natural language data in several domains.
These results suggest that simulation-to-real transfer is a practical framework
for developing NLP applications, and that improved models for transfer might
provide wide-ranging improvements in downstream tasks.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2004.13645v1
30,Challenges Encountered in Turkish Natural Language Processing Studies,http://arxiv.org/pdf/2101.11436v1,"[arxiv.Result.Author('Kadir Tohma'), arxiv.Result.Author('Yakup Kutlu')]",,2021-01-21 08:30:33+00:00,"Natural language processing is a branch of computer science that combines
artificial intelligence with linguistics. It aims to analyze a language element
such as writing or speaking with software and convert it into information.
Considering that each language has its own grammatical rules and vocabulary
diversity, the complexity of the studies in this field is somewhat
understandable. For instance, Turkish is a very interesting language in many
ways. Examples of this are agglutinative word structure, consonant/vowel
harmony, a large number of productive derivational morphemes (practically
infinite vocabulary), derivation and syntactic relations, a complex emphasis on
vocabulary and phonological rules. In this study, the interesting features of
Turkish in terms of natural language processing are mentioned. In addition,
summary info about natural language processing techniques, systems and various
sources developed for Turkish are given.","Natural and Engineering Sciences, 2020",cs.CL,"['cs.CL', 'cs.AI']",http://arxiv.org/abs/2101.11436v1
31,Generalisation of language and knowledge models for corpus analysis,http://arxiv.org/pdf/1203.3227v1,[arxiv.Result.Author('Anton Loss')],,2012-03-14 22:06:42+00:00,"This paper takes new look on language and knowledge modelling for corpus
linguistics. Using ideas of Chaitin, a line of argument is made against
language/knowledge separation in Natural Language Processing. A simplistic
model, that generalises approaches to language and knowledge, is proposed. One
of hypothetical consequences of this model is Strong AI.",,cs.AI,"['cs.AI', 'cs.CL']",http://arxiv.org/abs/1203.3227v1
32,The Dissecting Power of Regular Languages,http://arxiv.org/pdf/1202.4883v3,"[arxiv.Result.Author('Tomoyuki Yamakami'), arxiv.Result.Author('Yuichi Kato')]",,2012-02-22 11:16:47+00:00,"A recent study on structural properties of regular and context-free languages
has greatly promoted our basic understandings of the complex behaviors of those
languages. We continue the study to examine how regular languages behave when
they need to cut numerous infinite languages. A particular interest rests on a
situation in which a regular language needs to ""dissect"" a given infinite
language into two subsets of infinite size. Every context-free language is
dissected by carefully chosen regular languages (or it is REG-dissectible). In
a larger picture, we show that constantly-growing languages and semi-linear
languages are REG-dissectible. Under certain natural conditions, complements
and finite intersections of semi-linear languages also become REG-dissectible.
Restricted to bounded languages, the intersections of finitely many
context-free languages and, more surprisingly, the entire Boolean hierarchy
over bounded context-free languages are REG-dissectible. As an immediate
application of the REG-dissectibility, we show another structural property, in
which an appropriate bounded context-free language can ""separate with infinite
margins"" two given nested infinite bounded context-free languages.","Information Processing Letters, Vol.113, pp.116-122, 2013",cs.FL,"['cs.FL', 'cs.CC']",http://arxiv.org/abs/1202.4883v3
33,Cedille: A large autoregressive French language model,http://arxiv.org/pdf/2202.03371v1,"[arxiv.Result.Author('Martin Müller'), arxiv.Result.Author('Florian Laurent')]",,2022-02-07 17:40:43+00:00,"Scaling up the size and training of autoregressive language models has
enabled novel ways of solving Natural Language Processing tasks using zero-shot
and few-shot learning. While extreme-scale language models such as GPT-3 offer
multilingual capabilities, zero-shot learning for languages other than English
remain largely unexplored. Here, we introduce Cedille, a large open source
auto-regressive language model, specifically trained for the French language.
Our results show that Cedille outperforms existing French language models and
is competitive with GPT-3 on a range of French zero-shot benchmarks.
Furthermore, we provide an in-depth comparison of the toxicity exhibited by
these models, showing that Cedille marks an improvement in language model
safety thanks to dataset filtering.",,cs.CL,"['cs.CL', '68T50', 'I.2.7']",http://arxiv.org/abs/2202.03371v1
34,Development of Deep Learning Based Natural Language Processing Model for Turkish,http://arxiv.org/pdf/1905.05699v1,"[arxiv.Result.Author('Baris Baburoglu'), arxiv.Result.Author('Adem Tekerek'), arxiv.Result.Author('Mehmet Tekerek')]",,2019-05-07 21:09:49+00:00,"Natural language is one of the most fundamental features that distinguish
people from other living things and enable people to communicate each other.
Language is a tool that enables people to express their feelings and thoughts
and to transfers cultures through generations. Texts and audio are examples of
natural language in daily life. In the natural language, many words disappear
in time, on the other hand new words are derived. Therefore, while the process
of natural language processing (NLP) is complex even for human, it is difficult
to process in computer system. The area of linguistics examines how people use
language. NLP, which requires the collaboration of linguists and computer
scientists, plays an important role in human computer interaction. Studies in
NLP have increased with the use of artificial intelligence technologies in the
field of linguistics. With the deep learning methods which are one of the
artificial intelligence study areas, platforms close to natural language are
being developed. Developed platforms for language comprehension, machine
translation and part of speech (POS) tagging benefit from deep learning
methods. Recurrent Neural Network (RNN), one of the deep learning
architectures, is preferred for processing sequential data such as text or
audio data. In this study, Turkish POS tagging model has been proposed by using
Bidirectional Long-Short Term Memory (BLSTM) which is an RNN type. The proposed
POS tagging model is provided to natural language researchers with a platform
that allows them to perform and use their own analysis. In the development
phase of the platform developed by using BLSTM, the error rate of the POS
tagger has been reduced by taking feedback with expert opinion.",,cs.CL,"['cs.CL', 'cs.LG']",http://arxiv.org/abs/1905.05699v1
35,JamPatoisNLI: A Jamaican Patois Natural Language Inference Dataset,http://arxiv.org/pdf/2212.03419v1,"[arxiv.Result.Author('Ruth-Ann Armstrong'), arxiv.Result.Author('John Hewitt'), arxiv.Result.Author('Christopher Manning')]",,2022-12-07 03:07:02+00:00,"JamPatoisNLI provides the first dataset for natural language inference in a
creole language, Jamaican Patois. Many of the most-spoken low-resource
languages are creoles. These languages commonly have a lexicon derived from a
major world language and a distinctive grammar reflecting the languages of the
original speakers and the process of language birth by creolization. This gives
them a distinctive place in exploring the effectiveness of transfer from large
monolingual or multilingual pretrained models. While our work, along with
previous work, shows that transfer from these models to low-resource languages
that are unrelated to languages in their training set is not very effective, we
would expect stronger results from transfer to creoles. Indeed, our experiments
show considerably better results from few-shot learning of JamPatoisNLI than
for such unrelated languages, and help us begin to understand how the unique
relationship between creoles and their high-resource base languages affect
cross-lingual transfer. JamPatoisNLI, which consists of naturally-occurring
premises and expert-written hypotheses, is a step towards steering research
into a traditionally underserved language and a useful benchmark for
understanding cross-lingual NLP.",,cs.CL,"['cs.CL', 'cs.LG', 'I.2.7']",http://arxiv.org/abs/2212.03419v1
36,"Natural Language Reasoning, A Survey",http://arxiv.org/pdf/2303.14725v2,"[arxiv.Result.Author('Fei Yu'), arxiv.Result.Author('Hongbo Zhang'), arxiv.Result.Author('Prayag Tiwari'), arxiv.Result.Author('Benyou Wang')]",,2023-03-26 13:44:18+00:00,"This survey paper proposes a clearer view of natural language reasoning in
the field of Natural Language Processing (NLP), both conceptually and
practically. Conceptually, we provide a distinct definition for natural
language reasoning in NLP, based on both philosophy and NLP scenarios, discuss
what types of tasks require reasoning, and introduce a taxonomy of reasoning.
Practically, we conduct a comprehensive literature review on natural language
reasoning in NLP, mainly covering classical logical reasoning, natural language
inference, multi-hop question answering, and commonsense reasoning. The paper
also identifies and views backward reasoning, a powerful paradigm for
multi-step reasoning, and introduces defeasible reasoning as one of the most
important future directions in natural language reasoning research. We focus on
single-modality unstructured natural language text, excluding neuro-symbolic
techniques and mathematical reasoning.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2303.14725v2
37,File mapping Rule-based DBMS and Natural Language Processing,http://arxiv.org/pdf/cs/0106016v1,[arxiv.Result.Author('Vjacheslav M. Novikov')],,2001-06-10 14:56:51+00:00,"This paper describes the system of storage, extract and processing of
information structured similarly to the natural language. For recursive
inference the system uses the rules having the same representation, as the
data. The environment of storage of information is provided with the File
Mapping (SHM) mechanism of operating system. In the paper the main principles
of construction of dynamic data structure and language for record of the
inference rules are stated; the features of available implementation are
considered and the description of the application realizing semantic
information retrieval on the natural language is given.",,cs.CL,"['cs.CL', 'cs.AI', 'cs.DB', 'cs.IR', 'cs.LG', 'cs.PL', 'D.3.2; H.2.4']",http://arxiv.org/abs/cs/0106016v1
38,GANCoder: An Automatic Natural Language-to-Programming Language Translation Approach based on GAN,http://arxiv.org/pdf/1912.00609v1,"[arxiv.Result.Author('Yabing Zhu'), arxiv.Result.Author('Yanfeng Zhang'), arxiv.Result.Author('Huili Yang'), arxiv.Result.Author('Fangjing Wang')]",,2019-12-02 07:41:25+00:00,"We propose GANCoder, an automatic programming approach based on Generative
Adversarial Networks (GAN), which can generate the same functional and logical
programming language codes conditioned on the given natural language
utterances. The adversarial training between generator and discriminator helps
generator learn distribution of dataset and improve code generation quality.
Our experimental results show that GANCoder can achieve comparable accuracy
with the state-of-the-art methods and is more stable when programming
languages.",NLPCC 2019: Natural Language Processing and Chinese Computing,cs.CL,"['cs.CL', 'cs.LG']",http://arxiv.org/abs/1912.00609v1
39,Cross-lingual Adaption Model-Agnostic Meta-Learning for Natural Language Understanding,http://arxiv.org/pdf/2111.05805v1,"[arxiv.Result.Author('Qianying Liu'), arxiv.Result.Author('Fei Cheng'), arxiv.Result.Author('Sadao Kurohashi')]",,2021-11-10 16:53:50+00:00,"Meta learning with auxiliary languages has demonstrated promising
improvements for cross-lingual natural language processing. However, previous
studies sample the meta-training and meta-testing data from the same language,
which limits the ability of the model for cross-lingual transfer. In this
paper, we propose XLA-MAML, which performs direct cross-lingual adaption in the
meta-learning stage. We conduct zero-shot and few-shot experiments on Natural
Language Inference and Question Answering. The experimental results demonstrate
the effectiveness of our method across different languages, tasks, and
pretrained models. We also give analysis on various cross-lingual specific
settings for meta-learning including sampling strategy and parallelism.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2111.05805v1
40,Integrating Linguistic Theory and Neural Language Models,http://arxiv.org/pdf/2207.09643v1,[arxiv.Result.Author('Bai Li')],,2022-07-20 04:20:46+00:00,"Transformer-based language models have recently achieved remarkable results
in many natural language tasks. However, performance on leaderboards is
generally achieved by leveraging massive amounts of training data, and rarely
by encoding explicit linguistic knowledge into neural models. This has led many
to question the relevance of linguistics for modern natural language
processing. In this dissertation, I present several case studies to illustrate
how theoretical linguistics and neural language models are still relevant to
each other. First, language models are useful to linguists by providing an
objective tool to measure semantic distance, which is difficult to do using
traditional methods. On the other hand, linguistic theory contributes to
language modelling research by providing frameworks and sources of data to
probe our language models for specific aspects of language understanding.
  This thesis contributes three studies that explore different aspects of the
syntax-semantics interface in language models. In the first part of my thesis,
I apply language models to the problem of word class flexibility. Using mBERT
as a source of semantic distance measurements, I present evidence in favour of
analyzing word class flexibility as a directional process. In the second part
of my thesis, I propose a method to measure surprisal at intermediate layers of
language models. My experiments show that sentences containing morphosyntactic
anomalies trigger surprisals earlier in language models than semantic and
commonsense anomalies. Finally, in the third part of my thesis, I adapt several
psycholinguistic studies to show that language models contain knowledge of
argument structure constructions. In summary, my thesis develops new
connections between natural language processing, linguistic theory, and
psycholinguistics to provide fresh perspectives for the interpretation of
language models.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2207.09643v1
41,Composition by Conversation,http://arxiv.org/pdf/1709.02076v1,"[arxiv.Result.Author('Donya Quick'), arxiv.Result.Author('Clayton T. Morrison')]",,2017-09-07 05:39:00+00:00,"Most musical programming languages are developed purely for coding virtual
instruments or algorithmic compositions. Although there has been some work in
the domain of musical query languages for music information retrieval, there
has been little attempt to unify the principles of musical programming and
query languages with cognitive and natural language processing models that
would facilitate the activity of composition by conversation. We present a
prototype framework, called MusECI, that merges these domains, permitting
score-level algorithmic composition in a text editor while also supporting
connectivity to existing natural language processing frameworks.",,cs.SD,"['cs.SD', 'cs.CL', 'cs.IR', 'cs.PL', 'H.5.1; H.5.5; I.2.4; I.2.5; I.2.7']",http://arxiv.org/abs/1709.02076v1
42,Controlled Natural Languages and Default Reasoning,http://arxiv.org/pdf/1905.04422v1,[arxiv.Result.Author('Tiantian Gao')],,2019-05-11 02:02:55+00:00,"Controlled natural languages (CNLs) are effective languages for knowledge
representation and reasoning. They are designed based on certain natural
languages with restricted lexicon and grammar. CNLs are unambiguous and simple
as opposed to their base languages. They preserve the expressiveness and
coherence of natural languages. In this report, we focus on a class of CNLs,
called machine-oriented CNLs, which have well-defined semantics that can be
deterministically translated into formal languages, such as Prolog, to do
logical reasoning. Over the past 20 years, a number of machine-oriented CNLs
emerged and have been used in many application domains for problem solving and
question answering. However, few of them support non-monotonic inference. In
our work, we propose non-monotonic extensions of CNL to support defeasible
reasoning.
  In the first part of this report, we survey CNLs and compare three
influential systems: Attempto Controlled English (ACE), Processable English
(PENG), and Computer-processable English (CPL). We compare their language
design, semantic interpretations, and reasoning services. In the second part of
this report, we first identify typical non-monotonicity in natural languages,
such as defaults, exceptions and conversational implicatures. Then, we propose
their representation in CNL and the corresponding formalizations in a form of
defeasible reasoning known as Logic Programming with Defaults and Argumentation
Theory (LPDA).",,cs.AI,"['cs.AI', 'cs.CL']",http://arxiv.org/abs/1905.04422v1
43,Attributes as Semantic Units between Natural Language and Visual Recognition,http://arxiv.org/pdf/1604.03249v1,[arxiv.Result.Author('Marcus Rohrbach')],,2016-04-12 05:23:26+00:00,"Impressive progress has been made in the fields of computer vision and
natural language processing. However, it remains a challenge to find the best
point of interaction for these very different modalities. In this chapter we
discuss how attributes allow us to exchange information between the two
modalities and in this way lead to an interaction on a semantic level.
Specifically we discuss how attributes allow using knowledge mined from
language resources for recognizing novel visual categories, how we can generate
sentence description about images and video, how we can ground natural language
in visual content, and finally, how we can answer natural language questions
about images.",,cs.CV,"['cs.CV', 'cs.CL']",http://arxiv.org/abs/1604.03249v1
44,An Overview of Natural Language State Representation for Reinforcement Learning,http://arxiv.org/pdf/2007.09774v1,"[arxiv.Result.Author('Brielen Madureira'), arxiv.Result.Author('David Schlangen')]",,2020-07-19 20:15:55+00:00,"A suitable state representation is a fundamental part of the learning process
in Reinforcement Learning. In various tasks, the state can either be described
by natural language or be natural language itself. This survey outlines the
strategies used in the literature to build natural language state
representations. We appeal for more linguistically interpretable and grounded
representations, careful justification of design decisions and evaluation of
the effectiveness of different approaches.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2007.09774v1
45,Natural Language Specifications in Proof Assistants,http://arxiv.org/pdf/2205.07811v1,"[arxiv.Result.Author('Colin S. Gordon'), arxiv.Result.Author('Sergey Matskevich')]",,2022-05-16 17:05:45+00:00,"Interactive proof assistants are computer programs carefully constructed to
check a human-designed proof of a mathematical claim with high confidence in
the implementation. However, this only validates truth of a formal claim, which
may have been mistranslated from a claim made in natural language. This is
especially problematic when using proof assistants to formally verify the
correctness of software with respect to a natural language specification. The
translation from informal to formal remains a challenging, time-consuming
process that is difficult to audit for correctness. This paper argues that it
is possible to build support for natural language specifications within
existing proof assistants, in a way that complements the principles used to
establish trust and auditability in proof assistants themselves.",,cs.PL,"['cs.PL', 'cs.CL']",http://arxiv.org/abs/2205.07811v1
46,Can Transformers Reason in Fragments of Natural Language?,http://arxiv.org/pdf/2211.05417v1,"[arxiv.Result.Author('Viktor Schlegel'), arxiv.Result.Author('Kamen V. Pavlov'), arxiv.Result.Author('Ian Pratt-Hartmann')]",,2022-11-10 08:46:53+00:00,"State-of-the-art deep-learning-based approaches to Natural Language
Processing (NLP) are credited with various capabilities that involve reasoning
with natural language texts. In this paper we carry out a large-scale empirical
study investigating the detection of formally valid inferences in controlled
fragments of natural language for which the satisfiability problem becomes
increasingly complex. We find that, while transformer-based language models
perform surprisingly well in these scenarios, a deeper analysis re-veals that
they appear to overfit to superficial patterns in the data rather than
acquiring the logical principles governing the reasoning in these fragments.",,cs.CL,"['cs.CL', 'cs.AI']",http://arxiv.org/abs/2211.05417v1
47,Putting Natural in Natural Language Processing,http://arxiv.org/pdf/2305.04572v2,[arxiv.Result.Author('Grzegorz Chrupała')],,2023-05-08 09:29:31+00:00,"Human language is firstly spoken and only secondarily written. Text, however,
is a very convenient and efficient representation of language, and modern
civilization has made it ubiquitous. Thus the field of NLP has overwhelmingly
focused on processing written rather than spoken language. Work on spoken
language, on the other hand, has been siloed off within the largely separate
speech processing community which has been inordinately preoccupied with
transcribing speech into text. Recent advances in deep learning have led to a
fortuitous convergence in methods between speech processing and mainstream NLP.
Arguably, the time is ripe for a unification of these two fields, and for
starting to take spoken language seriously as the primary mode of human
communication. Truly natural language processing could lead to better
integration with the rest of language science and could lead to systems which
are more data-efficient and more human-like, and which can communicate beyond
the textual modality.",,cs.CL,"['cs.CL', 'cs.AI', 'eess.AS']",http://arxiv.org/abs/2305.04572v2
48,NLTK: The Natural Language Toolkit,http://arxiv.org/pdf/cs/0205028v1,"[arxiv.Result.Author('Edward Loper'), arxiv.Result.Author('Steven Bird')]",,2002-05-17 12:51:00+00:00,"NLTK, the Natural Language Toolkit, is a suite of open source program
modules, tutorials and problem sets, providing ready-to-use computational
linguistics courseware. NLTK covers symbolic and statistical natural language
processing, and is interfaced to annotated corpora. Students augment and
replace existing components, learn structured programming by example, and
manipulate sophisticated models from the outset.",,cs.CL,"['cs.CL', 'D.2.6; I.2.7; J.5; K.3.2']",http://arxiv.org/abs/cs/0205028v1
49,Incrementalizing RASA's Open-Source Natural Language Understanding Pipeline,http://arxiv.org/pdf/1907.05403v1,"[arxiv.Result.Author('Andrew Rafla'), arxiv.Result.Author('Casey Kennington')]",,2019-07-11 17:35:20+00:00,"As spoken dialogue systems and chatbots are gaining more widespread adoption,
commercial and open-sourced services for natural language understanding are
emerging. In this paper, we explain how we altered the open-source RASA natural
language understanding pipeline to process incrementally (i.e., word-by-word),
following the incremental unit framework proposed by Schlangen and Skantze. To
do so, we altered existing RASA components to process incrementally, and added
an update-incremental intent recognition model as a component to RASA. Our
evaluations on the Snips dataset show that our changes allow RASA to function
as an effective incremental natural language understanding service.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1907.05403v1
50,How Commonsense Knowledge Helps with Natural Language Tasks: A Survey of Recent Resources and Methodologies,http://arxiv.org/pdf/2108.04674v1,"[arxiv.Result.Author('Yubo Xie'), arxiv.Result.Author('Pearl Pu')]",,2021-08-10 13:25:29+00:00,"In this paper, we give an overview of commonsense reasoning in natural
language processing, which requires a deeper understanding of the contexts and
usually involves inference over implicit external knowledge. We first review
some popular commonsense knowledge bases and commonsense reasoning benchmarks,
but give more emphasis on the methodologies, including recent approaches that
aim at solving some general natural language problems that take advantage of
external knowledge bases. Finally, we discuss some future directions in pushing
the boundary of commonsense reasoning in natural language processing.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2108.04674v1
51,Evaluating Computational Language Models with Scaling Properties of Natural Language,http://arxiv.org/pdf/1906.09379v1,"[arxiv.Result.Author('Shuntaro Takahashi'), arxiv.Result.Author('Kumiko Tanaka-Ishii')]",,2019-06-22 03:24:32+00:00,"In this article, we evaluate computational models of natural language with
respect to the universal statistical behaviors of natural language. Statistical
mechanical analyses have revealed that natural language text is characterized
by scaling properties, which quantify the global structure in the vocabulary
population and the long memory of a text. We study whether five scaling
properties (given by Zipf's law, Heaps' law, Ebeling's method, Taylor's law,
and long-range correlation analysis) can serve for evaluation of computational
models. Specifically, we test $n$-gram language models, a probabilistic
context-free grammar (PCFG), language models based on Simon/Pitman-Yor
processes, neural language models, and generative adversarial networks (GANs)
for text generation. Our analysis reveals that language models based on
recurrent neural networks (RNNs) with a gating mechanism (i.e., long short-term
memory, LSTM; a gated recurrent unit, GRU; and quasi-recurrent neural networks,
QRNNs) are the only computational models that can reproduce the long memory
behavior of natural language. Furthermore, through comparison with recently
proposed model-based evaluation methods, we find that the exponent of Taylor's
law is a good indicator of model quality.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1906.09379v1
52,Benchmarking Language Models for Code Syntax Understanding,http://arxiv.org/pdf/2210.14473v1,"[arxiv.Result.Author('Da Shen'), arxiv.Result.Author('Xinyun Chen'), arxiv.Result.Author('Chenguang Wang'), arxiv.Result.Author('Koushik Sen'), arxiv.Result.Author('Dawn Song')]",,2022-10-26 04:47:18+00:00,"Pre-trained language models have demonstrated impressive performance in both
natural language processing and program understanding, which represent the
input as a token sequence without explicitly modeling its structure. Some prior
works show that pre-trained language models can capture the syntactic rules of
natural languages without finetuning on syntax understanding tasks. However,
there is limited understanding of how well pre-trained models understand the
code structure so far. In this work, we perform the first thorough benchmarking
of the state-of-the-art pre-trained models for identifying the syntactic
structures of programs. Specifically, we introduce CodeSyntax, a large-scale
dataset of programs annotated with the syntactic relationships in their
corresponding abstract syntax trees. Our key observation is that existing
language models pretrained on code still lack the understanding of code syntax.
In fact, these pre-trained programming language models fail to match the
performance of simple baselines based on positional offsets and keywords. We
also present a natural language benchmark to highlight the differences between
natural languages and programming languages in terms of syntactic structure
understanding. Our findings point out key limitations of existing pre-training
methods for programming languages, and suggest the importance of modeling code
syntactic structures.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2210.14473v1
53,Chart-driven Connectionist Categorial Parsing of Spoken Korean,http://arxiv.org/pdf/cmp-lg/9511005v1,"[arxiv.Result.Author('WonIl Lee'), arxiv.Result.Author('Geunbae Lee'), arxiv.Result.Author('Jong-Hyeok Lee')]",,1995-11-29 05:19:16+00:00,"While most of the speech and natural language systems which were developed
for English and other Indo-European languages neglect the morphological
processing and integrate speech and natural language at the word level, for the
agglutinative languages such as Korean and Japanese, the morphological
processing plays a major role in the language processing since these languages
have very complex morphological phenomena and relatively simple syntactic
functionality. Obviously degenerated morphological processing limits the usable
vocabulary size for the system and word-level dictionary results in exponential
explosion in the number of dictionary entries. For the agglutinative languages,
we need sub-word level integration which leaves rooms for general morphological
processing. In this paper, we developed a phoneme-level integration model of
speech and linguistic processings through general morphological analysis for
agglutinative languages and a efficient parsing scheme for that integration.
Korean is modeled lexically based on the categorial grammar formalism with
unordered argument and suppressed category extensions, and chart-driven
connectionist parsing method is introduced.",,cmp-lg,"['cmp-lg', 'cs.CL']",http://arxiv.org/abs/cmp-lg/9511005v1
54,Using the DIFF Command for Natural Language Processing,http://arxiv.org/pdf/cs/0208020v1,"[arxiv.Result.Author('Masaki Murata'), arxiv.Result.Author('Hitoshi Isahara')]",,2002-08-13 03:39:20+00:00,"Diff is a software program that detects differences between two data sets and
is useful in natural language processing. This paper shows several examples of
the application of diff. They include the detection of differences between two
different datasets, extraction of rewriting rules, merging of two different
datasets, and the optimal matching of two different data sets. Since diff comes
with any standard UNIX system, it is readily available and very easy to use.
Our studies showed that diff is a practical tool for research into natural
language processing.",,cs.CL,"['cs.CL', 'H.3.3; I.2.7']",http://arxiv.org/abs/cs/0208020v1
55,"Implementation of nlization framework for verbs, pronouns and determiners with eugene",http://arxiv.org/pdf/1309.2471v1,"[arxiv.Result.Author('Harinder Singh'), arxiv.Result.Author('Parteek Kumar')]",,2013-09-10 12:03:32+00:00,"UNL system is designed and implemented by a nonprofit organization, UNDL
Foundation at Geneva in 1999. UNL applications are application softwares that
allow end users to accomplish natural language tasks, such as translating,
summarizing, retrieving or extracting information, etc. Two major web based
application softwares are Interactive ANalyzer (IAN), which is a natural
language analysis system. It represents natural language sentences as semantic
networks in the UNL format. Other application software is dEep-to-sUrface
GENErator (EUGENE), which is an open-source interactive NLizer. It generates
natural language sentences out of semantic networks represented in the UNL
format. In this paper, NLization framework with EUGENE is focused, while using
UNL system for accomplishing the task of machine translation. In whole
NLization process, EUGENE takes a UNL input and delivers an output in natural
language without any human intervention. It is language-independent and has to
be parametrized to the natural language input through a dictionary and a
grammar, provided as separate interpretable files. In this paper, it is
explained that how UNL input is syntactically and semantically analyzed with
the UNL-NL T-Grammar for NLization of UNL sentences involving verbs, pronouns
and determiners for Punjabi natural language.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1309.2471v1
56,SKOPE: A connectionist/symbolic architecture of spoken Korean processing,http://arxiv.org/pdf/cmp-lg/9504008v2,"[arxiv.Result.Author('Geunbae Lee'), arxiv.Result.Author('Jong-Hyeok Lee')]",,1995-04-07 14:39:09+00:00,"Spoken language processing requires speech and natural language integration.
Moreover, spoken Korean calls for unique processing methodology due to its
linguistic characteristics. This paper presents SKOPE, a connectionist/symbolic
spoken Korean processing engine, which emphasizes that: 1) connectionist and
symbolic techniques must be selectively applied according to their relative
strength and weakness, and 2) the linguistic characteristics of Korean must be
fully considered for phoneme recognition, speech and language integration, and
morphological/syntactic processing. The design and implementation of SKOPE
demonstrates how connectionist/symbolic hybrid architectures can be constructed
for spoken agglutinative language processing. Also SKOPE presents many novel
ideas for speech and language processing. The phoneme recognition,
morphological analysis, and syntactic analysis experiments show that SKOPE is a
viable approach for the spoken Korean processing.",,cmp-lg,"['cmp-lg', 'cs.CL']",http://arxiv.org/abs/cmp-lg/9504008v2
57,Fuzzy Temporal Protoforms for the Quantitative Description of Processes in Natural Language,http://arxiv.org/pdf/2305.09506v1,"[arxiv.Result.Author('Yago Fontenla-Seco'), arxiv.Result.Author('Alberto Bugarín-Diz'), arxiv.Result.Author('Manuel Lama')]",10.1109/FUZZ45933.2021.9494444,2023-05-16 14:59:38+00:00,"In this paper, we propose a series of fuzzy temporal protoforms in the
framework of the automatic generation of quantitative and qualitative natural
language descriptions of processes. The model includes temporal and causal
information from processes and attributes, quantifies attributes in time during
the process life-span and recalls causal relations and temporal distances
between events, among other features. Through integrating process mining
techniques and fuzzy sets within the usual Data-to-Text architecture, our
framework is able to extract relevant quantitative temporal as well as
structural information from a process and describe it in natural language
involving uncertain terms. A real use-case in the cardiology domain is
presented, showing the potential of our model for providing natural language
explanations addressed to domain experts.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2305.09506v1
58,Gap Analysis of Natural Language Processing Systems with respect to Linguistic Modality,http://arxiv.org/pdf/1504.04716v1,[arxiv.Result.Author('Vishal Shukla')],,2015-04-18 13:28:59+00:00,"Modality is one of the important components of grammar in linguistics. It
lets speaker to express attitude towards, or give assessment or potentiality of
state of affairs. It implies different senses and thus has different
perceptions as per the context. This paper presents an account showing the gap
in the functionality of the current state of art Natural Language Processing
(NLP) systems. The contextual nature of linguistic modality is studied. In this
paper, the works and logical approaches employed by Natural Language Processing
systems dealing with modality are reviewed. It sees human cognition and
intelligence as multi-layered approach that can be implemented by intelligent
systems for learning. Lastly, current flow of research going on within this
field is talked providing futurology.",,cs.CL,"['cs.CL', 'cs.AI']",http://arxiv.org/abs/1504.04716v1
59,Spontaneous Emerging Preference in Two-tower Language Model,http://arxiv.org/pdf/2210.07041v1,"[arxiv.Result.Author('Zhengqi He'), arxiv.Result.Author('Taro Toyoizumi')]",,2022-10-13 13:55:19+00:00,"The ever-growing size of the foundation language model has brought
significant performance gains in various types of downstream tasks. With the
existence of side-effects brought about by the large size of the foundation
language model such as deployment cost, availability issues, and environmental
cost, there is some interest in exploring other possible directions, such as a
divide-and-conquer scheme. In this paper, we are asking a basic question: are
language processes naturally dividable? We study this problem with a simple
two-tower language model setting, where two language models with identical
configurations are trained side-by-side cooperatively. With this setting, we
discover the spontaneous emerging preference phenomenon, where some of the
tokens are consistently better predicted by one tower while others by another
tower. This phenomenon is qualitatively stable, regardless of model
configuration and type, suggesting this as an intrinsic property of natural
language. This study suggests that interesting properties of natural language
are still waiting to be discovered, which may aid the future development of
natural language processing techniques.",,cs.CL,"['cs.CL', 'cs.AI']",http://arxiv.org/abs/2210.07041v1
60,Large Language Models on the Chessboard: A Study on ChatGPT's Formal Language Comprehension and Complex Reasoning Skills,http://arxiv.org/pdf/2308.15118v1,"[arxiv.Result.Author('Mu-Tien Kuo'), arxiv.Result.Author('Chih-Chung Hsueh'), arxiv.Result.Author('Richard Tzong-Han Tsai')]",,2023-08-29 08:36:30+00:00,"While large language models have made strides in natural language processing,
their proficiency in complex reasoning tasks requiring formal language
comprehension, such as chess, remains less investigated. This paper probes the
performance of ChatGPT, a sophisticated language model by OpenAI in tackling
such complex reasoning tasks, using chess as a case study. Through robust
metrics examining both the legality and quality of moves, we assess ChatGPT's
understanding of the chessboard, adherence to chess rules, and strategic
decision-making abilities. Our evaluation identifies limitations within
ChatGPT's attention mechanism that affect its formal language comprehension and
uncovers the model's underdeveloped self-regulation abilities. Our study also
reveals ChatGPT's propensity for a coherent strategy in its gameplay and a
noticeable uptick in decision-making assertiveness when the model is presented
with a greater volume of natural language or possesses a more lucid
understanding of the state of the chessboard. These findings contribute to the
growing exploration of language models' abilities beyond natural language
processing, providing valuable information for future research towards models
demonstrating human-like cognitive abilities.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2308.15118v1
61,Persian Natural Language Inference: A Meta-learning approach,http://arxiv.org/pdf/2205.08755v1,"[arxiv.Result.Author('Heydar Soudani'), arxiv.Result.Author('Mohammad Hassan Mojab'), arxiv.Result.Author('Hamid Beigy')]",,2022-05-18 06:51:58+00:00,"Incorporating information from other languages can improve the results of
tasks in low-resource languages. A powerful method of building functional
natural language processing systems for low-resource languages is to combine
multilingual pre-trained representations with cross-lingual transfer learning.
In general, however, shared representations are learned separately, either
across tasks or across languages. This paper proposes a meta-learning approach
for inferring natural language in Persian. Alternately, meta-learning uses
different task information (such as QA in Persian) or other language
information (such as natural language inference in English). Also, we
investigate the role of task augmentation strategy for forming additional
high-quality tasks. We evaluate the proposed method using four languages and an
auxiliary task. Compared to the baseline approach, the proposed model
consistently outperforms it, improving accuracy by roughly six percent. We also
examine the effect of finding appropriate initial parameters using zero-shot
evaluation and CCA similarity.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2205.08755v1
62,Linking Emergent and Natural Languages via Corpus Transfer,http://arxiv.org/pdf/2203.13344v1,"[arxiv.Result.Author('Shunyu Yao'), arxiv.Result.Author('Mo Yu'), arxiv.Result.Author('Yang Zhang'), arxiv.Result.Author('Karthik R Narasimhan'), arxiv.Result.Author('Joshua B. Tenenbaum'), arxiv.Result.Author('Chuang Gan')]",,2022-03-24 21:24:54+00:00,"The study of language emergence aims to understand how human languages are
shaped by perceptual grounding and communicative intent. Computational
approaches to emergent communication (EC) predominantly consider referential
games in limited domains and analyze the learned protocol within the game
framework. As a result, it remains unclear how the emergent languages from
these settings connect to natural languages or provide benefits in real-world
language processing tasks, where statistical models trained on large text
corpora dominate. In this work, we propose a novel way to establish such a link
by corpus transfer, i.e. pretraining on a corpus of emergent language for
downstream natural language tasks, which is in contrast to prior work that
directly transfers speaker and listener parameters. Our approach showcases
non-trivial transfer benefits for two different tasks -- language modeling and
image captioning. For example, in a low-resource setup (modeling 2 million
natural language tokens), pre-training on an emergent language corpus with just
2 million tokens reduces model perplexity by $24.6\%$ on average across ten
natural languages. We also introduce a novel metric to predict the
transferability of an emergent language by translating emergent messages to
natural language captions grounded on the same images. We find that our
translation-based metric highly correlates with the downstream performance on
modeling natural languages (for instance $\rho=0.83$ on Hebrew), while
topographic similarity, a popular metric in previous work, shows surprisingly
low correlation ($\rho=0.003$), hinting that simple properties like attribute
disentanglement from synthetic domains might not capture the full complexities
of natural language. Our findings also indicate potential benefits of moving
language emergence forward with natural language resources and models.",,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2203.13344v1
63,Language understanding as a step towards human level intelligence - automatizing the construction of the initial dictionary from example sentences,http://arxiv.org/pdf/1108.3848v1,"[arxiv.Result.Author('Chitta Baral'), arxiv.Result.Author('Juraj Dzifcak')]",,2011-08-18 20:12:50+00:00,"For a system to understand natural language, it needs to be able to take
natural language text and answer questions given in natural language with
respect to that text; it also needs to be able to follow instructions given in
natural language. To achieve this, a system must be able to process natural
language and be able to capture the knowledge within that text. Thus it needs
to be able to translate natural language text into a formal language. We
discuss our approach to do this, where the translation is achieved by composing
the meaning of words in a sentence. Our initial approach uses an inverse lambda
method that we developed (and other methods) to learn meaning of words from
meaning of sentences and an initial lexicon. We then present an improved method
where the initial lexicon is also learned by analyzing the training sentence
and meaning pairs. We evaluate our methods and compare them with other existing
methods on a corpora of database querying and robot command and control.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1108.3848v1
64,Architecture of an Ontology-Based Domain-Specific Natural Language Question Answering System,http://arxiv.org/pdf/1311.3175v1,"[arxiv.Result.Author('Athira P. M.'), arxiv.Result.Author('Sreeja M.'), arxiv.Result.Author('P. C. Reghu Raj')]",,2013-11-13 15:36:12+00:00,"Question answering (QA) system aims at retrieving precise information from a
large collection of documents against a query. This paper describes the
architecture of a Natural Language Question Answering (NLQA) system for a
specific domain based on the ontological information, a step towards semantic
web question answering. The proposed architecture defines four basic modules
suitable for enhancing current QA capabilities with the ability of processing
complex questions. The first module was the question processing, which analyses
and classifies the question and also reformulates the user query. The second
module allows the process of retrieving the relevant documents. The next module
processes the retrieved documents, and the last module performs the extraction
and generation of a response. Natural language processing techniques are used
for processing the question and documents and also for answer extraction.
Ontology and domain knowledge are used for reformulating queries and
identifying the relations. The aim of the system is to generate short and
specific answer to the question that is asked in the natural language in a
specific domain. We have achieved 94 % accuracy of natural language question
answering in our implementation.","International Journal of Web & Semantic Technology (IJWesT) Vol.4,
  No.4, October 2013",cs.CL,"['cs.CL', 'cs.IR']",http://arxiv.org/abs/1311.3175v1
65,Defining and Evaluating Fair Natural Language Generation,http://arxiv.org/pdf/2008.01548v1,"[arxiv.Result.Author('Catherine Yeo'), arxiv.Result.Author('Alyssa Chen')]",,2020-07-28 04:11:10+00:00,"Our work focuses on the biases that emerge in the natural language generation
(NLG) task of sentence completion. In this paper, we introduce a framework of
fairness for NLG followed by an evaluation of gender biases in two
state-of-the-art language models. Our analysis provides a theoretical
formulation for biases in NLG and empirical evidence that existing language
generation models embed gender bias.",,cs.CL,"['cs.CL', 'cs.LG']",http://arxiv.org/abs/2008.01548v1
66,Large Language Models are not Models of Natural Language: they are Corpus Models,http://arxiv.org/pdf/2112.07055v2,[arxiv.Result.Author('Csaba Veres')],,2021-12-13 22:39:46+00:00,"Natural Language Processing (NLP) has become one of the leading application
areas in the current Artificial Intelligence boom. Transfer learning has
enabled large deep learning neural networks trained on the language modeling
task to vastly improve performance in almost all downstream language tasks.
Interestingly, when the language models are trained with data that includes
software code, they demonstrate remarkable abilities in generating functioning
computer code from natural language specifications. We argue that this creates
a conundrum for the claim that eliminative neural models are a radical
restructuring in our understanding of cognition in that they eliminate the need
for symbolic abstractions like generative phrase structure grammars. Because
the syntax of programming languages is by design determined by phrase structure
grammars, neural models that produce syntactic code are apparently
uninformative about the theoretical foundations of programming languages. The
demonstration that neural models perform well on tasks that involve clearly
symbolic systems, proves that they cannot be used as an argument that language
and other cognitive systems are not symbolic. Finally, we argue as a corollary
that the term language model is misleading and propose the adoption of the
working term corpus model instead, which better reflects the genesis and
contents of the model.",,cs.CL,"['cs.CL', 'cs.LG']",http://arxiv.org/abs/2112.07055v2
67,Stopwords in Technical Language Processing,http://arxiv.org/pdf/2006.02633v1,"[arxiv.Result.Author('Serhad Sarica'), arxiv.Result.Author('Jianxi Luo')]",10.1371/journal.pone.0254937,2020-06-04 03:52:59+00:00,"There are increasingly applications of natural language processing techniques
for information retrieval, indexing and topic modelling in the engineering
contexts. A standard component of such tasks is the removal of stopwords, which
are uninformative components of the data. While researchers use readily
available stopword lists which are derived for general English language, the
technical jargon of engineering fields contains their own highly frequent and
uninformative words and there exists no standard stopword list for technical
language processing applications. Here we address this gap by rigorously
identifying generic, insignificant, uninformative stopwords in engineering
texts beyond the stopwords in general texts, based on the synthesis of
alternative data-driven approaches, and curating a stopword list ready for
technical language processing applications.",,cs.IR,"['cs.IR', 'cs.CL']",http://arxiv.org/abs/2006.02633v1
68,Teaching natural language to computers,http://arxiv.org/pdf/1604.08781v2,"[arxiv.Result.Author('Joseph Corneli'), arxiv.Result.Author('Miriam Corneli')]",,2016-04-29 11:36:25+00:00,"""Natural Language,"" whether spoken and attended to by humans, or processed
and generated by computers, requires networked structures that reflect creative
processes in semantic, syntactic, phonetic, linguistic, social, emotional, and
cultural modules. Being able to produce novel and useful behavior following
repeated practice gets to the root of both artificial intelligence and human
language. This paper investigates the modalities involved in language-like
applications that computers -- and programmers -- engage with, and aims to fine
tune the questions we ask to better account for context, self-awareness, and
embodiment.",,cs.CL,"['cs.CL', 'cs.AI', 'H.5.2; D.1.2; J.5']",http://arxiv.org/abs/1604.08781v2
69,A State of the Art of Word Sense Induction: A Way Towards Word Sense Disambiguation for Under-Resourced Languages,http://arxiv.org/pdf/1310.1425v1,[arxiv.Result.Author('Mohammad Nasiruddin')],,2013-10-05 00:33:46+00:00,"Word Sense Disambiguation (WSD), the process of automatically identifying the
meaning of a polysemous word in a sentence, is a fundamental task in Natural
Language Processing (NLP). Progress in this approach to WSD opens up many
promising developments in the field of NLP and its applications. Indeed,
improvement over current performance levels could allow us to take a first step
towards natural language understanding. Due to the lack of lexical resources it
is sometimes difficult to perform WSD for under-resourced languages. This paper
is an investigation on how to initiate research in WSD for under-resourced
languages by applying Word Sense Induction (WSI) and suggests some interesting
topics to focus on.",,cs.CL,"['cs.CL', '68T50', 'I.2.7']",http://arxiv.org/abs/1310.1425v1
70,Racial Disparity in Natural Language Processing: A Case Study of Social Media African-American English,http://arxiv.org/pdf/1707.00061v1,"[arxiv.Result.Author('Su Lin Blodgett'), arxiv.Result.Author(""Brendan O'Connor"")]",,2017-06-30 22:57:50+00:00,"We highlight an important frontier in algorithmic fairness: disparity in the
quality of natural language processing algorithms when applied to language from
authors of different social groups. For example, current systems sometimes
analyze the language of females and minorities more poorly than they do of
whites and males. We conduct an empirical analysis of racial disparity in
language identification for tweets written in African-American English, and
discuss implications of disparity in NLP.",,cs.CY,"['cs.CY', 'cs.CL']",http://arxiv.org/abs/1707.00061v1
71,Automatic Code Generation using Pre-Trained Language Models,http://arxiv.org/pdf/2102.10535v1,"[arxiv.Result.Author('Luis Perez'), arxiv.Result.Author('Lizi Ottens'), arxiv.Result.Author('Sudharshan Viswanathan')]",,2021-02-21 07:21:26+00:00,"Recent advancements in natural language processing \cite{gpt2} \cite{BERT}
have led to near-human performance in multiple natural language tasks. In this
paper, we seek to understand whether similar techniques can be applied to a
highly structured environment with strict syntax rules. Specifically, we
propose an end-to-end machine learning model for code generation in the Python
language built on-top of pre-trained language models. We demonstrate that a
fine-tuned model can perform well in code generation tasks, achieving a BLEU
score of 0.22, an improvement of 46\% over a reasonable sequence-to-sequence
baseline. All results and related code used for training and data processing
are available on GitHub.",,cs.CL,"['cs.CL', 'cs.LG']",http://arxiv.org/abs/2102.10535v1
72,Are Style Guides Controlled Languages? The Case of Koenig & Bauer AG,http://arxiv.org/pdf/1406.3460v1,[arxiv.Result.Author('Karolina Suchowolec')],,2014-06-13 09:23:53+00:00,"Controlled natural languages for industrial application are often regarded as
a response to the challenges of translation and multilingual communication.
This paper presents a quite different approach taken by Koenig & Bauer AG,
where the main goal was the improvement of the authoring process for technical
documentation. Most importantly, this paper explores the notion of a controlled
language and demonstrates how style guides can emerge from non-linguistic
considerations. Moreover, it shows the transition from loose language
recommendations into precise and prescriptive rules and investigates whether
such rules can be regarded as a full-fledged controlled language.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1406.3460v1
73,Embedded Controlled Languages,http://arxiv.org/pdf/1406.4057v1,[arxiv.Result.Author('Aarne Ranta')],,2014-06-16 16:11:32+00:00,"Inspired by embedded programming languages, an embedded CNL (controlled
natural language) is a proper fragment of an entire natural language (its host
language), but it has a parser that recognizes the entire host language. This
makes it possible to process out-of-CNL input and give useful feedback to
users, instead of just reporting syntax errors. This extended abstract explains
the main concepts of embedded CNL implementation in GF (Grammatical Framework),
with examples from machine translation and some other ongoing work.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1406.4057v1
74,Syntax Representation in Word Embeddings and Neural Networks -- A Survey,http://arxiv.org/pdf/2010.01063v1,"[arxiv.Result.Author('Tomasz Limisiewicz'), arxiv.Result.Author('David Mareček')]",,2020-10-02 15:44:58+00:00,"Neural networks trained on natural language processing tasks capture syntax
even though it is not provided as a supervision signal. This indicates that
syntactic analysis is essential to the understating of language in artificial
intelligence systems. This overview paper covers approaches of evaluating the
amount of syntactic information included in the representations of words for
different neural network architectures. We mainly summarize re-search on
English monolingual data on language modeling tasks and multilingual data for
neural machine translation systems and multilingual language models. We
describe which pre-trained models and representations of language are best
suited for transfer to syntactic tasks.","Proceedings of the 20th Conference ITAT 2020: Automata, Formal and
  Natural Languages Workshop",cs.CL,['cs.CL'],http://arxiv.org/abs/2010.01063v1
75,Constraint Logic Programming for Natural Language Processing,http://arxiv.org/pdf/cmp-lg/9504005v1,"[arxiv.Result.Author('Philippe Blache'), arxiv.Result.Author('Nabil Hathout')]",,1995-04-05 14:16:33+00:00,"This paper proposes an evaluation of the adequacy of the constraint logic
programming paradigm for natural language processing. Theoretical aspects of
this question have been discussed in several works. We adopt here a pragmatic
point of view and our argumentation relies on concrete solutions. Using actual
contraints (in the CLP sense) is neither easy nor direct. However, CLP can
improve parsing techniques in several aspects such as concision, control,
efficiency or direct representation of linguistic formalism. This discussion is
illustrated by several examples and the presentation of an HPSG parser.",,cmp-lg,"['cmp-lg', 'cs.CL']",http://arxiv.org/abs/cmp-lg/9504005v1
76,Architecture of a Web-based Predictive Editor for Controlled Natural Language Processing,http://arxiv.org/pdf/1408.0016v1,"[arxiv.Result.Author('Stephen Guy'), arxiv.Result.Author('Rolf Schwitter')]",,2014-06-27 01:00:59+00:00,"In this paper, we describe the architecture of a web-based predictive text
editor being developed for the controlled natural language PENG$^{ASP)$. This
controlled language can be used to write non-monotonic specifications that have
the same expressive power as Answer Set Programs. In order to support the
writing process of these specifications, the predictive text editor
communicates asynchronously with the controlled natural language processor that
generates lookahead categories and additional auxiliary information for the
author of a specification text. The text editor can display multiple sets of
lookahead categories simultaneously for different possible sentence
completions, anaphoric expressions, and supports the addition of new content
words to the lexicon.",,cs.CL,"['cs.CL', 'cs.AI']",http://arxiv.org/abs/1408.0016v1
77,"Natural Language Processing: State of The Art, Current Trends and Challenges",http://arxiv.org/pdf/1708.05148v1,"[arxiv.Result.Author('Diksha Khurana'), arxiv.Result.Author('Aditya Koli'), arxiv.Result.Author('Kiran Khatter'), arxiv.Result.Author('Sukhdev Singh')]",10.1007/s11042-022-13428-4,2017-08-17 06:42:03+00:00,"Natural language processing (NLP) has recently gained much attention for
representing and analysing human language computationally. It has spread its
applications in various fields such as machine translation, email spam
detection, information extraction, summarization, medical, and question
answering etc. The paper distinguishes four phases by discussing different
levels of NLP and components of Natural Language Generation (NLG) followed by
presenting the history and evolution of NLP, state of the art presenting the
various applications of NLP and current trends and challenges.",Multimed Tools Appl (2022),cs.CL,['cs.CL'],http://arxiv.org/abs/1708.05148v1
78,An OLAC Extension for Dravidian Languages,http://arxiv.org/pdf/0908.4431v1,[arxiv.Result.Author('B Prabhulla Chandran Pillai')],,2009-08-30 23:20:41+00:00,"OLAC was founded in 2000 for creating online databases of language resources.
This paper intends to review the bottom-up distributed character of the project
and proposes an extension of the architecture for Dravidian languages. An
ontological structure is considered for effective natural language processing
(NLP) and its advantages over statistical methods are reviewed",,cs.CL,['cs.CL'],http://arxiv.org/abs/0908.4431v1
79,Natural Language Semantics and Computability,http://arxiv.org/pdf/1605.04122v1,"[arxiv.Result.Author('Richard Moot'), arxiv.Result.Author('Christian Retoré')]",,2016-05-13 10:46:22+00:00,"This paper is a reflexion on the computability of natural language semantics.
It does not contain a new model or new results in the formal semantics of
natural language: it is rather a computational analysis of the logical models
and algorithms currently used in natural language semantics, defined as the
mapping of a statement to logical formulas - formulas, because a statement can
be ambiguous. We argue that as long as possible world semantics is left out,
one can compute the semantic representation(s) of a given statement, including
aspects of lexical meaning. We also discuss the algorithmic complexity of this
process.",,cs.CL,"['cs.CL', 'cs.AI', 'cs.CC']",http://arxiv.org/abs/1605.04122v1
80,"Logical Semantics, Dialogical Argumentation, and Textual Entailment",http://arxiv.org/pdf/2008.07138v1,"[arxiv.Result.Author('Davide Catta'), arxiv.Result.Author('Richard Moot'), arxiv.Result.Author('Christian Retoré')]",,2020-08-17 08:04:11+00:00,"In this chapter, we introduce a new dialogical system for first order
classical logic which is close to natural language argumentation, and we prove
its completeness with respect to usual classical validity. We combine our
dialogical system with the Grail syntactic and semantic parser developed by the
second author in order to address automated textual entailment, that is, we use
it for deciding whether or not a sentence is a consequence of a short text.
This work-which connects natural language semantics and argumentation with
dialogical logic-can be viewed as a step towards an inferentialist view of
natural language semantics.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2008.07138v1
81,A perspective on the advancement of natural language processing tasks via topological analysis of complex networks,http://arxiv.org/pdf/1412.1342v1,[arxiv.Result.Author('Diego R. Amancio')],10.1016/j.plrev.2014.07.010,2014-12-03 14:37:36+00:00,"Comment on ""Approaching human language with complex networks"" by Cong and Liu
(Physics of Life Reviews, Volume 11, Issue 4, December 2014, Pages 598-618).","Physics of Life Reviews, v. 11, p. 641-643, 2014",cs.CL,['cs.CL'],http://arxiv.org/abs/1412.1342v1
82,Automatic Language Identification for Romance Languages using Stop Words and Diacritics,http://arxiv.org/pdf/1806.05480v1,"[arxiv.Result.Author('Ciprian-Octavian Truică'), arxiv.Result.Author('Julien Velcin'), arxiv.Result.Author('Alexandru Boicea')]",10.1109/SYNASC.2015.45,2018-06-14 11:38:24+00:00,"Automatic language identification is a natural language processing problem
that tries to determine the natural language of a given content. In this paper
we present a statistical method for automatic language identification of
written text using dictionaries containing stop words and diacritics. We
propose different approaches that combine the two dictionaries to accurately
determine the language of textual corpora. This method was chosen because stop
words and diacritics are very specific to a language, although some languages
have some similar words and special characters they are not all common. The
languages taken into account were romance languages because they are very
similar and usually it is hard to distinguish between them from a computational
point of view. We have tested our method using a Twitter corpus and a news
article corpus. Both corpora consists of UTF-8 encoded text, so the diacritics
could be taken into account, in the case that the text has no diacritics only
the stop words are used to determine the language of the text. The experimental
results show that the proposed method has an accuracy of over 90% for small
texts and over 99.8% for",,cs.CL,"['cs.CL', 'cs.IR']",http://arxiv.org/abs/1806.05480v1
83,Language Segmentation,http://arxiv.org/pdf/1510.01717v1,[arxiv.Result.Author('David Alfter')],,2015-10-06 19:35:23+00:00,"Language segmentation consists in finding the boundaries where one language
ends and another language begins in a text written in more than one language.
This is important for all natural language processing tasks. The problem can be
solved by training language models on language data. However, in the case of
low- or no-resource languages, this is problematic. I therefore investigate
whether unsupervised methods perform better than supervised methods when it is
difficult or impossible to train supervised approaches. A special focus is
given to difficult texts, i.e. texts that are rather short (one sentence),
containing abbreviations, low-resource languages and non-standard language. I
compare three approaches: supervised n-gram language models, unsupervised
clustering and weakly supervised n-gram language model induction. I devised the
weakly supervised approach in order to deal with difficult text specifically.
In order to test the approach, I compiled a small corpus of different text
types, ranging from one-sentence texts to texts of about 300 words. The weakly
supervised language model induction approach works well on short and difficult
texts, outperforming the clustering algorithm and reaching scores in the
vicinity of the supervised approach. The results look promising, but there is
room for improvement and a more thorough investigation should be undertaken.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1510.01717v1
84,Machine Translation between Spoken Languages and Signed Languages Represented in SignWriting,http://arxiv.org/pdf/2210.05404v2,"[arxiv.Result.Author('Zifan Jiang'), arxiv.Result.Author('Amit Moryossef'), arxiv.Result.Author('Mathias Müller'), arxiv.Result.Author('Sarah Ebling')]",,2022-10-11 12:28:06+00:00,"This paper presents work on novel machine translation (MT) systems between
spoken and signed languages, where signed languages are represented in
SignWriting, a sign language writing system. Our work seeks to address the lack
of out-of-the-box support for signed languages in current MT systems and is
based on the SignBank dataset, which contains pairs of spoken language text and
SignWriting content. We introduce novel methods to parse, factorize, decode,
and evaluate SignWriting, leveraging ideas from neural factored MT. In a
bilingual setup--translating from American Sign Language to (American)
English--our method achieves over 30 BLEU, while in two multilingual
setups--translating in both directions between spoken languages and signed
languages--we achieve over 20 BLEU. We find that common MT techniques used to
improve spoken language translation similarly affect the performance of sign
language translation. These findings validate our use of an intermediate text
representation for signed languages to include them in natural language
processing research.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2210.05404v2
85,How Good are Commercial Large Language Models on African Languages?,http://arxiv.org/pdf/2305.06530v1,"[arxiv.Result.Author('Jessica Ojo'), arxiv.Result.Author('Kelechi Ogueji')]",,2023-05-11 02:29:53+00:00,"Recent advancements in Natural Language Processing (NLP) has led to the
proliferation of large pretrained language models. These models have been shown
to yield good performance, using in-context learning, even on unseen tasks and
languages. They have also been exposed as commercial APIs as a form of
language-model-as-a-service, with great adoption. However, their performance on
African languages is largely unknown. We present a preliminary analysis of
commercial large language models on two tasks (machine translation and text
classification) across eight African languages, spanning different language
families and geographical areas. Our results suggest that commercial language
models produce below-par performance on African languages. We also find that
they perform better on text classification than machine translation. In
general, our findings present a call-to-action to ensure African languages are
well represented in commercial large language models, given their growing
popularity.",,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2305.06530v1
86,Development of Word Embeddings for Uzbek Language,http://arxiv.org/pdf/2009.14384v1,"[arxiv.Result.Author('B. Mansurov'), arxiv.Result.Author('A. Mansurov')]",,2020-09-30 01:52:00+00:00,"In this paper, we share the process of developing word embeddings for the
Cyrillic variant of the Uzbek language. The result of our work is the first
publicly available set of word vectors trained on the word2vec, GloVe, and
fastText algorithms using a high-quality web crawl corpus developed in-house.
The developed word embeddings can be used in many natural language processing
downstream tasks.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2009.14384v1
87,Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery,http://arxiv.org/pdf/2002.06053v1,"[arxiv.Result.Author('Hakime Öztürk'), arxiv.Result.Author('Arzucan Özgür'), arxiv.Result.Author('Philippe Schwaller'), arxiv.Result.Author('Teodoro Laino'), arxiv.Result.Author('Elif Ozkirimli')]",10.1016/j.drudis.2020.01.020,2020-02-10 21:02:05+00:00,"Text-based representations of chemicals and proteins can be thought of as
unstructured languages codified by humans to describe domain-specific
knowledge. Advances in natural language processing (NLP) methodologies in the
processing of spoken languages accelerated the application of NLP to elucidate
hidden knowledge in textual representations of these biochemical entities and
then use it to construct models to predict molecular properties or to design
novel molecules. This review outlines the impact made by these advances on drug
discovery and aims to further the dialogue between medicinal chemists and
computer scientists.",,q-bio.BM,"['q-bio.BM', 'cs.CL', 'cs.LG', 'stat.ML']",http://arxiv.org/abs/2002.06053v1
88,Natural Language Processing Chains Inside a Cross-lingual Event-Centric Knowledge Pipeline for European Union Under-resourced Languages,http://arxiv.org/pdf/2010.12433v1,"[arxiv.Result.Author('Diego Alves'), arxiv.Result.Author('Gaurish Thakkar'), arxiv.Result.Author('Marko Tadić')]",,2020-10-23 14:26:30+00:00,"This article presents the strategy for developing a platform containing
Language Processing Chains for European Union languages, consisting of
Tokenization to Parsing, also including Named Entity recognition andwith
addition ofSentiment Analysis. These chains are part of the first step of an
event-centric knowledge processing pipeline whose aim is to process
multilingual media information about major events that can cause an impactin
Europe and the rest of the world. Due to the differences in terms of
availability of language resources for each language, we have built this
strategy in three steps, starting with processing chains for the well-resourced
languages and finishing with the development of new modules for the
under-resourced ones. In order to classify all European Union official
languages in terms of resources, we have analysed the size of annotated corpora
as well as the existence of pre-trained models in mainstream Language
Processing tools, and we have combined this information with the proposed
classification published at META-NETwhitepaper series.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2010.12433v1
89,Parsing of part-of-speech tagged Assamese Texts,http://arxiv.org/pdf/0912.1820v1,"[arxiv.Result.Author('Mirzanur Rahman'), arxiv.Result.Author('Sufal Das'), arxiv.Result.Author('Utpal Sharma')]",,2009-12-09 18:03:20+00:00,"A natural language (or ordinary language) is a language that is spoken,
written, or signed by humans for general-purpose communication, as
distinguished from formal languages (such as computer-programming languages or
the ""languages"" used in the study of formal logic). The computational
activities required for enabling a computer to carry out information processing
using natural language is called natural language processing. We have taken
Assamese language to check the grammars of the input sentence. Our aim is to
produce a technique to check the grammatical structures of the sentences in
Assamese text. We have made grammar rules by analyzing the structures of
Assamese sentences. Our parsing program finds the grammatical errors, if any,
in the Assamese sentence. If there is no error, the program will generate the
parse tree for the Assamese sentence","M. Rahman, S. Das and U. Sharma, ""Parsing of part-of-speech tagged
  Assamese Texts"", International Journal of Computer Science Issues, IJCSI,
  Volume 6, Issue 1, pp28-34, November 2009",cs.CL,['cs.CL'],http://arxiv.org/abs/0912.1820v1
90,Statistical patterns of word frequency suggesting the probabilistic nature of human languages,http://arxiv.org/pdf/2012.00187v1,"[arxiv.Result.Author('Shuiyuan Yu'), arxiv.Result.Author('Chunshan Xu'), arxiv.Result.Author('Haitao Liu')]",,2020-12-01 00:48:27+00:00,"Traditional linguistic theories have largely regard language as a formal
system composed of rigid rules. However, their failures in processing real
language, the recent successes in statistical natural language processing, and
the findings of many psychological experiments have suggested that language may
be more a probabilistic system than a formal system, and thus cannot be
faithfully modeled with the either/or rules of formal linguistic theory. The
present study, based on authentic language data, confirmed that those important
linguistic issues, such as linguistic universal, diachronic drift, and language
variations can be translated into probability and frequency patterns in parole.
These findings suggest that human language may well be probabilistic systems by
nature, and that statistical may well make inherent properties of human
languages.",,cs.CL,"['cs.CL', 'physics.comp-ph']",http://arxiv.org/abs/2012.00187v1
91,Contextualising Levels of Language Resourcedness affecting Digital Processing of Text,http://arxiv.org/pdf/2309.17035v1,"[arxiv.Result.Author('C. Maria Keet'), arxiv.Result.Author('Langa Khumalo')]",,2023-09-29 07:48:24+00:00,"Application domains such as digital humanities and tool like chatbots involve
some form of processing natural language, from digitising hardcopies to speech
generation. The language of the content is typically characterised as either a
low resource language (LRL) or high resource language (HRL), also known as
resource-scarce and well-resourced languages, respectively. African languages
have been characterized as resource-scarce languages (Bosch et al. 2007;
Pretorius & Bosch 2003; Keet & Khumalo 2014) and English is by far the most
well-resourced language. Varied language resources are used to develop software
systems for these languages to accomplish a wide range of tasks. In this paper
we argue that the dichotomous typology LRL and HRL for all languages is
problematic. Through a clear understanding of language resources situated in a
society, a matrix is developed that characterizes languages as Very LRL, LRL,
RL, HRL and Very HRL. The characterization is based on the typology of
contextual features for each category, rather than counting tools, and
motivation is provided for each feature and each characterization. The
contextualisation of resourcedness, with a focus on African languages in this
paper, and an increased understanding of where on the scale the language used
in a project is, may assist in, among others, better planning of research and
implementation projects. We thus argue in this paper that the characterization
of language resources within a given scale in a project is an indispensable
component particularly in the context of low-resourced languages.",,cs.CL,"['cs.CL', 'I.2.7']",http://arxiv.org/abs/2309.17035v1
92,Novel Keyword Extraction and Language Detection Approaches,http://arxiv.org/pdf/2009.11832v1,"[arxiv.Result.Author('Malgorzata Pikies'), arxiv.Result.Author('Andronicus Riyono'), arxiv.Result.Author('Junade Ali')]",,2020-09-24 17:28:59+00:00,"Fuzzy string matching and language classification are important tools in
Natural Language Processing pipelines, this paper provides advances in both
areas. We propose a fast novel approach to string tokenisation for fuzzy
language matching and experimentally demonstrate an 83.6% decrease in
processing time with an estimated improvement in recall of 3.1% at the cost of
a 2.6% decrease in precision. This approach is able to work even where keywords
are subdivided into multiple words, without needing to scan
character-to-character. So far there has been little work considering using
metadata to enhance language classification algorithms. We provide
observational data and find the Accept-Language header is 14% more likely to
match the classification than the IP Address.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2009.11832v1
93,Investigating Masking-based Data Generation in Language Models,http://arxiv.org/pdf/2307.00008v1,[arxiv.Result.Author('Ed S. Ma')],,2023-06-16 16:48:27+00:00,"The current era of natural language processing (NLP) has been defined by the
prominence of pre-trained language models since the advent of BERT. A feature
of BERT and models with similar architecture is the objective of masked
language modeling, in which part of the input is intentionally masked and the
model is trained to predict this piece of masked information. Data augmentation
is a data-driven technique widely used in machine learning, including research
areas like computer vision and natural language processing, to improve model
performance by artificially augmenting the training data set by designated
techniques. Masked language models (MLM), an essential training feature of
BERT, have introduced a novel approach to perform effective pre-training on
Transformer based models in natural language processing tasks. Recent studies
have utilized masked language model to generate artificially augmented data for
NLP downstream tasks. The experimental results show that Mask based data
augmentation method provides a simple but efficient approach to improve the
model performance. In this paper, we explore and discuss the broader
utilization of these data augmentation methods based on MLM.",,cs.CL,"['cs.CL', 'cs.AI']",http://arxiv.org/abs/2307.00008v1
94,Understanding scholarly Natural Language Processing system diagrams through application of the Richards-Engelhardt framework,http://arxiv.org/pdf/2008.11785v1,"[arxiv.Result.Author('Guy Clarke Marshall'), arxiv.Result.Author('Caroline Jay'), arxiv.Result.Author('André Freitas')]",,2020-08-26 20:06:30+00:00,"We utilise Richards-Engelhardt framework as a tool for understanding Natural
Language Processing systems diagrams. Through four examples from scholarly
proceedings, we find that the application of the framework to this ecological
and complex domain is effective for reflecting on these diagrams. We argue for
vocabulary to describe multiple-codings, semiotic variability, and
inconsistency or misuse of visual encoding principles in diagrams. Further, for
application to scholarly Natural Language Processing systems, and perhaps
systems diagrams more broadly, we propose the addition of ""Grouping by Object""
as a new visual encoding principle, and ""Emphasising"" as a new visual encoding
type.",,cs.HC,"['cs.HC', 'cs.AI', 'cs.CL']",http://arxiv.org/abs/2008.11785v1
95,Indian Legal NLP Benchmarks : A Survey,http://arxiv.org/pdf/2107.06056v1,"[arxiv.Result.Author('Prathamesh Kalamkar'), arxiv.Result.Author('Janani Venugopalan Ph. D.'), arxiv.Result.Author('Vivek Raghavan Ph. D')]",,2021-07-13 13:10:10+00:00,"Availability of challenging benchmarks is the key to advancement of AI in a
specific field.Since Legal Text is significantly different than normal English
text, there is a need to create separate Natural Language Processing benchmarks
for Indian Legal Text which are challenging and focus on tasks specific to
Legal Systems. This will spur innovation in applications of Natural language
Processing for Indian Legal Text and will benefit AI community and Legal
fraternity. We review the existing work in this area and propose ideas to
create new benchmarks for Indian Legal Natural Language Processing.",,cs.CL,"['cs.CL', 'cs.AI']",http://arxiv.org/abs/2107.06056v1
96,Hierarchical Representation in Neural Language Models: Suppression and Recovery of Expectations,http://arxiv.org/pdf/1906.04068v1,"[arxiv.Result.Author('Ethan Wilcox'), arxiv.Result.Author('Roger Levy'), arxiv.Result.Author('Richard Futrell')]",,2019-06-10 15:20:32+00:00,"Deep learning sequence models have led to a marked increase in performance
for a range of Natural Language Processing tasks, but it remains an open
question whether they are able to induce proper hierarchical generalizations
for representing natural language from linear input alone. Work using
artificial languages as training input has shown that LSTMs are capable of
inducing the stack-like data structures required to represent context-free and
certain mildly context-sensitive languages---formal language classes which
correspond in theory to the hierarchical structures of natural language. Here
we present a suite of experiments probing whether neural language models
trained on linguistic data induce these stack-like data structures and deploy
them while incrementally predicting words. We study two natural language
phenomena: center embedding sentences and syntactic island constraints on the
filler--gap dependency. In order to properly predict words in these structures,
a model must be able to temporarily suppress certain expectations and then
recover those expectations later, essentially pushing and popping these
expectations on a stack. Our results provide evidence that models can
successfully suppress and recover expectations in many cases, but do not fully
recover their previous grammatical state.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1906.04068v1
97,OCR Post Correction for Endangered Language Texts,http://arxiv.org/pdf/2011.05402v1,"[arxiv.Result.Author('Shruti Rijhwani'), arxiv.Result.Author('Antonios Anastasopoulos'), arxiv.Result.Author('Graham Neubig')]",,2020-11-10 21:21:08+00:00,"There is little to no data available to build natural language processing
models for most endangered languages. However, textual data in these languages
often exists in formats that are not machine-readable, such as paper books and
scanned images. In this work, we address the task of extracting text from these
resources. We create a benchmark dataset of transcriptions for scanned books in
three critically endangered languages and present a systematic analysis of how
general-purpose OCR tools are not robust to the data-scarce setting of
endangered languages. We develop an OCR post-correction method tailored to ease
training in this data-scarce setting, reducing the recognition error rate by
34% on average across the three languages.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2011.05402v1
98,An Assessment of the Impact of OCR Noise on Language Models,http://arxiv.org/pdf/2202.00470v1,"[arxiv.Result.Author('Konstantin Todorov'), arxiv.Result.Author('Giovanni Colavizza')]",,2022-01-26 21:56:14+00:00,"Neural language models are the backbone of modern-day natural language
processing applications. Their use on textual heritage collections which have
undergone Optical Character Recognition (OCR) is therefore also increasing.
Nevertheless, our understanding of the impact OCR noise could have on language
models is still limited. We perform an assessment of the impact OCR noise has
on a variety of language models, using data in Dutch, English, French and
German. We find that OCR noise poses a significant obstacle to language
modelling, with language models increasingly diverging from their noiseless
targets as OCR quality lowers. In the presence of small corpora, simpler models
including PPMI and Word2Vec consistently outperform transformer-based models in
this respect.",,cs.CL,"['cs.CL', 'cs.LG']",http://arxiv.org/abs/2202.00470v1
99,LangPro: Natural Language Theorem Prover,http://arxiv.org/pdf/1708.09417v1,[arxiv.Result.Author('Lasha Abzianidze')],,2017-08-30 18:22:28+00:00,"LangPro is an automated theorem prover for natural language
(https://github.com/kovvalsky/LangPro). Given a set of premises and a
hypothesis, it is able to prove semantic relations between them. The prover is
based on a version of analytic tableau method specially designed for natural
logic. The proof procedure operates on logical forms that preserve linguistic
expressions to a large extent. %This property makes the logical forms easily
obtainable from syntactic trees. %, in particular, Combinatory Categorial
Grammar derivation trees. The nature of proofs is deductive and transparent. On
the FraCaS and SICK textual entailment datasets, the prover achieves high
results comparable to state-of-the-art.",,cs.CL,"['cs.CL', '68T50', 'I.2.7']",http://arxiv.org/abs/1708.09417v1
100,"SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects",http://arxiv.org/pdf/2309.07445v1,"[arxiv.Result.Author('David Ifeoluwa Adelani'), arxiv.Result.Author('Hannah Liu'), arxiv.Result.Author('Xiaoyu Shen'), arxiv.Result.Author('Nikita Vassilyev'), arxiv.Result.Author('Jesujoba O. Alabi'), arxiv.Result.Author('Yanke Mao'), arxiv.Result.Author('Haonan Gao'), arxiv.Result.Author('Annie En-Shiun Lee')]",,2023-09-14 05:56:49+00:00,"Despite the progress we have recorded in the last few years in multilingual
natural language processing, evaluation is typically limited to a small set of
languages with available datasets which excludes a large number of low-resource
languages. In this paper, we created SIB-200 -- a large-scale open-sourced
benchmark dataset for topic classification in 200 languages and dialects to
address the lack of evaluation dataset for Natural Language Understanding
(NLU). For many of the languages covered in SIB-200, this is the first publicly
available evaluation dataset for NLU. The dataset is based on Flores-200
machine translation corpus. We annotated the English portion of the dataset and
extended the sentence-level annotation to the remaining 203 languages covered
in the corpus. Despite the simplicity of this task, our evaluation in
full-supervised setting, cross-lingual transfer setting and prompting of large
language model setting show that there is still a large gap between the
performance of high-resource and low-resource languages when multilingual
evaluation is scaled to numerous world languages. We found that languages
unseen during the pre-training of multilingual language models,
under-represented language families (like Nilotic and Altantic-Congo), and
languages from the regions of Africa, Americas, Oceania and South East Asia,
often have the lowest performance on our topic classification dataset. We hope
our dataset will encourage a more inclusive evaluation of multilingual language
models on a more diverse set of languages. https://github.com/dadelani/sib-200",,cs.CL,['cs.CL'],http://arxiv.org/abs/2309.07445v1
101,Basic Classes of Grammars with Prohibition,http://arxiv.org/pdf/1302.5181v1,[arxiv.Result.Author('Mark Burgin')],,2013-02-21 04:58:48+00:00,"A practical tool for natural language modeling and development of
human-machine interaction is developed in the context of formal grammars and
languages. A new type of formal grammars, called grammars with prohibition, is
introduced. Grammars with prohibition provide more powerful tools for natural
language generation and better describe processes of language learning than the
conventional formal grammars. Here we study relations between languages
generated by different grammars with prohibition based on conventional types of
formal grammars such as context-free or context sensitive grammars. Besides, we
compare languages generated by different grammars with prohibition and
languages generated by conventional formal grammars. In particular, it is
demonstrated that they have essentially higher computational power and
expressive possibilities in comparison with the conventional formal grammars.
Thus, while conventional formal grammars are recursive and subrecursive
algorithms, many classes of grammars with prohibition are superrecursive
algorithms. Results presented in this work are aimed at the development of
human-machine interaction, modeling natural languages, empowerment of
programming languages, computer simulation, better software systems, and theory
of recursion.",,cs.FL,"['cs.FL', 'cs.CL']",http://arxiv.org/abs/1302.5181v1
102,XLDA: Cross-Lingual Data Augmentation for Natural Language Inference and Question Answering,http://arxiv.org/pdf/1905.11471v1,"[arxiv.Result.Author('Jasdeep Singh'), arxiv.Result.Author('Bryan McCann'), arxiv.Result.Author('Nitish Shirish Keskar'), arxiv.Result.Author('Caiming Xiong'), arxiv.Result.Author('Richard Socher')]",,2019-05-27 19:44:33+00:00,"While natural language processing systems often focus on a single language,
multilingual transfer learning has the potential to improve performance,
especially for low-resource languages. We introduce XLDA, cross-lingual data
augmentation, a method that replaces a segment of the input text with its
translation in another language. XLDA enhances performance of all 14 tested
languages of the cross-lingual natural language inference (XNLI) benchmark.
With improvements of up to $4.8\%$, training with XLDA achieves
state-of-the-art performance for Greek, Turkish, and Urdu. XLDA is in contrast
to, and performs markedly better than, a more naive approach that aggregates
examples in various languages in a way that each example is solely in one
language. On the SQuAD question answering task, we see that XLDA provides a
$1.0\%$ performance increase on the English evaluation set. Comprehensive
experiments suggest that most languages are effective as cross-lingual
augmentors, that XLDA is robust to a wide range of translation quality, and
that XLDA is even more effective for randomly initialized models than for
pretrained models.",,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/1905.11471v1
103,BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model,http://arxiv.org/pdf/2204.03905v2,"[arxiv.Result.Author('Hongyi Yuan'), arxiv.Result.Author('Zheng Yuan'), arxiv.Result.Author('Ruyi Gan'), arxiv.Result.Author('Jiaxing Zhang'), arxiv.Result.Author('Yutao Xie'), arxiv.Result.Author('Sheng Yu')]",,2022-04-08 08:07:42+00:00,"Pretrained language models have served as important backbones for natural
language processing. Recently, in-domain pretraining has been shown to benefit
various domain-specific downstream tasks. In the biomedical domain, natural
language generation (NLG) tasks are of critical importance, while understudied.
Approaching natural language understanding (NLU) tasks as NLG achieves
satisfying performance in the general domain through constrained language
generation or language prompting. We emphasize the lack of in-domain generative
language models and the unsystematic generative downstream benchmarks in the
biomedical domain, hindering the development of the research community. In this
work, we introduce the generative language model BioBART that adapts BART to
the biomedical domain. We collate various biomedical language generation tasks
including dialogue, summarization, entity linking, and named entity
recognition. BioBART pretrained on PubMed abstracts has enhanced performance
compared to BART and set strong baselines on several tasks. Furthermore, we
conduct ablation studies on the pretraining tasks for BioBART and find that
sentence permutation has negative effects on downstream tasks.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2204.03905v2
104,XNLI 2.0: Improving XNLI dataset and performance on Cross Lingual Understanding (XLU),http://arxiv.org/pdf/2301.06527v1,"[arxiv.Result.Author('Ankit Kumar Upadhyay'), arxiv.Result.Author('Harsit Kumar Upadhya')]",,2023-01-16 17:24:57+00:00,"Natural Language Processing systems are heavily dependent on the availability
of annotated data to train practical models. Primarily, models are trained on
English datasets. In recent times, significant advances have been made in
multilingual understanding due to the steeply increasing necessity of working
in different languages. One of the points that stands out is that since there
are now so many pre-trained multilingual models, we can utilize them for
cross-lingual understanding tasks. Using cross-lingual understanding and
Natural Language Inference, it is possible to train models whose applications
extend beyond the training language. We can leverage the power of machine
translation to skip the tiresome part of translating datasets from one language
to another. In this work, we focus on improving the original XNLI dataset by
re-translating the MNLI dataset in all of the 14 different languages present in
XNLI, including the test and dev sets of XNLI using Google Translate. We also
perform experiments by training models in all 15 languages and analyzing their
performance on the task of natural language inference. We then expand our
boundary to investigate if we could improve performance in low-resource
languages such as Swahili and Urdu by training models in languages other than
English.",,cs.CL,"['cs.CL', 'cs.LG']",http://arxiv.org/abs/2301.06527v1
105,Zero-shot cross-lingual transfer language selection using linguistic similarity,http://arxiv.org/pdf/2301.13720v1,"[arxiv.Result.Author('Juuso Eronen'), arxiv.Result.Author('Michal Ptaszynski'), arxiv.Result.Author('Fumito Masui')]",10.1016/j.ipm.2022.103250,2023-01-31 15:56:40+00:00,"We study the selection of transfer languages for different Natural Language
Processing tasks, specifically sentiment analysis, named entity recognition and
dependency parsing. In order to select an optimal transfer language, we propose
to utilize different linguistic similarity metrics to measure the distance
between languages and make the choice of transfer language based on this
information instead of relying on intuition. We demonstrate that linguistic
similarity correlates with cross-lingual transfer performance for all of the
proposed tasks. We also show that there is a statistically significant
difference in choosing the optimal language as the transfer source instead of
English. This allows us to select a more suitable transfer language which can
be used to better leverage knowledge from high-resource languages in order to
improve the performance of language applications lacking data. For the study,
we used datasets from eight different languages from three language families.","Information Processing & Management, Volume 60, Issue 3, 2023,
  103250",cs.CL,['cs.CL'],http://arxiv.org/abs/2301.13720v1
106,Syllable-based Neural Named Entity Recognition for Myanmar Language,http://arxiv.org/pdf/1903.04739v1,"[arxiv.Result.Author('Hsu Myat Mo'), arxiv.Result.Author('Khin Mar Soe')]",10.5121/ijnlc.2019.8101,2019-03-12 05:52:41+00:00,"Named Entity Recognition (NER) for Myanmar Language is essential to Myanmar
natural language processing research work. In this work, NER for Myanmar
language is treated as a sequence tagging problem and the effectiveness of deep
neural networks on NER for Myanmar language has been investigated. Experiments
are performed by applying deep neural network architectures on syllable level
Myanmar contexts. Very first manually annotated NER corpus for Myanmar language
is also constructed and proposed. In developing our in-house NER corpus,
sentences from online news website and also sentences supported from
ALT-Parallel-Corpus are also used. This ALT corpus is one part of the Asian
Language Treebank (ALT) project under ASEAN IVO. This paper contributes the
first evaluation of neural network models on NER task for Myanmar language. The
experimental results show that those neural sequence models can produce
promising results compared to the baseline CRF model. Among those neural
architectures, bidirectional LSTM network added CRF layer above gives the
highest F-score value. This work also aims to discover the effectiveness of
neural network approaches to Myanmar textual processing as well as to promote
further researches on this understudied language.","International Journal on Natural Language Computing (IJNLC) Vol.8,
  No.1, February 2019",cs.CL,['cs.CL'],http://arxiv.org/abs/1903.04739v1
107,Comparative Analysis of Word Embeddings for Capturing Word Similarities,http://arxiv.org/pdf/2005.03812v1,"[arxiv.Result.Author('Martina Toshevska'), arxiv.Result.Author('Frosina Stojanovska'), arxiv.Result.Author('Jovan Kalajdjieski')]",10.5121/csit.2020.100402,2020-05-08 01:16:03+00:00,"Distributed language representation has become the most widely used technique
for language representation in various natural language processing tasks. Most
of the natural language processing models that are based on deep learning
techniques use already pre-trained distributed word representations, commonly
called word embeddings. Determining the most qualitative word embeddings is of
crucial importance for such models. However, selecting the appropriate word
embeddings is a perplexing task since the projected embedding space is not
intuitive to humans. In this paper, we explore different approaches for
creating distributed word representations. We perform an intrinsic evaluation
of several state-of-the-art word embedding methods. Their performance on
capturing word similarities is analysed with existing benchmark datasets for
word pairs similarities. The research in this paper conducts a correlation
analysis between ground truth word similarities and similarities obtained by
different word embedding methods.","6th International Conference on Natural Language Processing (NATP
  2020)",cs.CL,"['cs.CL', 'cs.LG']",http://arxiv.org/abs/2005.03812v1
108,Do learned speech symbols follow Zipf's law?,http://arxiv.org/pdf/2309.09690v1,"[arxiv.Result.Author('Shinnosuke Takamichi'), arxiv.Result.Author('Hiroki Maeda'), arxiv.Result.Author('Joonyong Park'), arxiv.Result.Author('Daisuke Saito'), arxiv.Result.Author('Hiroshi Saruwatari')]",,2023-09-18 11:56:10+00:00,"In this study, we investigate whether speech symbols, learned through deep
learning, follow Zipf's law, akin to natural language symbols. Zipf's law is an
empirical law that delineates the frequency distribution of words, forming
fundamentals for statistical analysis in natural language processing. Natural
language symbols, which are invented by humans to symbolize speech content, are
recognized to comply with this law. On the other hand, recent breakthroughs in
spoken language processing have given rise to the development of learned speech
symbols; these are data-driven symbolizations of speech content. Our objective
is to ascertain whether these data-driven speech symbols follow Zipf's law, as
the same as natural language symbols. Through our investigation, we aim to
forge new ways for the statistical analysis of spoken language processing.",,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']",http://arxiv.org/abs/2309.09690v1
109,LanideNN: Multilingual Language Identification on Character Window,http://arxiv.org/pdf/1701.03338v2,"[arxiv.Result.Author('Tom Kocmi'), arxiv.Result.Author('Ondřej Bojar')]",,2017-01-12 13:41:08+00:00,"In language identification, a common first step in natural language
processing, we want to automatically determine the language of some input text.
Monolingual language identification assumes that the given document is written
in one language. In multilingual language identification, the document is
usually in two or three languages and we just want their names. We aim one step
further and propose a method for textual language identification where
languages can change arbitrarily and the goal is to identify the spans of each
of the languages. Our method is based on Bidirectional Recurrent Neural
Networks and it performs well in monolingual and multilingual language
identification tasks on six datasets covering 131 languages. The method keeps
the accuracy also for short documents and across domains, so it is ideal for
off-the-shelf use without preparation of training data.","Proceedings of the 15th Conference of the European Chapter of the
  Association for Computational Linguistics: Volume 1, Long Papers, 927-936
  (2017)",cs.CL,['cs.CL'],http://arxiv.org/abs/1701.03338v2
110,HinFlair: pre-trained contextual string embeddings for pos tagging and text classification in the Hindi language,http://arxiv.org/pdf/2101.06949v1,[arxiv.Result.Author('Harsh Patel')],,2021-01-18 09:23:35+00:00,"Recent advancements in language models based on recurrent neural networks and
transformers architecture have achieved state-of-the-art results on a wide
range of natural language processing tasks such as pos tagging, named entity
recognition, and text classification. However, most of these language models
are pre-trained in high resource languages like English, German, Spanish.
Multi-lingual language models include Indian languages like Hindi, Telugu,
Bengali in their training corpus, but they often fail to represent the
linguistic features of these languages as they are not the primary language of
the study. We introduce HinFlair, which is a language representation model
(contextual string embeddings) pre-trained on a large monolingual Hindi corpus.
Experiments were conducted on 6 text classification datasets and a Hindi
dependency treebank to analyze the performance of these contextualized string
embeddings for the Hindi language. Results show that HinFlair outperforms
previous state-of-the-art publicly available pre-trained embeddings for
downstream tasks like text classification and pos tagging. Also, HinFlair when
combined with FastText embeddings outperforms many transformers-based language
models trained particularly for the Hindi language.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2101.06949v1
111,Exploring the Naturalness of Buggy Code with Recurrent Neural Networks,http://arxiv.org/pdf/1803.08793v1,"[arxiv.Result.Author('Jack Lanchantin'), arxiv.Result.Author('Ji Gao')]",,2018-03-21 16:14:22+00:00,"Statistical language models are powerful tools which have been used for many
tasks within natural language processing. Recently, they have been used for
other sequential data such as source code.(Ray et al., 2015) showed that it is
possible train an n-gram source code language mode, and use it to predict buggy
lines in code by determining ""unnatural"" lines via entropy with respect to the
language model. In this work, we propose using a more advanced language
modeling technique, Long Short-term Memory recurrent neural networks, to model
source code and classify buggy lines based on entropy. We show that our method
slightly outperforms an n-gram model in the buggy line classification task
using AUC.",,cs.SE,"['cs.SE', 'cs.CL', 'cs.LG']",http://arxiv.org/abs/1803.08793v1
112,Collaborative construction of lexicographic and parallel datasets for African languages: first assessment,http://arxiv.org/pdf/2103.16712v1,[arxiv.Result.Author('Elvis Mboning Tchiaze')],,2021-03-30 22:43:13+00:00,"Faced with a considerable lack of resources in African languages to carry out
work in Natural Language Processing (NLP), Natural Language Understanding (NLU)
and artificial intelligence, the research teams of NTeALan association has set
itself the objective of building open-source platforms for the collaborative
construction of lexicographic data in African languages. In this article, we
present our first reports after 2 years of collaborative construction of
lexicographic resources useful for African NLP tools.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2103.16712v1
113,Message-Passing Protocols for Real-World Parsing -- An Object-Oriented Model and its Preliminary Evaluation,http://arxiv.org/pdf/cmp-lg/9709010v1,"[arxiv.Result.Author('Udo Hahn'), arxiv.Result.Author('Peter Neuhaus'), arxiv.Result.Author('Norbert Broeker')]",,1997-09-23 12:43:09+00:00,"We argue for a performance-based design of natural language grammars and
their associated parsers in order to meet the constraints imposed by real-world
NLP. Our approach incorporates declarative and procedural knowledge about
language and language use within an object-oriented specification framework. We
discuss several message-passing protocols for parsing and provide reasons for
sacrificing completeness of the parse in favor of efficiency based on a
preliminary empirical evaluation.","Proc. Int'l Workshop on Parsing Technologies, 1997, Boston/MA:
  MIT, pp 101-112",cmp-lg,"['cmp-lg', 'cs.CL']",http://arxiv.org/abs/cmp-lg/9709010v1
114,Knowledge Engineering using Large Language Models,http://arxiv.org/pdf/2310.00637v1,"[arxiv.Result.Author('Bradley P. Allen'), arxiv.Result.Author('Lise Stork'), arxiv.Result.Author('Paul Groth')]",,2023-10-01 10:26:25+00:00,"Knowledge engineering is a discipline that focuses on the creation and
maintenance of processes that generate and apply knowledge. Traditionally,
knowledge engineering approaches have focused on knowledge expressed in formal
languages. The emergence of large language models and their capabilities to
effectively work with natural language, in its broadest sense, raises questions
about the foundations and practice of knowledge engineering. Here, we outline
the potential role of LLMs in knowledge engineering, identifying two central
directions: 1) creating hybrid neuro-symbolic knowledge systems; and 2)
enabling knowledge engineering in natural language. Additionally, we formulate
key open research questions to tackle these directions.",,cs.AI,"['cs.AI', 'cs.CL']",http://arxiv.org/abs/2310.00637v1
115,GreekBART: The First Pretrained Greek Sequence-to-Sequence Model,http://arxiv.org/pdf/2304.00869v1,"[arxiv.Result.Author('Iakovos Evdaimon'), arxiv.Result.Author('Hadi Abdine'), arxiv.Result.Author('Christos Xypolopoulos'), arxiv.Result.Author('Stamatis Outsios'), arxiv.Result.Author('Michalis Vazirgiannis'), arxiv.Result.Author('Giorgos Stamou')]",,2023-04-03 10:48:51+00:00,"The era of transfer learning has revolutionized the fields of Computer Vision
and Natural Language Processing, bringing powerful pretrained models with
exceptional performance across a variety of tasks. Specifically, Natural
Language Processing tasks have been dominated by transformer-based language
models. In Natural Language Inference and Natural Language Generation tasks,
the BERT model and its variants, as well as the GPT model and its successors,
demonstrated exemplary performance. However, the majority of these models are
pretrained and assessed primarily for the English language or on a multilingual
corpus. In this paper, we introduce GreekBART, the first Seq2Seq model based on
BART-base architecture and pretrained on a large-scale Greek corpus. We
evaluate and compare GreekBART against BART-random, Greek-BERT, and XLM-R on a
variety of discriminative tasks. In addition, we examine its performance on two
NLG tasks from GreekSUM, a newly introduced summarization dataset for the Greek
language. The model, the code, and the new summarization dataset will be
publicly available.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2304.00869v1
116,Description Logics based Formalization of Wh-Queries,http://arxiv.org/pdf/1312.6948v1,"[arxiv.Result.Author('Sourish Dasgupta'), arxiv.Result.Author('Rupali KaPatel'), arxiv.Result.Author('Ankur Padia'), arxiv.Result.Author('Kushal Shah')]",,2013-12-25 09:23:49+00:00,"The problem of Natural Language Query Formalization (NLQF) is to translate a
given user query in natural language (NL) into a formal language so that the
semantic interpretation has equivalence with the NL interpretation.
Formalization of NL queries enables logic based reasoning during information
retrieval, database query, question-answering, etc. Formalization also helps in
Web query normalization and indexing, query intent analysis, etc. In this paper
we are proposing a Description Logics based formal methodology for wh-query
intent (also called desire) identification and corresponding formal
translation. We evaluated the scalability of our proposed formalism using
Microsoft Encarta 98 query dataset and OWL-S TC v.4.0 dataset.",,cs.CL,"['cs.CL', 'cs.AI']",http://arxiv.org/abs/1312.6948v1
117,Extracting and filtering paraphrases by bridging natural language inference and paraphrasing,http://arxiv.org/pdf/2111.07119v1,"[arxiv.Result.Author('Matej Klemen'), arxiv.Result.Author('Marko Robnik-Šikonja')]",,2021-11-13 14:06:37+00:00,"Paraphrasing is a useful natural language processing task that can contribute
to more diverse generated or translated texts. Natural language inference (NLI)
and paraphrasing share some similarities and can benefit from a joint approach.
We propose a novel methodology for the extraction of paraphrasing datasets from
NLI datasets and cleaning existing paraphrasing datasets. Our approach is based
on bidirectional entailment; namely, if two sentences can be mutually entailed,
they are paraphrases. We evaluate our approach using several large pretrained
transformer language models in the monolingual and cross-lingual setting. The
results show high quality of extracted paraphrasing datasets and surprisingly
high noise levels in two existing paraphrasing datasets.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2111.07119v1
118,Extension of hidden markov model for recognizing large vocabulary of sign language,http://arxiv.org/pdf/1304.3265v1,"[arxiv.Result.Author('Maher Jebali'), arxiv.Result.Author('Patrice Dalle'), arxiv.Result.Author('Mohamed Jemni')]",,2013-04-11 11:56:39+00:00,"Computers still have a long way to go before they can interact with users in
a truly natural fashion. From a users perspective, the most natural way to
interact with a computer would be through a speech and gesture interface.
Although speech recognition has made significant advances in the past ten
years, gesture recognition has been lagging behind. Sign Languages (SL) are the
most accomplished forms of gestural communication. Therefore, their automatic
analysis is a real challenge, which is interestingly implied to their lexical
and syntactic organization levels. Statements dealing with sign language occupy
a significant interest in the Automatic Natural Language Processing (ANLP)
domain. In this work, we are dealing with sign language recognition, in
particular of French Sign Language (FSL). FSL has its own specificities, such
as the simultaneity of several parameters, the important role of the facial
expression or movement and the use of space for the proper utterance
organization. Unlike speech recognition, Frensh sign language (FSL) events
occur both sequentially and simultaneously. Thus, the computational processing
of FSL is too complex than the spoken languages. We present a novel approach
based on HMM to reduce the recognition complexity.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1304.3265v1
119,"Knowledge Representation for Conceptual, Motivational, and Affective Processes in Natural Language Communication",http://arxiv.org/pdf/2210.08994v2,"[arxiv.Result.Author('Seng-Beng Ho'), arxiv.Result.Author('Zhaoxia Wang'), arxiv.Result.Author('Boon-Kiat Quek'), arxiv.Result.Author('Erik Cambria')]",,2022-09-26 01:37:50+00:00,"Natural language communication is an intricate and complex process. The
speaker usually begins with an intention and motivation of what is to be
communicated, and what effects are expected from the communication, while
taking into consideration the listener's mental model to concoct an appropriate
sentence. The listener likewise has to interpret what the speaker means, and
respond accordingly, also with the speaker's mental state in mind. To do this
successfully, conceptual, motivational, and affective processes have to be
represented appropriately to drive the language generation and understanding
processes. Language processing has succeeded well with the big data approach in
applications such as chatbots and machine translation. However, in human-robot
collaborative social communication and in using natural language for delivering
precise instructions to robots, a deeper representation of the conceptual,
motivational, and affective processes is needed. This paper capitalizes on the
UGALRS (Unified General Autonomous and Language Reasoning System) framework and
the CD+ (Conceptual Representation Plus) representational scheme to illustrate
how social communication through language is supported by a knowledge
representational scheme that handles conceptual, motivational, and affective
processes in a deep and general way. Though a small set of concepts,
motivations, and emotions is treated in this paper, its main contribution is in
articulating a general framework of knowledge representation and processing to
link these aspects together in serving the purpose of natural language
communication for an intelligent system.",,cs.AI,['cs.AI'],http://arxiv.org/abs/2210.08994v2
120,"Natural language processing: she needs something old and something new (maybe something borrowed and something blue, too)",http://arxiv.org/pdf/cmp-lg/9512004v1,[arxiv.Result.Author('Karen Sparck Jones')],,1995-12-21 16:38:12+00:00,"Given the present state of work in natural language processing, this address
argues first, that advance in both science and applications requires a revival
of concern about what language is about, broadly speaking the world; and
second, that an attack on the summarising task, which is made ever more
important by the growth of electronic text resources and requires an
understanding of the role of large-scale discourse structure in marking
important text content, is a good way forward.",,cmp-lg,"['cmp-lg', 'cs.CL']",http://arxiv.org/abs/cmp-lg/9512004v1
121,What is word sense disambiguation good for?,http://arxiv.org/pdf/cmp-lg/9712008v1,[arxiv.Result.Author('Adam Kilgarriff')],,1997-12-23 15:32:05+00:00,"Word sense disambiguation has developed as a sub-area of natural language
processing, as if, like parsing, it was a well-defined task which was a
pre-requisite to a wide range of language-understanding applications. First, I
review earlier work which shows that a set of senses for a word is only ever
defined relative to a particular human purpose, and that a view of word senses
as part of the linguistic furniture lacks theoretical underpinnings. Then, I
investigate whether and how word sense ambiguity is in fact a problem for
different varieties of NLP application.","Proc. Natural Language Processing Pacific Rim Symposium. Phuket,
  Thailand. December 1997. Pp 209--214.",cmp-lg,"['cmp-lg', 'cs.CL']",http://arxiv.org/abs/cmp-lg/9712008v1
122,Evaluation of Computational Grammar Formalisms for Indian Languages,http://arxiv.org/pdf/1209.1301v1,"[arxiv.Result.Author('Nisheeth Joshi'), arxiv.Result.Author('Iti Mathur')]",,2012-08-19 02:31:29+00:00,"Natural Language Parsing has been the most prominent research area since the
genesis of Natural Language Processing. Probabilistic Parsers are being
developed to make the process of parser development much easier, accurate and
fast. In Indian context, identification of which Computational Grammar
Formalism is to be used is still a question which needs to be answered. In this
paper we focus on this problem and try to analyze different formalisms for
Indian languages.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1209.1301v1
123,From Textual Information Sources to Linked Data in the Agatha Project,http://arxiv.org/pdf/1909.05359v1,"[arxiv.Result.Author('Paulo Quaresma'), arxiv.Result.Author('Vitor Beires Nogueira'), arxiv.Result.Author('Kashyap Raiyani'), arxiv.Result.Author('Roy Bayot'), arxiv.Result.Author('Teresa Gonçalves')]",,2019-09-03 08:27:37+00:00,"Automatic reasoning about textual information is a challenging task in modern
Natural Language Processing (NLP) systems. In this work we describe our
proposal for representing and reasoning about Portuguese documents by means of
Linked Data like ontologies and thesauri. Our approach resorts to a specialized
pipeline of natural language processing (part-of-speech tagger, named entity
recognition, semantic role labeling) to populate an ontology for the domain of
criminal investigations. The provided architecture and ontology are language
independent. Although some of the NLP modules are language dependent, they can
be built using adequate AI methodologies.",,cs.CL,"['cs.CL', 'cs.AI']",http://arxiv.org/abs/1909.05359v1
124,Parametrized Quantum Circuits of Synonymous Sentences in Quantum Natural Language Processing,http://arxiv.org/pdf/2102.02204v1,"[arxiv.Result.Author('Mina Abbaszadeh'), arxiv.Result.Author('S. Shahin Mousavi'), arxiv.Result.Author('Vahid Salari')]",,2021-02-02 23:11:41+00:00,"In this paper, we develop a compositional vector-based semantics of positive
transitive sentences in quantum natural language processing for a non-English
language, i.e. Persian, to compare the parametrized quantum circuits of two
synonymous sentences in two languages, English and Persian. By considering
grammar+meaning of a transitive sentence, we translate DisCoCat diagram via
ZX-calculus into quantum circuit form. Also, we use a bigraph method to rewrite
DisCoCat diagram and turn into quantum circuit in the semantic side.",,quant-ph,"['quant-ph', 'cs.CL']",http://arxiv.org/abs/2102.02204v1
125,The Role of Explanatory Value in Natural Language Processing,http://arxiv.org/pdf/2209.06169v1,[arxiv.Result.Author('Kees van Deemter')],,2022-09-13 17:19:04+00:00,"A key aim of science is explanation, yet the idea of explaining language
phenomena has taken a backseat in mainstream Natural Language Processing (NLP)
and many other areas of Artificial Intelligence. I argue that explanation of
linguistic behaviour should be a main goal of NLP, and that this is not the
same as making NLP models explainable. To illustrate these ideas, some recent
models of human language production are compared with each other. I conclude by
asking what it would mean for NLP research and institutional policies if our
community took explanatory value seriously, while heeding some possible
pitfalls.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2209.06169v1
126,Opportunities for Large Language Models and Discourse in Engineering Design,http://arxiv.org/pdf/2306.09169v1,"[arxiv.Result.Author('Jan Göpfert'), arxiv.Result.Author('Jann M. Weinand'), arxiv.Result.Author('Patrick Kuckertz'), arxiv.Result.Author('Detlef Stolten')]",,2023-06-15 14:46:44+00:00,"In recent years, large language models have achieved breakthroughs on a wide
range of benchmarks in natural language processing and continue to increase in
performance. Recently, the advances of large language models have raised
interest outside the natural language processing community and could have a
large impact on daily life. In this paper, we pose the question: How will large
language models and other foundation models shape the future product
development process? We provide the reader with an overview of the subject by
summarizing both recent advances in natural language processing and the use of
information technology in the engineering design process. We argue that
discourse should be regarded as the core of engineering design processes, and
therefore should be represented in a digital artifact. On this basis, we
describe how foundation models such as large language models could contribute
to the design discourse by automating parts thereof that involve creativity and
reasoning, and were previously reserved for humans. We describe how
simulations, experiments, topology optimizations, and other process steps can
be integrated into a machine-actionable, discourse-centric design process.
Finally, we outline the future research that will be necessary for the
implementation of the conceptualized framework.",,cs.CL,"['cs.CL', 'cs.AI', 'cs.CE']",http://arxiv.org/abs/2306.09169v1
127,Autonomous requirements specification processing using natural language processing,http://arxiv.org/pdf/1407.6099v1,"[arxiv.Result.Author('S. G. Macdonell'), arxiv.Result.Author('K. Min'), arxiv.Result.Author('A. M. Connor')]",,2014-07-23 03:29:44+00:00,"We describe our ongoing research that centres on the application of natural
language processing (NLP) to software engineering and systems development
activities. In particular, this paper addresses the use of NLP in the
requirements analysis and systems design processes. We have developed a
prototype toolset that can assist the systems analyst or software engineer to
select and verify terms relevant to a project. In this paper we describe the
processes employed by the system to extract and classify objects of interest
from requirements documents. These processes are illustrated using a small
example.",,cs.CL,"['cs.CL', 'cs.SE']",http://arxiv.org/abs/1407.6099v1
128,Semi-supervised Classification for Natural Language Processing,http://arxiv.org/pdf/1409.7612v1,[arxiv.Result.Author('Rushdi Shams')],,2014-09-25 15:18:44+00:00,"Semi-supervised classification is an interesting idea where classification
models are learned from both labeled and unlabeled data. It has several
advantages over supervised classification in natural language processing
domain. For instance, supervised classification exploits only labeled data that
are expensive, often difficult to get, inadequate in quantity, and require
human experts for annotation. On the other hand, unlabeled data are inexpensive
and abundant. Despite the fact that many factors limit the wide-spread use of
semi-supervised classification, it has become popular since its level of
performance is empirically as good as supervised classification. This study
explores the possibilities and achievements as well as complexity and
limitations of semi-supervised classification for several natural langue
processing tasks like parsing, biomedical information processing, text
classification, and summarization.",,cs.CL,"['cs.CL', 'cs.LG']",http://arxiv.org/abs/1409.7612v1
129,New Approaches for Natural Language Understanding based on the Idea that Natural Language encodes both Information and its Processing Procedures,http://arxiv.org/pdf/2010.12789v3,[arxiv.Result.Author('Limin Zhang')],,2020-10-24 05:40:47+00:00,"We must recognize that natural language is a way of information encoding, and
it encodes not only the information but also the procedures for how information
is processed. To understand natural language, the same as we conceive and
design computer languages, the first step is to separate information (or data)
and the processing procedures of information (or data). In natural language,
some processing procedures of data are encoded directly as the structure chunk
and the pointer chunk (this paper has reclassified lexical chunks as the data
chunk, structure chunk, and the pointer chunk); some processing procedures of
data imply in sentences structures; some requests of processing procedures are
expressed by information senders and processed by information receivers. For
the data parts, the classification encoding system of attribute information and
the information organization architecture (including constitutional structures
of information sets and the hierarchy between the information sets) were
discussed. In section 2, the theoretical part elaborated in section 2 has been
verified in examples and proofed that the studies in this paper have achieved
the goal of enabling machines to understand the information conveyed in the
dialogue. In section 4, the author summarizes the basic conditions of
""Understanding"", rethinks what ""Understanding"" is and how to proceed. The study
in this paper provides a practical, theoretical basis and research methods for
NLU. It also can be applied in large-scale and multi-type information
processing in the artificial intelligence (AI) area.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2010.12789v3
130,Semantic Parsing Natural Language into SPARQL: Improving Target Language Representation with Neural Attention,http://arxiv.org/pdf/1803.04329v1,"[arxiv.Result.Author('Fabiano Ferreira Luz'), arxiv.Result.Author('Marcelo Finger')]",,2018-03-12 15:59:10+00:00,"Semantic parsing is the process of mapping a natural language sentence into a
formal representation of its meaning. In this work we use the neural network
approach to transform natural language sentence into a query to an ontology
database in the SPARQL language. This method does not rely on handcraft-rules,
high-quality lexicons, manually-built templates or other handmade complex
structures. Our approach is based on vector space model and neural networks.
The proposed model is based in two learning steps. The first step generates a
vector representation for the sentence in natural language and SPARQL query.
The second step uses this vector representation as input to a neural network
(LSTM with attention mechanism) to generate a model able to encode natural
language and decode SPARQL.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1803.04329v1
131,Towards a Natural Language Query Processing System,http://arxiv.org/pdf/2009.12414v1,"[arxiv.Result.Author('Chantal Montgomery'), arxiv.Result.Author('Haruna Isah'), arxiv.Result.Author('Farhana Zulkernine')]",,2020-09-25 19:52:20+00:00,"Tackling the information retrieval gap between non-technical database
end-users and those with the knowledge of formal query languages has been an
interesting area of data management and analytics research. The use of natural
language interfaces to query information from databases offers the opportunity
to bridge the communication challenges between end-users and systems that use
formal query languages. Previous research efforts mainly focused on developing
structured query interfaces to relational databases. However, the evolution of
unstructured big data such as text, images, and video has exposed the
limitations of traditional structured query interfaces. While the existing web
search tools prove the popularity and usability of natural language query, they
return complete documents and web pages instead of focused query responses and
are not applicable to database systems. This paper reports our study on the
design and development of a natural language query interface to a backend
relational database. The novelty in the study lies in defining a graph database
as a middle layer to store necessary metadata needed to transform a natural
language query into structured query language that can be executed on backend
databases. We implemented and evaluated our approach using a restaurant
dataset. The translation results for some sample queries yielded a 90% accuracy
rate.",,cs.IR,"['cs.IR', 'cs.AI', 'cs.DB']",http://arxiv.org/abs/2009.12414v1
132,Exploring Software Naturalness through Neural Language Models,http://arxiv.org/pdf/2006.12641v2,"[arxiv.Result.Author('Luca Buratti'), arxiv.Result.Author('Saurabh Pujar'), arxiv.Result.Author('Mihaela Bornea'), arxiv.Result.Author('Scott McCarley'), arxiv.Result.Author('Yunhui Zheng'), arxiv.Result.Author('Gaetano Rossiello'), arxiv.Result.Author('Alessandro Morari'), arxiv.Result.Author('Jim Laredo'), arxiv.Result.Author('Veronika Thost'), arxiv.Result.Author('Yufan Zhuang'), arxiv.Result.Author('Giacomo Domeniconi')]",,2020-06-22 21:56:14+00:00,"The Software Naturalness hypothesis argues that programming languages can be
understood through the same techniques used in natural language processing. We
explore this hypothesis through the use of a pre-trained transformer-based
language model to perform code analysis tasks. Present approaches to code
analysis depend heavily on features derived from the Abstract Syntax Tree (AST)
while our transformer-based language models work on raw source code. This work
is the first to investigate whether such language models can discover AST
features automatically. To achieve this, we introduce a sequence labeling task
that directly probes the language models understanding of AST. Our results show
that transformer based language models achieve high accuracy in the AST tagging
task. Furthermore, we evaluate our model on a software vulnerability
identification task. Importantly, we show that our approach obtains
vulnerability identification results comparable to graph based approaches that
rely heavily on compilers for feature extraction.",,cs.CL,"['cs.CL', 'cs.LG', 'cs.PL']",http://arxiv.org/abs/2006.12641v2
133,From BERT to GPT-3 Codex: Harnessing the Potential of Very Large Language Models for Data Management,http://arxiv.org/pdf/2306.09339v1,[arxiv.Result.Author('Immanuel Trummer')],10.14778/3554821.3554896,2023-06-15 17:59:29+00:00,"Large language models have recently advanced the state of the art on many
natural language processing benchmarks. The newest generation of models can be
applied to a variety of tasks with little to no specialized training. This
technology creates various opportunities for applications in the context of
data management.
  The tutorial will introduce participants to basic background on language
models, discuss different methods to use language models, and give an overview
and short demonstration of available libraries and APIs. Models for generating
natural language will be considered as well as models, such as GPT-3 Codex,
which complete program code or generate code from natural language
instructions. Finally, the tutorial will discuss recent research in the
database community that exploits language models in the context of traditional
database systems or proposes novel system architectures that are based on them.
  The tutorial is targeted at database researchers. No prior background on
language models is required. The goal of the tutorial is to introduce database
researchers to the latest generation of language models, and to their use cases
in the domain of data management.","PVLDB 2022, Volume 15, Issue 12, Pages 3770-3773",cs.DB,['cs.DB'],http://arxiv.org/abs/2306.09339v1
134,Reasoning about Procedures with Natural Language Processing: A Tutorial,http://arxiv.org/pdf/2205.07455v1,[arxiv.Result.Author('Li Zhang')],,2022-05-16 05:42:00+00:00,"This tutorial provides a comprehensive and in-depth view of the research on
procedures, primarily in Natural Language Processing. A procedure is a sequence
of steps intended to achieve some goal. Understanding procedures in natural
language has a long history, with recent breakthroughs made possible by
advances in technology. First, we discuss established approaches to collect
procedures, by human annotation or extraction from web resources. Then, we
examine different angles from which procedures can be reasoned about, as well
as ways to represent them. Finally, we enumerate scenarios where procedural
knowledge can be applied to the real world.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2205.07455v1
135,One Model to Rule them all: Multitask and Multilingual Modelling for Lexical Analysis,http://arxiv.org/pdf/1711.01100v1,[arxiv.Result.Author('Johannes Bjerva')],,2017-11-03 10:53:05+00:00,"When learning a new skill, you take advantage of your preexisting skills and
knowledge. For instance, if you are a skilled violinist, you will likely have
an easier time learning to play cello. Similarly, when learning a new language
you take advantage of the languages you already speak. For instance, if your
native language is Norwegian and you decide to learn Dutch, the lexical overlap
between these two languages will likely benefit your rate of language
acquisition. This thesis deals with the intersection of learning multiple tasks
and learning multiple languages in the context of Natural Language Processing
(NLP), which can be defined as the study of computational processing of human
language. Although these two types of learning may seem different on the
surface, we will see that they share many similarities.
  The traditional approach in NLP is to consider a single task for a single
language at a time. However, recent advances allow for broadening this
approach, by considering data for multiple tasks and languages simultaneously.
This is an important approach to explore further as the key to improving the
reliability of NLP, especially for low-resource languages, is to take advantage
of all relevant data whenever possible. In doing so, the hope is that in the
long term, low-resource languages can benefit from the advances made in NLP
which are currently to a large extent reserved for high-resource languages.
This, in turn, may then have positive consequences for, e.g., language
preservation, as speakers of minority languages will have a lower degree of
pressure to using high-resource languages. In the short term, answering the
specific research questions posed should be of use to NLP researchers working
towards the same goal.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1711.01100v1
136,"Sejarah dan Perkembangan Teknik Natural Language Processing (NLP) Bahasa Indonesia: Tinjauan tentang sejarah, perkembangan teknologi, dan aplikasi NLP dalam bahasa Indonesia",http://arxiv.org/pdf/2304.02746v1,[arxiv.Result.Author('Mukhlis Amien')],,2023-03-28 02:31:47+00:00,"This study provides an overview of the history of the development of Natural
Language Processing (NLP) in the context of the Indonesian language, with a
focus on the basic technologies, methods, and practical applications that have
been developed. This review covers developments in basic NLP technologies such
as stemming, part-of-speech tagging, and related methods; practical
applications in cross-language information retrieval systems, information
extraction, and sentiment analysis; and methods and techniques used in
Indonesian language NLP research, such as machine learning, statistics-based
machine translation, and conflict-based approaches. This study also explores
the application of NLP in Indonesian language industry and research and
identifies challenges and opportunities in Indonesian language NLP research and
development. Recommendations for future Indonesian language NLP research and
development include developing more efficient methods and technologies,
expanding NLP applications, increasing sustainability, further research into
the potential of NLP, and promoting interdisciplinary collaboration. It is
hoped that this review will help researchers, practitioners, and the government
to understand the development of Indonesian language NLP and identify
opportunities for further research and development.",,cs.CL,"['cs.CL', '68T50', 'I.2.7']",http://arxiv.org/abs/2304.02746v1
137,Evolution of Natural Language Processing Technology: Not Just Language Processing Towards General Purpose AI,http://arxiv.org/pdf/2310.06228v1,[arxiv.Result.Author('Masahiro Yamamoto')],,2023-10-10 00:41:38+00:00,"Since the invention of computers, communication through natural language
(actual human language) has been a dream technology. However, natural language
is extremely difficult to mathematically formulate, making it difficult to
realize as an algorithm without considering programming. While there have been
numerous technological developments, one cannot say that any results allowing
free utilization have been achieved thus far. In the case of language learning
in humans, for instance when learning one's mother tongue or foreign language,
one must admit that this process is similar to the adage ""practice makes
perfect"" in principle, even though the learning method is significant up to a
point. Deep learning has played a central role in contemporary AI technology in
recent years. When applied to natural language processing (NLP), this produced
unprecedented results. Achievements exceeding the initial predictions have been
reported from the results of learning vast amounts of textual data using deep
learning. For instance, four arithmetic operations could be performed without
explicit learning, thereby enabling the explanation of complex images and the
generation of images from corresponding explanatory texts. It is an accurate
example of the learner embodying the concept of ""practice makes perfect"" by
using vast amounts of textual data. This report provides a technological
explanation of how cutting-edge NLP has made it possible to realize the
""practice makes perfect"" principle. Additionally, examples of how this can be
applied to business are provided. We reported in June 2022 in Japanese on the
NLP movement from late 2021 to early 2022. We would like to summarize this as a
memorandum since this is just the initial movement leading to the current large
language models (LLMs).",,cs.CL,"['cs.CL', 'cs.AI', 'I.2.7']",http://arxiv.org/abs/2310.06228v1
138,The language (and series) of Hammersley-type processes,http://arxiv.org/pdf/1802.03436v2,"[arxiv.Result.Author('Cosmin Bonchis'), arxiv.Result.Author('Gabriel Istrate'), arxiv.Result.Author('Vlad Rochian')]",,2018-02-09 20:04:12+00:00,"We study languages and formal power series associated to (variants of)
Hammersley's process. We show that the ordinary Hammersley process yields a
regular language and the Hammersley tree process yields deterministic
context-free (but non-regular) languages. For the extension to intervals of the
Hammersley process we show that there are two relevant formal languages. One of
them leads to the same class of languages as the ordinary Hammersley tree
process. The other one yields non-context-free languages. The results are
motivated by the problem of studying the analog of the famous Ulam-Hammersley
problem for heapable sequences. Towards this goal we also give an algorithm for
computing formal power series associated to the variants of Hammersley's
process. We employ these algorithms to settle the nature of the scaling
constant, conjectured in previous work to be the golden ratio. Our results
provide experimental support to this conjecture.",,cs.FL,"['cs.FL', 'cs.DM', 'math.CO', 'math.PR']",http://arxiv.org/abs/1802.03436v2
139,LINDA: Unsupervised Learning to Interpolate in Natural Language Processing,http://arxiv.org/pdf/2112.13969v1,"[arxiv.Result.Author('Yekyung Kim'), arxiv.Result.Author('Seohyeong Jeong'), arxiv.Result.Author('Kyunghyun Cho')]",,2021-12-28 02:56:41+00:00,"Despite the success of mixup in data augmentation, its applicability to
natural language processing (NLP) tasks has been limited due to the discrete
and variable-length nature of natural languages. Recent studies have thus
relied on domain-specific heuristics and manually crafted resources, such as
dictionaries, in order to apply mixup in NLP. In this paper, we instead propose
an unsupervised learning approach to text interpolation for the purpose of data
augmentation, to which we refer as ""Learning to INterpolate for Data
Augmentation"" (LINDA), that does not require any heuristics nor manually
crafted resources but learns to interpolate between any pair of natural
language sentences over a natural language manifold. After empirically
demonstrating the LINDA's interpolation capability, we show that LINDA indeed
allows us to seamlessly apply mixup in NLP and leads to better generalization
in text classification both in-domain and out-of-domain.",,cs.CL,"['cs.CL', 'cs.LG']",http://arxiv.org/abs/2112.13969v1
140,SBNet: Segmentation-based Network for Natural Language-based Vehicle Search,http://arxiv.org/pdf/2104.11589v1,"[arxiv.Result.Author('Sangrok Lee'), arxiv.Result.Author('Taekang Woo'), arxiv.Result.Author('Sang Hun Lee')]",10.1109/CVPRW53098.2021.00457,2021-04-22 08:06:17+00:00,"Natural language-based vehicle retrieval is a task to find a target vehicle
within a given image based on a natural language description as a query. This
technology can be applied to various areas including police searching for a
suspect vehicle. However, it is challenging due to the ambiguity of language
descriptions and the difficulty of processing multi-modal data. To tackle this
problem, we propose a deep neural network called SBNet that performs natural
language-based segmentation for vehicle retrieval. We also propose two
task-specific modules to improve performance: a substitution module that helps
features from different domains to be embedded in the same space and a future
prediction module that learns temporal information. SBnet has been trained
using the CityFlow-NL dataset that contains 2,498 tracks of vehicles with three
unique natural language descriptions each and tested 530 unique vehicle tracks
and their corresponding query sets. SBNet achieved a significant improvement
over the baseline in the natural language-based vehicle tracking track in the
AI City Challenge 2021.","2021 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition Workshops (CVPRW), pp. 4049-4055",cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'I.2.10; I.5.1; I.4.8']",http://arxiv.org/abs/2104.11589v1
141,Emergence of Numeric Concepts in Multi-Agent Autonomous Communication,http://arxiv.org/pdf/1911.01098v1,[arxiv.Result.Author('Shangmin Guo')],,2019-11-04 09:58:23+00:00,"With the rapid development of deep learning, most of current state-of-the-art
techniques in natural langauge processing are based on deep learning models
trained with argescaled static textual corpora. However, we human beings learn
and understand in a different way. Thus, grounded language learning argues that
models need to learn and understand language by the experience and perceptions
obtained by interacting with enviroments, like how humans do. With the help of
deep reinforcement learning techniques, there are already lots of works
focusing on facilitating the emergence of communication protocols that have
compositionalities like natural languages among computational agents
population. Unlike these works, we, on the other hand, focus on the numeric
concepts which correspond to abstractions in cognition and function words in
natural language. Based on a specifically designed language game, we verify
that computational agents are capable of transmitting numeric concepts during
autonomous communication, and the emergent communication protocols can reflect
the underlying structure of meaning space. Although their encodeing method is
not compositional like natural languages from a perspective of human beings,
the emergent languages can be generalised to unseen inputs and, more
importantly, are easier for models to learn. Besides, iterated learning can
help further improving the compositionality of the emergent languages, under
the measurement of topological similarity. Furthermore, we experiment another
representation method, i.e. directly encode numerals into concatenations of
one-hot vectors, and find that the emergent languages would become
compositional like human natural languages. Thus, we argue that there are 2
important factors for the emergence of compositional languages.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1911.01098v1
142,Natural Language is All a Graph Needs,http://arxiv.org/pdf/2308.07134v3,"[arxiv.Result.Author('Ruosong Ye'), arxiv.Result.Author('Caiqi Zhang'), arxiv.Result.Author('Runhui Wang'), arxiv.Result.Author('Shuyuan Xu'), arxiv.Result.Author('Yongfeng Zhang')]",,2023-08-14 13:41:09+00:00,"The emergence of large-scale pre-trained language models, such as ChatGPT,
has revolutionized various research fields in artificial intelligence.
Transformers-based large language models (LLMs) have gradually replaced CNNs
and RNNs to unify fields of computer vision and natural language processing.
Compared with the data that exists relatively independently such as images,
videos or texts, graph is a type of data that contains rich structural and
relational information. Meanwhile, natural language, as one of the most
expressive mediums, excels in describing complex structures. However, existing
work on incorporating graph learning problems into the generative language
modeling framework remains very limited. As the importance of large language
models continues to grow, it becomes essential to explore whether LLMs can also
replace GNNs as the foundation model for graphs. In this paper, we propose
InstructGLM (Instruction-finetuned Graph Language Model), systematically design
highly scalable prompts based on natural language instructions, and use natural
language to describe the geometric structure and node features of the graph for
instruction tuning an LLM to perform learning and inference on graphs in a
generative manner. Our method exceeds all competitive GNN baselines on
ogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of
our method and sheds light on generative large language models as the
foundation model for graph machine learning.",,cs.CL,"['cs.CL', 'cs.AI', 'cs.IR', 'cs.LG']",http://arxiv.org/abs/2308.07134v3
143,Language-Family Adapters for Low-Resource Multilingual Neural Machine Translation,http://arxiv.org/pdf/2209.15236v3,"[arxiv.Result.Author('Alexandra Chronopoulou'), arxiv.Result.Author('Dario Stojanovski'), arxiv.Result.Author('Alexander Fraser')]",,2022-09-30 05:02:42+00:00,"Large multilingual models trained with self-supervision achieve
state-of-the-art results in a wide range of natural language processing tasks.
Self-supervised pretrained models are often fine-tuned on parallel data from
one or multiple language pairs for machine translation. Multilingual
fine-tuning improves performance on low-resource languages but requires
modifying the entire model and can be prohibitively expensive. Training a new
adapter on each language pair or training a single adapter on all language
pairs without updating the pretrained model has been proposed as a
parameter-efficient alternative. However, the former does not permit any
sharing between languages, while the latter shares parameters for all languages
and is susceptible to negative interference. In this paper, we propose training
language-family adapters on top of mBART-50 to facilitate cross-lingual
transfer. Our approach outperforms related baselines, yielding higher
translation scores on average when translating from English to 17 different
low-resource languages. We also show that language-family adapters provide an
effective method to translate to languages unseen during pretraining.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2209.15236v3
144,NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language Selection for Low-Resource Multilingual Sentiment Analysis,http://arxiv.org/pdf/2305.00090v1,"[arxiv.Result.Author('Mingyang Wang'), arxiv.Result.Author('Heike Adel'), arxiv.Result.Author('Lukas Lange'), arxiv.Result.Author('Jannik Strötgen'), arxiv.Result.Author('Hinrich Schütze')]",,2023-04-28 21:02:58+00:00,"This paper describes our system developed for the SemEval-2023 Task 12
""Sentiment Analysis for Low-resource African Languages using Twitter Dataset"".
Sentiment analysis is one of the most widely studied applications in natural
language processing. However, most prior work still focuses on a small number
of high-resource languages. Building reliable sentiment analysis systems for
low-resource languages remains challenging, due to the limited training data in
this task. In this work, we propose to leverage language-adaptive and
task-adaptive pretraining on African texts and study transfer learning with
source language selection on top of an African language-centric pretrained
language model. Our key findings are: (1) Adapting the pretrained model to the
target language and task using a small yet relevant corpus improves performance
remarkably by more than 10 F1 score points. (2) Selecting source languages with
positive transfer gains during training can avoid harmful interference from
dissimilar languages, leading to better results in multilingual and
cross-lingual settings. In the shared task, our system wins 8 out of 15 tracks
and, in particular, performs best in the multilingual evaluation.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2305.00090v1
145,"LIMIT: Language Identification, Misidentification, and Translation using Hierarchical Models in 350+ Languages",http://arxiv.org/pdf/2305.14263v1,"[arxiv.Result.Author('Milind Agarwal'), arxiv.Result.Author('Md Mahfuz Ibn Alam'), arxiv.Result.Author('Antonios Anastasopoulos')]",,2023-05-23 17:15:43+00:00,"Knowing the language of an input text/audio is a necessary first step for
using almost every natural language processing (NLP) tool such as taggers,
parsers, or translation systems. Language identification is a well-studied
problem, sometimes even considered solved; in reality, most of the world's 7000
languages are not supported by current systems. This lack of representation
affects large-scale data mining efforts and further exacerbates data shortage
for low-resource languages. We take a step towards tackling the data bottleneck
by compiling a corpus of over 50K parallel children's stories in 350+ languages
and dialects, and the computation bottleneck by building lightweight
hierarchical models for language identification. Our data can serve as
benchmark data for language identification of short texts and for understudied
translation directions such as those between Indian or African languages. Our
proposed method, Hierarchical LIMIT, uses limited computation to expand
coverage into excluded languages while maintaining prediction quality.",,cs.CL,"['cs.CL', 'cs.AI']",http://arxiv.org/abs/2305.14263v1
146,An implementation of Apertium based Assamese morphological analyzer,http://arxiv.org/pdf/1503.03989v1,"[arxiv.Result.Author('Mirzanur Rahman'), arxiv.Result.Author('Shikhar Kumar Sarma')]",,2015-03-13 09:03:21+00:00,"Morphological Analysis is an important branch of linguistics for any Natural
Language Processing Technology. Morphology studies the word structure and
formation of word of a language. In current scenario of NLP research,
morphological analysis techniques have become more popular day by day. For
processing any language, morphology of the word should be first analyzed.
Assamese language contains very complex morphological structure. In our work we
have used Apertium based Finite-State-Transducers for developing morphological
analyzer for Assamese Language with some limited domain and we get 72.7%
accuracy",,cs.CL,['cs.CL'],http://arxiv.org/abs/1503.03989v1
147,Stemmers for Tamil Language: Performance Analysis,http://arxiv.org/pdf/1310.0754v1,"[arxiv.Result.Author('M. Thangarasu'), arxiv.Result.Author('R. Manavalan')]",,2013-10-02 16:23:00+00:00,"Stemming is the process of extracting root word from the given inflection
word and also plays significant role in numerous application of Natural
Language Processing (NLP). Tamil Language raises several challenges to NLP,
since it has rich morphological patterns than other languages. The rule based
approach light-stemmer is proposed in this paper, to find stem word for given
inflection Tamil word. The performance of proposed approach is compared to a
rule based suffix removal stemmer based on correctly and incorrectly predicted.
The experimental result clearly show that the proposed approach light stemmer
for Tamil language perform better than suffix removal stemmer and also more
effective in Information Retrieval System (IRS).","International Journal of Computer Science & Engineering
  Technology, Vol. 4, No. 07, Jul 2013",cs.CL,['cs.CL'],http://arxiv.org/abs/1310.0754v1
148,BNLP: Natural language processing toolkit for Bengali language,http://arxiv.org/pdf/2102.00405v2,[arxiv.Result.Author('Sagor Sarker')],,2021-01-31 07:56:08+00:00,"BNLP is an open source language processing toolkit for Bengali language
consisting with tokenization, word embedding, POS tagging, NER tagging
facilities. BNLP provides pre-trained model with high accuracy to do model
based tokenization, embedding, POS tagging, NER tagging task for Bengali
language. BNLP pre-trained model achieves significant results in Bengali text
tokenization, word embedding, POS tagging and NER tagging task. BNLP is using
widely in the Bengali research communities with 16K downloads, 119 stars and 31
forks. BNLP is available at https://github.com/sagorbrur/bnlp.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2102.00405v2
149,SwissBERT: The Multilingual Language Model for Switzerland,http://arxiv.org/pdf/2303.13310v2,"[arxiv.Result.Author('Jannis Vamvas'), arxiv.Result.Author('Johannes Graën'), arxiv.Result.Author('Rico Sennrich')]",,2023-03-23 14:44:47+00:00,"We present SwissBERT, a masked language model created specifically for
processing Switzerland-related text. SwissBERT is a pre-trained model that we
adapted to news articles written in the national languages of Switzerland --
German, French, Italian, and Romansh. We evaluate SwissBERT on natural language
understanding tasks related to Switzerland and find that it tends to outperform
previous models on these tasks, especially when processing contemporary news
and/or Romansh Grischun. Since SwissBERT uses language adapters, it may be
extended to Swiss German dialects in future work. The model and our open-source
code are publicly released at https://github.com/ZurichNLP/swissbert.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2303.13310v2
150,Analyzing the Surprising Variability in Word Embedding Stability Across Languages,http://arxiv.org/pdf/2004.14876v2,"[arxiv.Result.Author('Laura Burdick'), arxiv.Result.Author('Jonathan K. Kummerfeld'), arxiv.Result.Author('Rada Mihalcea')]",,2020-04-30 15:24:43+00:00,"Word embeddings are powerful representations that form the foundation of many
natural language processing architectures, both in English and in other
languages. To gain further insight into word embeddings, we explore their
stability (e.g., overlap between the nearest neighbors of a word in different
embedding spaces) in diverse languages. We discuss linguistic properties that
are related to stability, drawing out insights about correlations with
affixing, language gender systems, and other features. This has implications
for embedding use, particularly in research that uses them to study language
trends.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2004.14876v2
151,Differentially Private Language Models Benefit from Public Pre-training,http://arxiv.org/pdf/2009.05886v2,"[arxiv.Result.Author('Gavin Kerrigan'), arxiv.Result.Author('Dylan Slack'), arxiv.Result.Author('Jens Tuyls')]",,2020-09-13 00:50:44+00:00,"Language modeling is a keystone task in natural language processing. When
training a language model on sensitive information, differential privacy (DP)
allows us to quantify the degree to which our private data is protected.
However, training algorithms which enforce differential privacy often lead to
degradation in model quality. We study the feasibility of learning a language
model which is simultaneously high-quality and privacy preserving by tuning a
public base model on a private corpus. We find that DP fine-tuning boosts the
performance of language models in the private domain, making the training of
such models possible.",,cs.LG,"['cs.LG', 'cs.CL', 'cs.CR']",http://arxiv.org/abs/2009.05886v2
152,Towards a Deep Multi-layered Dialectal Language Analysis: A Case Study of African-American English,http://arxiv.org/pdf/2206.08978v1,[arxiv.Result.Author('Jamell Dacon')],,2022-06-03 01:05:58+00:00,"Currently, natural language processing (NLP) models proliferate language
discrimination leading to potentially harmful societal impacts as a result of
biased outcomes. For example, part-of-speech taggers trained on Mainstream
American English (MAE) produce non-interpretable results when applied to
African American English (AAE) as a result of language features not seen during
training. In this work, we incorporate a human-in-the-loop paradigm to gain a
better understanding of AAE speakers' behavior and their language use, and
highlight the need for dialectal language inclusivity so that native AAE
speakers can extensively interact with NLP systems while reducing feelings of
disenfranchisement.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2206.08978v1
153,Improved Text Language Identification for the South African Languages,http://arxiv.org/pdf/1711.00247v1,"[arxiv.Result.Author('Bernardt Duvenhage'), arxiv.Result.Author('Mfundo Ntini'), arxiv.Result.Author('Phala Ramonyai')]",,2017-11-01 08:27:46+00:00,"Virtual assistants and text chatbots have recently been gaining popularity.
Given the short message nature of text-based chat interactions, the language
identification systems of these bots might only have 15 or 20 characters to
make a prediction. However, accurate text language identification is important,
especially in the early stages of many multilingual natural language processing
pipelines.
  This paper investigates the use of a naive Bayes classifier, to accurately
predict the language family that a piece of text belongs to, combined with a
lexicon based classifier to distinguish the specific South African language
that the text is written in. This approach leads to a 31% reduction in the
language detection error.
  In the spirit of reproducible research the training and testing datasets as
well as the code are published on github. Hopefully it will be useful to create
a text language identification shared task for South African languages.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1711.00247v1
154,Extracting Weighted Finite Automata from Recurrent Neural Networks for Natural Languages,http://arxiv.org/pdf/2206.14621v2,"[arxiv.Result.Author('Zeming Wei'), arxiv.Result.Author('Xiyue Zhang'), arxiv.Result.Author('Meng Sun')]",,2022-06-27 09:30:13+00:00,"Recurrent Neural Networks (RNNs) have achieved tremendous success in
sequential data processing. However, it is quite challenging to interpret and
verify RNNs' behaviors directly. To this end, many efforts have been made to
extract finite automata from RNNs. Existing approaches such as exact learning
are effective in extracting finite-state models to characterize the state
dynamics of RNNs for formal languages, but are limited in the scalability to
process natural languages. Compositional approaches that are scablable to
natural languages fall short in extraction precision. In this paper, we
identify the transition sparsity problem that heavily impacts the extraction
precision. To address this problem, we propose a transition rule extraction
approach, which is scalable to natural language processing models and effective
in improving extraction precision. Specifically, we propose an empirical method
to complement the missing rules in the transition diagram. In addition, we
further adjust the transition matrices to enhance the context-aware ability of
the extracted weighted finite automaton (WFA). Finally, we propose two data
augmentation tactics to track more dynamic behaviors of the target RNN.
Experiments on two popular natural language datasets show that our method can
extract WFA from RNN for natural language processing with better precision than
existing approaches. Our code is available at
https://github.com/weizeming/Extract_WFA_from_RNN_for_NL.",,cs.CL,"['cs.CL', 'cs.LG']",http://arxiv.org/abs/2206.14621v2
155,MyProLang - My Programming Language: A Template-Driven Automatic Natural Programming Language,http://arxiv.org/pdf/1204.0221v1,"[arxiv.Result.Author('Youssef Bassil'), arxiv.Result.Author('Aziz Barbar')]",,2012-04-01 15:35:11+00:00,"Modern computer programming languages are governed by complex syntactic
rules. They are unlike natural languages; they require extensive manual work
and a significant amount of learning and practicing for an individual to become
skilled at and to write correct programs. Computer programming is a difficult,
complicated, unfamiliar, non-automated, and a challenging discipline for
everyone; especially, for students, new programmers and end-users. This paper
proposes a new programming language and an environment for writing computer
applications based on source-code generation. It is mainly a template-driven
automatic natural imperative programming language called MyProLang. It
harnesses GUI templates to generate proprietary natural language source-code,
instead of having computer programmers write the code manually. MyProLang is a
blend of five elements. A proprietary natural programming language with
unsophisticated grammatical rules and expressive syntax; automation templates
that automate the generation of instructions and thereby minimizing the
learning and training time; an NLG engine to generate natural instructions; a
source-to-source compiler that analyzes, parses, and build executables; and an
ergonomic IDE that houses diverse functions whose role is to simplify the
software development process. MyProLang is expected to make programming open to
everyone including students, programmers and end-users. In that sense, anyone
can start programming systematically, in an automated manner and in natural
language; without wasting time in learning how to formulate instructions and
arrange expressions, without putting up with unfamiliar structures and symbols,
and without being annoyed by syntax errors. In the long run, this increases the
productivity, quality and time-to-market in software development.",,cs.PL,['cs.PL'],http://arxiv.org/abs/1204.0221v1
156,Towards More Robust Natural Language Understanding,http://arxiv.org/pdf/2112.02992v2,[arxiv.Result.Author('Xinliang Frederick Zhang')],,2021-12-01 17:27:19+00:00,"Natural Language Understanding (NLU) is a branch of Natural Language
Processing (NLP) that uses intelligent computer software to understand texts
that encode human knowledge. Recent years have witnessed notable progress
across various NLU tasks with deep learning techniques, especially with
pretrained language models. Besides proposing more advanced model
architectures, constructing more reliable and trustworthy datasets also plays a
huge role in improving NLU systems, without which it would be impossible to
train a decent NLU model. It's worth noting that the human ability of
understanding natural language is flexible and robust. On the contrary, most of
existing NLU systems fail to achieve desirable performance on out-of-domain
data or struggle on handling challenging items (e.g., inherently ambiguous
items, adversarial items) in the real world. Therefore, in order to have NLU
models understand human language more effectively, it is expected to prioritize
the study on robust natural language understanding. In this thesis, we deem
that NLU systems are consisting of two components: NLU models and NLU datasets.
As such, we argue that, to achieve robust NLU, the model architecture/training
and the dataset are equally important. Specifically, we will focus on three NLU
tasks to illustrate the robustness problem in different NLU tasks and our
contributions (i.e., novel models and new datasets) to help achieve more robust
natural language understanding. Moving forward, the ultimate goal for robust
natural language understanding is to build NLU models which can behave humanly.
That is, it's expected that robust NLU systems are capable to transfer the
knowledge from training corpus to unseen documents more reliably and survive
when encountering challenging items even if the system doesn't know a priori of
users' inputs.",,cs.CL,"['cs.CL', 'cs.AI']",http://arxiv.org/abs/2112.02992v2
157,SufiSent - Universal Sentence Representations Using Suffix Encodings,http://arxiv.org/pdf/1802.07370v1,[arxiv.Result.Author('Siddhartha Brahma')],,2018-02-20 23:08:19+00:00,"Computing universal distributed representations of sentences is a fundamental
task in natural language processing. We propose a method to learn such
representations by encoding the suffixes of word sequences in a sentence and
training on the Stanford Natural Language Inference (SNLI) dataset. We
demonstrate the effectiveness of our approach by evaluating it on the SentEval
benchmark, improving on existing approaches on several transfer tasks.",,cs.CL,"['cs.CL', 'cs.AI']",http://arxiv.org/abs/1802.07370v1
158,Integrated speech and morphological processing in a connectionist continuous speech understanding for Korean,http://arxiv.org/pdf/cmp-lg/9603005v1,"[arxiv.Result.Author('Geunbae Lee'), arxiv.Result.Author('Jong-Hyeok Lee')]",,1996-03-18 06:59:58+00:00,"A new tightly coupled speech and natural language integration model is
presented for a TDNN-based continuous possibly large vocabulary speech
recognition system for Korean. Unlike popular n-best techniques developed for
integrating mainly HMM-based speech recognition and natural language processing
in a {\em word level}, which is obviously inadequate for morphologically
complex agglutinative languages, our model constructs a spoken language system
based on a {\em morpheme-level} speech and language integration. With this
integration scheme, the spoken Korean processing engine (SKOPE) is designed and
implemented using a TDNN-based diphone recognition module integrated with a
Viterbi-based lexical decoding and symbolic phonological/morphological
co-analysis. Our experiment results show that the speaker-dependent continuous
{\em eojeol} (Korean word) recognition and integrated morphological analysis
can be achieved with over 80.6% success rate directly from speech inputs for
the middle-level vocabularies.",,cmp-lg,"['cmp-lg', 'cs.CL']",http://arxiv.org/abs/cmp-lg/9603005v1
159,Generalized Optimal Linear Orders,http://arxiv.org/pdf/2108.10692v1,[arxiv.Result.Author('Rishi Bommasani')],10.7298/5x0j-me63,2021-08-13 13:10:15+00:00,"The sequential structure of language, and the order of words in a sentence
specifically, plays a central role in human language processing. Consequently,
in designing computational models of language, the de facto approach is to
present sentences to machines with the words ordered in the same order as in
the original human-authored sentence. The very essence of this work is to
question the implicit assumption that this is desirable and inject theoretical
soundness into the consideration of word order in natural language processing.
In this thesis, we begin by uniting the disparate treatments of word order in
cognitive science, psycholinguistics, computational linguistics, and natural
language processing under a flexible algorithmic framework. We proceed to use
this heterogeneous theoretical foundation as the basis for exploring new word
orders with an undercurrent of psycholinguistic optimality. In particular, we
focus on notions of dependency length minimization given the difficulties in
human and computational language processing in handling long-distance
dependencies. We then discuss algorithms for finding optimal word orders
efficiently in spite of the combinatorial space of possibilities. We conclude
by addressing the implications of these word orders on human language and their
downstream impacts when integrated in computational models.",,cs.CL,"['cs.CL', 'cs.LG']",http://arxiv.org/abs/2108.10692v1
160,A Chomsky-Schützenberger representation for weighted multiple context-free languages,http://arxiv.org/pdf/1606.03982v2,[arxiv.Result.Author('Tobias Denkinger')],,2016-06-13 14:55:53+00:00,"We prove a Chomsky-Sch\""utzenberger representation theorem for multiple
context-free languages weighted over complete commutative strong bimonoids.",,cs.FL,['cs.FL'],http://arxiv.org/abs/1606.03982v2
161,Error Analysis for Vietnamese Dependency Parsing,http://arxiv.org/pdf/1911.03724v1,"[arxiv.Result.Author('Kiet Van Nguyen'), arxiv.Result.Author('Ngan Luu-Thuy Nguyen')]",,2019-11-09 16:00:26+00:00,"Dependency parsing is needed in different applications of natural language
processing. In this paper, we present a thorough error analysis for dependency
parsing for the Vietnamese language, using two state-of-the-art parsers:
MSTParser and MaltParser. The error analysis results provide us insights in
order to improve the performance of dependency parsing for the Vietnamese
language.","2015 Seventh International Conference on Knowledge and Systems
  Engineering (KSE)",cs.CL,['cs.CL'],http://arxiv.org/abs/1911.03724v1
162,Functorial Language Games for Question Answering,http://arxiv.org/pdf/2005.09439v2,"[arxiv.Result.Author('Giovanni de Felice'), arxiv.Result.Author('Elena Di Lavore'), arxiv.Result.Author('Mario Román'), arxiv.Result.Author('Alexis Toumi')]",10.4204/EPTCS.333.21,2020-05-19 13:35:13+00:00,"We present some categorical investigations into Wittgenstein's
language-games, with applications to game-theoretic pragmatics and
question-answering in natural language processing.","EPTCS 333, 2021, pp. 311-321",cs.CL,"['cs.CL', 'cs.GT']",http://arxiv.org/abs/2005.09439v2
163,Bilingual Lexicon Induction for Low-Resource Languages using Graph Matching via Optimal Transport,http://arxiv.org/pdf/2210.14378v1,"[arxiv.Result.Author('Kelly Marchisio'), arxiv.Result.Author('Ali Saad-Eldin'), arxiv.Result.Author('Kevin Duh'), arxiv.Result.Author('Carey Priebe'), arxiv.Result.Author('Philipp Koehn')]",,2022-10-25 23:09:20+00:00,"Bilingual lexicons form a critical component of various natural language
processing applications, including unsupervised and semisupervised machine
translation and crosslingual information retrieval. We improve bilingual
lexicon induction performance across 40 language pairs with a graph-matching
method based on optimal transport. The method is especially strong with low
amounts of supervision.",,cs.CL,"['cs.CL', 'cs.LG']",http://arxiv.org/abs/2210.14378v1
164,Du TAL au TIL,http://arxiv.org/pdf/1201.4733v1,"[arxiv.Result.Author('Michael Zock'), arxiv.Result.Author('Guy Lapalme')]",,2012-01-20 17:35:29+00:00,"Historically two types of NLP have been investigated: fully automated
processing of language by machines (NLP) and autonomous processing of natural
language by people, i.e. the human brain (psycholinguistics). We believe that
there is room and need for another kind, INLP: interactive natural language
processing. This intermediate approach starts from peoples' needs, trying to
bridge the gap between their actual knowledge and a given goal. Given the fact
that peoples' knowledge is variable and often incomplete, the aim is to build
bridges linking a given knowledge state to a given goal. We present some
examples, trying to show that this goal is worth pursuing, achievable and at a
reasonable cost.",,cs.CL,"['cs.CL', 'cs.HC']",http://arxiv.org/abs/1201.4733v1
165,Deep Learning Based Natural Language Processing for End to End Speech Translation,http://arxiv.org/pdf/1808.04459v1,[arxiv.Result.Author('Sarvesh Patil')],,2018-08-09 14:21:35+00:00,"Deep Learning methods employ multiple processing layers to learn hierarchial
representations of data. They have already been deployed in a humongous number
of applications and have produced state-of-the-art results. Recently with the
growth in processing power of computers to be able to do high dimensional
tensor calculations, Natural Language Processing (NLP) applications have been
given a significant boost in terms of efficiency as well as accuracy. In this
paper, we will take a look at various signal processing techniques and then
application of them to produce a speech-to-text system using Deep Recurrent
Neural Networks.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1808.04459v1
166,CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex,http://arxiv.org/pdf/2204.08941v1,[arxiv.Result.Author('Immanuel Trummer')],,2022-04-19 15:19:35+00:00,"CodexDB is an SQL processing engine whose internals can be customized via
natural language instructions. CodexDB is based on OpenAI's GPT-3 Codex model
which translates text into code. It is a framework on top of GPT-3 Codex that
decomposes complex SQL queries into a series of simple processing steps,
described in natural language. Processing steps are enriched with user-provided
instructions and descriptions of database properties. Codex translates the
resulting text into query processing code. An early prototype of CodexDB is
able to generate correct code for a majority of queries of the WikiSQL
benchmark and can be customized in various ways.",,cs.DB,"['cs.DB', 'cs.CL', 'cs.LG', 'H.2.4']",http://arxiv.org/abs/2204.08941v1
167,Modelling Word Burstiness in Natural Language: A Generalised Polya Process for Document Language Models in Information Retrieval,http://arxiv.org/pdf/1708.06011v1,[arxiv.Result.Author('Ronan Cummins')],,2017-08-20 19:41:58+00:00,"We introduce a generalised multivariate Polya process for document language
modelling. The framework outlined here generalises a number of statistical
language models used in information retrieval for modelling document
generation. In particular, we show that the choice of replacement matrix M
ultimately defines the type of random process and therefore defines a
particular type of document language model. We show that a particular variant
of the general model is useful for modelling term-specific burstiness.
Furthermore, via experimentation we show that this variant significantly
improves retrieval effectiveness over a strong baseline on a number of small
test collections.",,cs.IR,['cs.IR'],http://arxiv.org/abs/1708.06011v1
168,"Computational Language Assessment in patients with speech, language, and communication impairments",http://arxiv.org/pdf/2305.20046v1,[arxiv.Result.Author('Charalambos Themistocleous')],,2023-05-31 17:20:45+00:00,"Speech, language, and communication symptoms enable the early detection,
diagnosis, treatment planning, and monitoring of neurocognitive disease
progression. Nevertheless, traditional manual neurologic assessment, the speech
and language evaluation standard, is time-consuming and resource-intensive for
clinicians. We argue that Computational Language Assessment (C.L.A.) is an
improvement over conventional manual neurological assessment. Using machine
learning, natural language processing, and signal processing, C.L.A. provides a
neuro-cognitive evaluation of speech, language, and communication in elderly
and high-risk individuals for dementia. ii. facilitates the diagnosis,
prognosis, and therapy efficacy in at-risk and language-impaired populations;
and iii. allows easier extensibility to assess patients from a wide range of
languages. Also, C.L.A. employs Artificial Intelligence models to inform theory
on the relationship between language symptoms and their neural bases. It
significantly advances our ability to optimize the prevention and treatment of
elderly individuals with communication disorders, allowing them to age
gracefully with social engagement.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2305.20046v1
169,InstructProtein: Aligning Human and Protein Language via Knowledge Instruction,http://arxiv.org/pdf/2310.03269v1,"[arxiv.Result.Author('Zeyuan Wang'), arxiv.Result.Author('Qiang Zhang'), arxiv.Result.Author('Keyan Ding'), arxiv.Result.Author('Ming Qin'), arxiv.Result.Author('Xiang Zhuang'), arxiv.Result.Author('Xiaotong Li'), arxiv.Result.Author('Huajun Chen')]",,2023-10-05 02:45:39+00:00,"Large Language Models (LLMs) have revolutionized the field of natural
language processing, but they fall short in comprehending biological sequences
such as proteins. To address this challenge, we propose InstructProtein, an
innovative LLM that possesses bidirectional generation capabilities in both
human and protein languages: (i) taking a protein sequence as input to predict
its textual function description and (ii) using natural language to prompt
protein sequence generation. To achieve this, we first pre-train an LLM on both
protein and natural language corpora, enabling it to comprehend individual
languages. Then supervised instruction tuning is employed to facilitate the
alignment of these two distinct languages. Herein, we introduce a knowledge
graph-based instruction generation framework to construct a high-quality
instruction dataset, addressing annotation imbalance and instruction deficits
in existing protein-text corpus. In particular, the instructions inherit the
structural relations between proteins and function annotations in knowledge
graphs, which empowers our model to engage in the causal modeling of protein
functions, akin to the chain-of-thought processes in natural languages.
Extensive experiments on bidirectional protein-text generation tasks show that
InstructProtein outperforms state-of-the-art LLMs by large margins. Moreover,
InstructProtein serves as a pioneering step towards text-based protein function
prediction and sequence design, effectively bridging the gap between protein
and human language understanding.",,q-bio.BM,"['q-bio.BM', 'cs.CL']",http://arxiv.org/abs/2310.03269v1
170,Morphological Processing of Low-Resource Languages: Where We Are and What's Next,http://arxiv.org/pdf/2203.08909v1,"[arxiv.Result.Author('Adam Wiemerslage'), arxiv.Result.Author('Miikka Silfverberg'), arxiv.Result.Author('Changbing Yang'), arxiv.Result.Author('Arya D. McCarthy'), arxiv.Result.Author('Garrett Nicolai'), arxiv.Result.Author('Eliana Colunga'), arxiv.Result.Author('Katharina Kann')]",,2022-03-16 19:47:04+00:00,"Automatic morphological processing can aid downstream natural language
processing applications, especially for low-resource languages, and assist
language documentation efforts for endangered languages. Having long been
multilingual, the field of computational morphology is increasingly moving
towards approaches suitable for languages with minimal or no annotated
resources. First, we survey recent developments in computational morphology
with a focus on low-resource languages. Second, we argue that the field is
ready to tackle the logical next challenge: understanding a language's
morphology from raw text alone. We perform an empirical study on a truly
unsupervised version of the paradigm completion task and show that, while
existing state-of-the-art models bridged by two newly proposed models we devise
perform reasonably, there is still much room for improvement. The stakes are
high: solving this task will increase the language coverage of morphological
resources by a number of magnitudes.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2203.08909v1
171,Modular Mechanistic Networks: On Bridging Mechanistic and Phenomenological Models with Deep Neural Networks in Natural Language Processing,http://arxiv.org/pdf/1807.09844v2,"[arxiv.Result.Author('Simon Dobnik'), arxiv.Result.Author('John D. Kelleher')]",,2018-07-21 11:37:15+00:00,"Natural language processing (NLP) can be done using either top-down (theory
driven) and bottom-up (data driven) approaches, which we call mechanistic and
phenomenological respectively. The approaches are frequently considered to
stand in opposition to each other. Examining some recent approaches in deep
learning we argue that deep neural networks incorporate both perspectives and,
furthermore, that leveraging this aspect of deep learning may help in solving
complex problems within language technology, such as modelling language and
perception in the domain of spatial cognition.","CLASP Papers in Computational Linguistics Vol. 1: Proceedings of
  the Conference on Logic and Machine Learning in Natural Language (LaML 2017).
  ISSN: 2002-9764. URI: http://hdl.handle.net/2077/54911",cs.CL,"['cs.CL', 'cs.AI', 'cs.LG', 'cs.NE', 'stat.ML']",http://arxiv.org/abs/1807.09844v2
172,Natural Language Decomposition and Interpretation of Complex Utterances,http://arxiv.org/pdf/2305.08677v1,"[arxiv.Result.Author('Harsh Jhamtani'), arxiv.Result.Author('Hao Fang'), arxiv.Result.Author('Patrick Xia'), arxiv.Result.Author('Eran Levy'), arxiv.Result.Author('Jacob Andreas'), arxiv.Result.Author('Ben Van Durme')]",,2023-05-15 14:35:00+00:00,"Natural language interfaces often require supervised data to translate user
requests into programs, database queries, or other structured intent
representations. During data collection, it can be difficult to anticipate and
formalize the full range of user needs -- for example, in a system designed to
handle simple requests (like $\textit{find my meetings tomorrow}$ or
$\textit{move my meeting with my manager to noon})$, users may also express
more elaborate requests (like $\textit{swap all my calls on Monday and
Tuesday}$). We introduce an approach for equipping a simple language-to-code
model to handle complex utterances via a process of hierarchical natural
language decomposition. Our approach uses a pre-trained language model to
decompose a complex utterance into a sequence of smaller natural language
steps, then interprets each step using the language-to-code model. To test our
approach, we collect and release DeCU -- a new NL-to-program benchmark to
evaluate Decomposition of Complex Utterances. Experiments show that the
proposed approach enables the interpretation of complex utterances with almost
no complex training data, while outperforming standard few-shot prompting
approaches.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2305.08677v1
173,Leveraging Large Language Models to Generate Answer Set Programs,http://arxiv.org/pdf/2307.07699v1,"[arxiv.Result.Author('Adam Ishay'), arxiv.Result.Author('Zhun Yang'), arxiv.Result.Author('Joohyung Lee')]",,2023-07-15 03:40:55+00:00,"Large language models (LLMs), such as GPT-3 and GPT-4, have demonstrated
exceptional performance in various natural language processing tasks and have
shown the ability to solve certain reasoning problems. However, their reasoning
capabilities are limited and relatively shallow, despite the application of
various prompting techniques. In contrast, formal logic is adept at handling
complex reasoning, but translating natural language descriptions into formal
logic is a challenging task that non-experts struggle with. This paper proposes
a neuro-symbolic method that combines the strengths of large language models
and answer set programming. Specifically, we employ an LLM to transform natural
language descriptions of logic puzzles into answer set programs. We carefully
design prompts for an LLM to convert natural language descriptions into answer
set programs in a step by step manner. Surprisingly, with just a few in-context
learning examples, LLMs can generate reasonably complex answer set programs.
The majority of errors made are relatively simple and can be easily corrected
by humans, thus enabling LLMs to effectively assist in the creation of answer
set programs.",,cs.AI,"['cs.AI', 'cs.CL', 'cs.SC']",http://arxiv.org/abs/2307.07699v1
174,Learning Lexical Entries for Robotic Commands using Crowdsourcing,http://arxiv.org/pdf/1609.02549v3,"[arxiv.Result.Author('Junjie Hu'), arxiv.Result.Author('Jean Oh'), arxiv.Result.Author('Anatole Gershman')]",,2016-09-08 19:51:47+00:00,"Robotic commands in natural language usually contain various spatial
descriptions that are semantically similar but syntactically different. Mapping
such syntactic variants into semantic concepts that can be understood by robots
is challenging due to the high flexibility of natural language expressions. To
tackle this problem, we collect robotic commands for navigation and
manipulation tasks using crowdsourcing. We further define a robot language and
use a generative machine translation model to translate robotic commands from
natural language to robot language. The main purpose of this paper is to
simulate the interaction process between human and robots using crowdsourcing
platforms, and investigate the possibility of translating natural language to
robot language with paraphrases.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1609.02549v3
175,"Listen, Interact and Talk: Learning to Speak via Interaction",http://arxiv.org/pdf/1705.09906v1,"[arxiv.Result.Author('Haichao Zhang'), arxiv.Result.Author('Haonan Yu'), arxiv.Result.Author('Wei Xu')]",,2017-05-28 07:48:14+00:00,"One of the long-term goals of artificial intelligence is to build an agent
that can communicate intelligently with human in natural language. Most
existing work on natural language learning relies heavily on training over a
pre-collected dataset with annotated labels, leading to an agent that
essentially captures the statistics of the fixed external training data. As the
training data is essentially a static snapshot representation of the knowledge
from the annotator, the agent trained this way is limited in adaptiveness and
generalization of its behavior. Moreover, this is very different from the
language learning process of humans, where language is acquired during
communication by taking speaking action and learning from the consequences of
speaking action in an interactive manner. This paper presents an interactive
setting for grounded natural language learning, where an agent learns natural
language by interacting with a teacher and learning from feedback, thus
learning and improving language skills while taking part in the conversation.
To achieve this goal, we propose a model which incorporates both imitation and
reinforcement by leveraging jointly sentence and reward feedbacks from the
teacher. Experiments are conducted to validate the effectiveness of the
proposed approach.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1705.09906v1
176,Natural Language Statistical Features of LSTM-generated Texts,http://arxiv.org/pdf/1804.04087v2,"[arxiv.Result.Author('Marco Lippi'), arxiv.Result.Author('Marcelo A Montemurro'), arxiv.Result.Author('Mirko Degli Esposti'), arxiv.Result.Author('Giampaolo Cristadoro')]",10.1109/TNNLS.2019.2890970,2018-04-10 13:17:36+00:00,"Long Short-Term Memory (LSTM) networks have recently shown remarkable
performance in several tasks dealing with natural language generation, such as
image captioning or poetry composition. Yet, only few works have analyzed text
generated by LSTMs in order to quantitatively evaluate to which extent such
artificial texts resemble those generated by humans. We compared the
statistical structure of LSTM-generated language to that of written natural
language, and to those produced by Markov models of various orders. In
particular, we characterized the statistical structure of language by assessing
word-frequency statistics, long-range correlations, and entropy measures. Our
main finding is that while both LSTM and Markov-generated texts can exhibit
features similar to real ones in their word-frequency statistics and entropy
measures, LSTM-texts are shown to reproduce long-range correlations at scales
comparable to those found in natural language. Moreover, for LSTM networks a
temperature-like parameter controlling the generation process shows an optimal
value---for which the produced texts are closest to real language---consistent
across all the different statistical features investigated.","IEEE Transactions on Neural Networks and Learning Systems, 2019",cs.CL,"['cs.CL', 'cs.LG']",http://arxiv.org/abs/1804.04087v2
177,Spiral Language Modeling,http://arxiv.org/pdf/2112.10543v1,"[arxiv.Result.Author('Yong Cao'), arxiv.Result.Author('Yukun Feng'), arxiv.Result.Author('Shaohui Kuang'), arxiv.Result.Author('Gu Xu')]",,2021-12-20 14:08:38+00:00,"In almost all text generation applications, word sequences are constructed in
a left-to-right (L2R) or right-to-left (R2L) manner, as natural language
sentences are written either L2R or R2L. However, we find that the natural
language written order is not essential for text generation. In this paper, we
propose Spiral Language Modeling (SLM), a general approach that enables one to
construct natural language sentences beyond the L2R and R2L order. SLM allows
one to form natural language text by starting from an arbitrary token inside
the result text and expanding the rest tokens around the selected ones. It
makes the decoding order a new optimization objective besides the language
model perplexity, which further improves the diversity and quality of the
generated text. Furthermore, SLM makes it possible to manipulate the text
construction process by selecting a proper starting token. SLM also introduces
generation orderings as additional regularization to improve model robustness
in low-resource scenarios. Experiments on 8 widely studied Neural Machine
Translation (NMT) tasks show that SLM is constantly effective with up to 4.7
BLEU increase comparing to the conventional L2R decoding approach.",,cs.CL,"['cs.CL', 'cs.AI']",http://arxiv.org/abs/2112.10543v1
178,Do Neural Nets Learn Statistical Laws behind Natural Language?,http://arxiv.org/pdf/1707.04848v2,"[arxiv.Result.Author('Shuntaro Takahashi'), arxiv.Result.Author('Kumiko Tanaka-Ishii')]",10.1371/journal.pone.0189326,2017-07-16 09:08:42+00:00,"The performance of deep learning in natural language processing has been
spectacular, but the reasons for this success remain unclear because of the
inherent complexity of deep learning. This paper provides empirical evidence of
its effectiveness and of a limitation of neural networks for language
engineering. Precisely, we demonstrate that a neural language model based on
long short-term memory (LSTM) effectively reproduces Zipf's law and Heaps' law,
two representative statistical properties underlying natural language. We
discuss the quality of reproducibility and the emergence of Zipf's law and
Heaps' law as training progresses. We also point out that the neural language
model has a limitation in reproducing long-range correlation, another
statistical property of natural language. This understanding could provide a
direction for improving the architectures of neural networks.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1707.04848v2
179,Low-Dimensional Structure in the Space of Language Representations is Reflected in Brain Responses,http://arxiv.org/pdf/2106.05426v4,"[arxiv.Result.Author('Richard Antonello'), arxiv.Result.Author('Javier Turek'), arxiv.Result.Author('Vy Vo'), arxiv.Result.Author('Alexander Huth')]",,2021-06-09 22:59:12+00:00,"How related are the representations learned by neural language models,
translation models, and language tagging tasks? We answer this question by
adapting an encoder-decoder transfer learning method from computer vision to
investigate the structure among 100 different feature spaces extracted from
hidden representations of various networks trained on language tasks. This
method reveals a low-dimensional structure where language models and
translation models smoothly interpolate between word embeddings, syntactic and
semantic tasks, and future word embeddings. We call this low-dimensional
structure a language representation embedding because it encodes the
relationships between representations needed to process language for a variety
of NLP tasks. We find that this representation embedding can predict how well
each individual feature space maps to human brain responses to natural language
stimuli recorded using fMRI. Additionally, we find that the principal dimension
of this structure can be used to create a metric which highlights the brain's
natural language processing hierarchy. This suggests that the embedding
captures some part of the brain's natural language representation structure.",,cs.CL,"['cs.CL', 'cs.LG']",http://arxiv.org/abs/2106.05426v4
180,From Natural Language Instructions to Complex Processes: Issues in Chaining Trigger Action Rules,http://arxiv.org/pdf/2001.02462v1,"[arxiv.Result.Author('Nobuhiro Ito'), arxiv.Result.Author('Yuya Suzuki'), arxiv.Result.Author('Akiko Aizawa')]",,2020-01-08 11:44:47+00:00,"Automation services for complex business processes usually require a high
level of information technology literacy. There is a strong demand for a
smartly assisted process automation (IPA: intelligent process automation)
service that enables even general users to easily use advanced automation. A
natural language interface for such automation is expected as an elemental
technology for the IPA realization. The workflow targeted by IPA is generally
composed of a combination of multiple tasks. However, semantic parsing, one of
the natural language processing methods, for such complex workflows has not yet
been fully studied. The reasons are that (1) the formal expression and grammar
of the workflow required for semantic analysis have not been sufficiently
examined and (2) the dataset of the workflow formal expression with its
corresponding natural language description required for learning workflow
semantics did not exist. This paper defines a new grammar for complex workflows
with chaining machine-executable meaning representations for semantic parsing.
The representations are at a high abstraction level. Additionally, an approach
to creating datasets is proposed based on this grammar.",,cs.AI,"['cs.AI', 'cs.CL']",http://arxiv.org/abs/2001.02462v1
181,"COOL, a Context Outlooker, and its Application to Question Answering and other Natural Language Processing Tasks",http://arxiv.org/pdf/2204.09593v2,"[arxiv.Result.Author('Fangyi Zhu'), arxiv.Result.Author('See-Kiong Ng'), arxiv.Result.Author('Stéphane Bressan')]",,2022-04-01 07:03:40+00:00,"Vision outlooker improves the performance of vision transformers, which
implements a self-attention mechanism by adding an outlook attention, a form of
local attention.
  In natural language processing, as has been the case in computer vision and
other domains, transformer-based models constitute the state-of-the-art for
most processing tasks. In this domain, too, many authors have argued and
demonstrated the importance of local context.
  We present an outlook attention mechanism, COOL, for natural language
processing. COOL, added on top of the self-attention layers of a
transformer-based model, encodes local syntactic context considering word
proximity and more pair-wise constraints than dynamic convolution used by
existing approaches.
  A comparative empirical performance evaluation of an implementation of COOL
with different transformer-based models confirms the opportunity for
improvement over a baseline using the original models alone for various natural
language processing tasks, including question answering. The proposed approach
achieves competitive performance with existing state-of-the-art methods on some
tasks.",,cs.CL,"['cs.CL', 'cs.AI']",http://arxiv.org/abs/2204.09593v2
182,Assessing Language Models with Scaling Properties,http://arxiv.org/pdf/1804.08881v1,"[arxiv.Result.Author('Shuntaro Takahashi'), arxiv.Result.Author('Kumiko Tanaka-Ishii')]",,2018-04-24 07:58:20+00:00,"Language models have primarily been evaluated with perplexity. While
perplexity quantifies the most comprehensible prediction performance, it does
not provide qualitative information on the success or failure of models.
Another approach for evaluating language models is thus proposed, using the
scaling properties of natural language. Five such tests are considered, with
the first two accounting for the vocabulary population and the other three for
the long memory of natural language. The following models were evaluated with
these tests: n-grams, probabilistic context-free grammar (PCFG), Simon and
Pitman-Yor (PY) processes, hierarchical PY, and neural language models. Only
the neural language models exhibit the long memory properties of natural
language, but to a limited degree. The effectiveness of every test of these
models is also discussed.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1804.08881v1
183,ArNLI: Arabic Natural Language Inference for Entailment and Contradiction Detection,http://arxiv.org/pdf/2209.13953v1,"[arxiv.Result.Author('Khloud Al Jallad'), arxiv.Result.Author('Nada Ghneim')]",10.7494/csci.2023.24.2.4378,2022-09-28 09:37:16+00:00,"Natural Language Inference (NLI) is a hot topic research in natural language
processing, contradiction detection between sentences is a special case of NLI.
This is considered a difficult NLP task which has a big influence when added as
a component in many NLP applications, such as Question Answering Systems, text
Summarization. Arabic Language is one of the most challenging low-resources
languages in detecting contradictions due to its rich lexical, semantics
ambiguity. We have created a data set of more than 12k sentences and named
ArNLI, that will be publicly available. Moreover, we have applied a new model
inspired by Stanford contradiction detection proposed solutions on English
language. We proposed an approach to detect contradictions between pairs of
sentences in Arabic language using contradiction vector combined with language
model vector as an input to machine learning model. We analyzed results of
different traditional machine learning classifiers and compared their results
on our created data set (ArNLI) and on an automatic translation of both PHEME,
SICK English data sets. Best results achieved using Random Forest classifier
with an accuracy of 99%, 60%, 75% on PHEME, SICK and ArNLI respectively.","(2023) Computer Science, 24(2)",cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']",http://arxiv.org/abs/2209.13953v1
184,Estimating Subjective Crowd-Evaluations as an Additional Objective to Improve Natural Language Generation,http://arxiv.org/pdf/2104.05224v1,"[arxiv.Result.Author('Jakob Nyberg'), arxiv.Result.Author('Ramesh Manuvinakurike'), arxiv.Result.Author('Maike Paetzel-Prüsmann')]",,2021-04-12 06:33:16+00:00,"Human ratings are one of the most prevalent methods to evaluate the
performance of natural language processing algorithms. Similarly, it is common
to measure the quality of sentences generated by a natural language generation
model using human raters. In this paper, we argue for exploring the use of
subjective evaluations within the process of training language generation
models in a multi-task learning setting. As a case study, we use a
crowd-authored dialogue corpus to fine-tune six different language generation
models. Two of these models incorporate multi-task learning and use subjective
ratings of lines as part of an explicit learning goal. A human evaluation of
the generated dialogue lines reveals that utterances generated by the
multi-tasking models were subjectively rated as the most typical, most moving
the conversation forward, and least offensive. Based on these promising first
results, we discuss future research directions for incorporating subjective
human evaluations into language model training and to hence keep the human user
in the loop during the development process.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2104.05224v1
185,Ad Text Classification with Transformer-Based Natural Language Processing Methods,http://arxiv.org/pdf/2106.10899v2,"[arxiv.Result.Author('Umut Özdil'), arxiv.Result.Author('Büşra Arslan'), arxiv.Result.Author('D. Emre Taşar'), arxiv.Result.Author('Gökçe Polat'), arxiv.Result.Author('Şükrü Ozan')]",,2021-06-21 07:38:31+00:00,"In this study, a natural language processing-based (NLP-based) method is
proposed for the sector-wise automatic classification of ad texts created on
online advertising platforms. Our data set consists of approximately 21,000
labeled advertising texts from 12 different sectors. In the study, the
Bidirectional Encoder Representations from Transformers (BERT) model, which is
a transformer-based language model that is recently used in fields such as text
classification in the natural language processing literature, was used. The
classification efficiencies obtained using a pre-trained BERT model for the
Turkish language are shown in detail.",,cs.CL,['cs.CL'],http://arxiv.org/abs/2106.10899v2
186,Language Without Words: A Pointillist Model for Natural Language Processing,http://arxiv.org/pdf/1212.3228v1,"[arxiv.Result.Author('Peiyou Song'), arxiv.Result.Author('Anhei Shu'), arxiv.Result.Author('David Phipps'), arxiv.Result.Author('Dan Wallach'), arxiv.Result.Author('Mohit Tiwari'), arxiv.Result.Author('Jedidiah Crandall'), arxiv.Result.Author('George Luger')]",,2012-12-11 20:19:58+00:00,"This paper explores two separate questions: Can we perform natural language
processing tasks without a lexicon?; and, Should we? Existing natural language
processing techniques are either based on words as units or use units such as
grams only for basic classification tasks. How close can a machine come to
reasoning about the meanings of words and phrases in a corpus without using any
lexicon, based only on grams?
  Our own motivation for posing this question is based on our efforts to find
popular trends in words and phrases from online Chinese social media. This form
of written Chinese uses so many neologisms, creative character placements, and
combinations of writing systems that it has been dubbed the ""Martian Language.""
Readers must often use visual queues, audible queues from reading out loud, and
their knowledge and understanding of current events to understand a post. For
analysis of popular trends, the specific problem is that it is difficult to
build a lexicon when the invention of new ways to refer to a word or concept is
easy and common. For natural language processing in general, we argue in this
paper that new uses of language in social media will challenge machines'
abilities to operate with words as the basic unit of understanding, not only in
Chinese but potentially in other languages.","The 6th International Conference on Soft Computing and Intelligent
  Systems (SCIS-ISIS 2012) Kobe, Japan",cs.CL,"['cs.CL', 'cs.IR', 'cs.SI', 'I.2.7; H.2.8; H.3.1']",http://arxiv.org/abs/1212.3228v1
187,Hidden Markov Model Based Part of Speech Tagger for Sinhala Language,http://arxiv.org/pdf/1407.2989v1,"[arxiv.Result.Author('A. J. P. M. P. Jayaweera'), arxiv.Result.Author('N. G. J. Dias')]",,2014-07-10 23:57:54+00:00,"In this paper we present a fundamental lexical semantics of Sinhala language
and a Hidden Markov Model (HMM) based Part of Speech (POS) Tagger for Sinhala
language. In any Natural Language processing task, Part of Speech is a very
vital topic, which involves analysing of the construction, behaviour and the
dynamics of the language, which the knowledge could utilized in computational
linguistics analysis and automation applications. Though Sinhala is a
morphologically rich and agglutinative language, in which words are inflected
with various grammatical features, tagging is very essential for further
analysis of the language. Our research is based on statistical based approach,
in which the tagging process is done by computing the tag sequence probability
and the word-likelihood probability from the given corpus, where the linguistic
knowledge is automatically extracted from the annotated corpus. The current
tagger could reach more than 90% of accuracy for known words.","International Journal on Natural Language Computing (IJNLC) Vol.
  3, No.3, June 2014. Pages 9-23",cs.CL,"['cs.CL', 'I.2.7']",http://arxiv.org/abs/1407.2989v1
188,HerBERT: Efficiently Pretrained Transformer-based Language Model for Polish,http://arxiv.org/pdf/2105.01735v1,"[arxiv.Result.Author('Robert Mroczkowski'), arxiv.Result.Author('Piotr Rybak'), arxiv.Result.Author('Alina Wróblewska'), arxiv.Result.Author('Ireneusz Gawlik')]",,2021-05-04 20:16:17+00:00,"BERT-based models are currently used for solving nearly all Natural Language
Processing (NLP) tasks and most often achieve state-of-the-art results.
Therefore, the NLP community conducts extensive research on understanding these
models, but above all on designing effective and efficient training procedures.
Several ablation studies investigating how to train BERT-like models have been
carried out, but the vast majority of them concerned only the English language.
A training procedure designed for English does not have to be universal and
applicable to other especially typologically different languages. Therefore,
this paper presents the first ablation study focused on Polish, which, unlike
the isolating English language, is a fusional language. We design and
thoroughly evaluate a pretraining procedure of transferring knowledge from
multilingual to monolingual BERT-based models. In addition to multilingual
model initialization, other factors that possibly influence pretraining are
also explored, i.e. training objective, corpus size, BPE-Dropout, and
pretraining length. Based on the proposed procedure, a Polish BERT-based
language model -- HerBERT -- is trained. This model achieves state-of-the-art
results on multiple downstream tasks.",,cs.CL,"['cs.CL', 'cs.LG']",http://arxiv.org/abs/2105.01735v1
189,A Novel Task-Oriented Text Corpus in Silent Speech Recognition and its Natural Language Generation Construction Method,http://arxiv.org/pdf/1905.01974v1,"[arxiv.Result.Author('Dong Cao'), arxiv.Result.Author('Dongdong Zhang'), arxiv.Result.Author('HaiBo Chen')]",,2019-04-19 08:21:01+00:00,"Millions of people with severe speech disorders around the world may regain
their communication capabilities through techniques of silent speech
recognition (SSR). Using electroencephalography (EEG) as a biomarker for speech
decoding has been popular for SSR. However, the lack of SSR text corpus has
impeded the development of this technique. Here, we construct a novel
task-oriented text corpus, which is utilized in the field of SSR. In the
process of construction, we propose a task-oriented hybrid construction method
based on natural language generation algorithm. The algorithm focuses on the
strategy of data-to-text generation, and has two advantages including
linguistic quality and high diversity. These two advantages use template-based
method and deep neural networks respectively. In an SSR experiment with the
generated text corpus, analysis results show that the performance of our hybrid
construction method outperforms the pure method such as template-based natural
language generation or neural natural language generation models.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1905.01974v1
190,Specifying and Verbalising Answer Set Programs in Controlled Natural Language,http://arxiv.org/pdf/1804.10765v1,[arxiv.Result.Author('Rolf Schwitter')],,2018-04-28 09:12:38+00:00,"We show how a bi-directional grammar can be used to specify and verbalise
answer set programs in controlled natural language. We start from a program
specification in controlled natural language and translate this specification
automatically into an executable answer set program. The resulting answer set
program can be modified following certain naming conventions and the revised
version of the program can then be verbalised in the same subset of natural
language that was used as specification language. The bi-directional grammar is
parametrised for processing and generation, deals with referring expressions,
and exploits symmetries in the data structure of the grammar rules whenever
these grammar rules need to be duplicated. We demonstrate that verbalisation
requires sentence planning in order to aggregate similar structures with the
aim to improve the readability of the generated specification. Without
modifications, the generated specification is always semantically equivalent to
the original one; our bi-directional grammar is the first one that allows for
semantic round-tripping in the context of controlled natural language
processing. This paper is under consideration for acceptance in TPLP.",,cs.AI,['cs.AI'],http://arxiv.org/abs/1804.10765v1
191,A Framework for Creating Natural Language User Interfaces for Action-Based Applications,http://arxiv.org/pdf/cs/0412065v1,"[arxiv.Result.Author('Stephen Chong'), arxiv.Result.Author('Riccardo Pucella')]",,2004-12-17 03:24:51+00:00,"In this paper we present a framework for creating natural language interfaces
to action-based applications. Our framework uses a number of reusable
application-independent components, in order to reduce the effort of creating a
natural language interface for a given application. Using a type-logical
grammar, we first translate natural language sentences into expressions in an
extended higher-order logic. These expressions can be seen as executable
specifications corresponding to the original sentences. The executable
specifications are then interpreted by invoking appropriate procedures provided
by the application for which a natural language interface is being created.",,cs.CL,"['cs.CL', 'cs.HC', 'H.5.2; I.2.7; F.4.2; F.4.1']",http://arxiv.org/abs/cs/0412065v1
192,Knowledge Graph Extraction from Videos,http://arxiv.org/pdf/2007.10040v1,"[arxiv.Result.Author('Louis Mahon'), arxiv.Result.Author('Eleonora Giunchiglia'), arxiv.Result.Author('Bowen Li'), arxiv.Result.Author('Thomas Lukasiewicz')]",,2020-07-20 12:23:39+00:00,"Nearly all existing techniques for automated video annotation (or captioning)
describe videos using natural language sentences. However, this has several
shortcomings: (i) it is very hard to then further use the generated natural
language annotations in automated data processing, (ii) generating natural
language annotations requires to solve the hard subtask of generating
semantically precise and syntactically correct natural language sentences,
which is actually unrelated to the task of video annotation, (iii) it is
difficult to quantitatively measure performance, as standard metrics (e.g.,
accuracy and F1-score) are inapplicable, and (iv) annotations are
language-specific. In this paper, we propose the new task of knowledge graph
extraction from videos, i.e., producing a description in the form of a
knowledge graph of the contents of a given video. Since no datasets exist for
this task, we also include a method to automatically generate them, starting
from datasets where videos are annotated with natural language. We then
describe an initial deep-learning model for knowledge graph extraction from
videos, and report results on MSVD* and MSR-VTT*, two datasets obtained from
MSVD and MSR-VTT using our method.",,cs.CL,"['cs.CL', 'cs.AI']",http://arxiv.org/abs/2007.10040v1
193,"What a Creole Wants, What a Creole Needs",http://arxiv.org/pdf/2206.00437v1,"[arxiv.Result.Author('Heather Lent'), arxiv.Result.Author('Kelechi Ogueji'), arxiv.Result.Author('Miryam de Lhoneux'), arxiv.Result.Author('Orevaoghene Ahia'), arxiv.Result.Author('Anders Søgaard')]",,2022-06-01 12:22:34+00:00,"In recent years, the natural language processing (NLP) community has given
increased attention to the disparity of efforts directed towards high-resource
languages over low-resource ones. Efforts to remedy this delta often begin with
translations of existing English datasets into other languages. However, this
approach ignores that different language communities have different needs. We
consider a group of low-resource languages, Creole languages. Creoles are both
largely absent from the NLP literature, and also often ignored by society at
large due to stigma, despite these languages having sizable and vibrant
communities. We demonstrate, through conversations with Creole experts and
surveys of Creole-speaking communities, how the things needed from language
technology can change dramatically from one language to another, even when the
languages are considered to be very similar to each other, as with Creoles. We
discuss the prominent themes arising from these conversations, and ultimately
demonstrate that useful language technology cannot be built without involving
the relevant community.",,cs.CL,"['cs.CL', 'cs.CY']",http://arxiv.org/abs/2206.00437v1
194,LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation,http://arxiv.org/pdf/2210.15461v2,"[arxiv.Result.Author('Hongcheng Guo'), arxiv.Result.Author('Jiaheng Liu'), arxiv.Result.Author('Haoyang Huang'), arxiv.Result.Author('Jian Yang'), arxiv.Result.Author('Zhoujun Li'), arxiv.Result.Author('Dongdong Zhang'), arxiv.Result.Author('Zheng Cui'), arxiv.Result.Author('Furu Wei')]",,2022-10-19 12:21:39+00:00,"Multimodal Machine Translation (MMT) focuses on enhancing text-only
translation with visual features, which has attracted considerable attention
from both natural language processing and computer vision communities. Recent
advances still struggle to train a separate model for each language pair, which
is costly and unaffordable when the number of languages increases in the real
world. In other words, the multilingual multimodal machine translation
(Multilingual MMT) task has not been investigated, which aims to handle the
aforementioned issues by providing a shared semantic space for multiple
languages. Besides, the image modality has no language boundaries, which is
superior to bridging the semantic gap between languages. To this end, we first
propose the Multilingual MMT task by establishing two new Multilingual MMT
benchmark datasets covering seven languages. Then, an effective baseline LVP-M3
using visual prompts is proposed to support translations between different
languages, which includes three stages (token encoding, language-aware visual
prompt generation, and language translation). Extensive experimental results on
our constructed benchmark datasets demonstrate the effectiveness of LVP-M3
method for Multilingual MMT.",,cs.CL,"['cs.CL', 'cs.AI']",http://arxiv.org/abs/2210.15461v2
195,Improve Bilingual TTS Using Dynamic Language and Phonology Embedding,http://arxiv.org/pdf/2212.03435v1,"[arxiv.Result.Author('Fengyu Yang'), arxiv.Result.Author('Jian Luan'), arxiv.Result.Author('Yujun Wang')]",,2022-12-07 03:46:18+00:00,"In most cases, bilingual TTS needs to handle three types of input scripts:
first language only, second language only, and second language embedded in the
first language. In the latter two situations, the pronunciation and intonation
of the second language are usually quite different due to the influence of the
first language. Therefore, it is a big challenge to accurately model the
pronunciation and intonation of the second language in different contexts
without mutual interference. This paper builds a Mandarin-English TTS system to
acquire more standard spoken English speech from a monolingual Chinese speaker.
We introduce phonology embedding to capture the English differences between
different phonology. Embedding mask is applied to language embedding for
distinguishing information between different languages and to phonology
embedding for focusing on English expression. We specially design an embedding
strength modulator to capture the dynamic strength of language and phonology.
Experiments show that our approach can produce significantly more natural and
standard spoken English speech of the monolingual Chinese speaker. From
analysis, we find that suitable phonology control contributes to better
performance in different scenarios.",,cs.SD,"['cs.SD', 'cs.CL', 'eess.AS']",http://arxiv.org/abs/2212.03435v1
196,Exploring Language Similarities with Dimensionality Reduction Technique,http://arxiv.org/pdf/1902.06092v1,[arxiv.Result.Author('Sangarshanan Veeraraghavan')],,2019-02-16 11:27:21+00:00,"In recent years several novel models were developed to process natural
language, development of accurate language translation systems have helped us
overcome geographical barriers and communicate ideas effectively. These models
are developed mostly for a few languages that are widely used while other
languages are ignored. Most of the languages that are spoken share lexical,
syntactic and sematic similarity with several other languages and knowing this
can help us leverage the existing model to build more specific and accurate
models that can be used for other languages, so here I have explored the idea
of representing several known popular languages in a lower dimension such that
their similarities can be visualized using simple 2 dimensional plots. This can
even help us understand newly discovered languages that may not share its
vocabulary with any of the existing languages.",,cs.CL,['cs.CL'],http://arxiv.org/abs/1902.06092v1
197,On the Language-specificity of Multilingual BERT and the Impact of Fine-tuning,http://arxiv.org/pdf/2109.06935v2,"[arxiv.Result.Author('Marc Tanti'), arxiv.Result.Author('Lonneke van der Plas'), arxiv.Result.Author('Claudia Borg'), arxiv.Result.Author('Albert Gatt')]",,2021-09-14 19:28:31+00:00,"Recent work has shown evidence that the knowledge acquired by multilingual
BERT (mBERT) has two components: a language-specific and a language-neutral
one. This paper analyses the relationship between them, in the context of
fine-tuning on two tasks -- POS tagging and natural language inference -- which
require the model to bring to bear different degrees of language-specific
knowledge. Visualisations reveal that mBERT loses the ability to cluster
representations by language after fine-tuning, a result that is supported by
evidence from language identification experiments. However, further experiments
on 'unlearning' language-specific representations using gradient reversal and
iterative adversarial learning are shown not to add further improvement to the
language-independent component over and above the effect of fine-tuning. The
results presented here suggest that the process of fine-tuning causes a
reorganisation of the model's limited representational capacity, enhancing
language-independent representations at the expense of language-specific ones.",,cs.CL,"['cs.CL', 'cs.NE']",http://arxiv.org/abs/2109.06935v2
198,On the Applicability of Language Models to Block-Based Programs,http://arxiv.org/pdf/2302.03927v1,"[arxiv.Result.Author('Elisabeth Griebl'), arxiv.Result.Author('Benedikt Fein'), arxiv.Result.Author('Florian Obermüller'), arxiv.Result.Author('Gordon Fraser'), arxiv.Result.Author('René Just')]",,2023-02-08 07:54:25+00:00,"Block-based programming languages like Scratch are increasingly popular for
programming education and end-user programming. Recent program analyses build
on the insight that source code can be modelled using techniques from natural
language processing. Many of the regularities of source code that support this
approach are due to the syntactic overhead imposed by textual programming
languages. This syntactic overhead, however, is precisely what block-based
languages remove in order to simplify programming. Consequently, it is unclear
how well this modelling approach performs on block-based programming languages.
In this paper, we investigate the applicability of language models for the
popular block-based programming language Scratch. We model Scratch programs
using n-gram models, the most essential type of language model, and
transformers, a popular deep learning model. Evaluation on the example tasks of
code completion and bug finding confirm that blocks inhibit predictability, but
the use of language models is nevertheless feasible. Our findings serve as
foundation for improving tooling and analyses for block-based languages.",,cs.PL,"['cs.PL', 'cs.SE', '68-04', 'D.2.5; D.2.3']",http://arxiv.org/abs/2302.03927v1
199,Process-Oriented Parallel Programming with an Application to Data-Intensive Computing,http://arxiv.org/pdf/1407.5524v1,[arxiv.Result.Author('Edward Givelberg')],,2014-07-21 15:16:37+00:00,"We introduce process-oriented programming as a natural extension of
object-oriented programming for parallel computing. It is based on the
observation that every class of an object-oriented language can be instantiated
as a process, accessible via a remote pointer. The introduction of process
pointers requires no syntax extension, identifies processes with programming
objects, and enables processes to exchange information simply by executing
remote methods. Process-oriented programming is a high-level language
alternative to multithreading, MPI and many other languages, environments and
tools currently used for parallel computations. It implements natural
object-based parallelism using only minimal syntax extension of existing
languages, such as C++ and Python, and has therefore the potential to lead to
widespread adoption of parallel programming. We implemented a prototype system
for running processes using C++ with MPI and used it to compute a large
three-dimensional Fourier transform on a computer cluster built of commodity
hardware components. Three-dimensional Fourier transform is a prototype of a
data-intensive application with a complex data-access pattern. The
process-oriented code is only a few hundred lines long, and attains very high
data throughput by achieving massive parallelism and maximizing hardware
utilization.",,cs.PL,"['cs.PL', 'cs.DC']",http://arxiv.org/abs/1407.5524v1
