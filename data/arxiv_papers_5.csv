,Title,PDF URL,Author,DOI,Published Date,Summary
0,DialogueLLM: Context and Emotion Knowledge-Tuned LLaMA Models for Emotion Recognition in Conversations,http://arxiv.org/pdf/2310.11374v1,"[arxiv.Result.Author('Yazhou Zhang'), arxiv.Result.Author('Mengyao Wang'), arxiv.Result.Author('Prayag Tiwari'), arxiv.Result.Author('Qiuchi Li'), arxiv.Result.Author('Benyou Wang'), arxiv.Result.Author('Jing Qin')]",,2023-10-17 16:15:34+00:00,"Large language models (LLMs) and their variants have shown extraordinary
efficacy across numerous downstream natural language processing (NLP) tasks,
which has presented a new vision for the development of NLP. Despite their
remarkable performance in natural language generating (NLG), LLMs lack a
distinct focus on the emotion understanding domain. As a result, using LLMs for
emotion recognition may lead to suboptimal and inadequate precision. Another
limitation of LLMs is that they are typical trained without leveraging
multi-modal information. To overcome these limitations, we propose DialogueLLM,
a context and emotion knowledge tuned LLM that is obtained by fine-tuning LLaMA
models with 13,638 multi-modal (i.e., texts and videos) emotional dialogues.
The visual information is considered as the supplementary knowledge to
construct high-quality instructions. We offer a comprehensive evaluation of our
proposed model on three benchmarking emotion recognition in conversations (ERC)
datasets and compare the results against the SOTA baselines and other SOTA
LLMs. Additionally, DialogueLLM-7B can be easily trained using LoRA on a 40GB
A100 GPU in 5 hours, facilitating reproducibility for other researchers."
1,VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights,http://arxiv.org/pdf/2310.11368v1,"[arxiv.Result.Author('Shanshan Xu'), arxiv.Result.Author('Leon Staufer'), arxiv.Result.Author('Santosh T. Y. S. S'), arxiv.Result.Author('Oana Ichim'), arxiv.Result.Author('Corina Heri'), arxiv.Result.Author('Matthias Grabmair')]",,2023-10-17 16:05:52+00:00,"Recognizing vulnerability is crucial for understanding and implementing
targeted support to empower individuals in need. This is especially important
at the European Court of Human Rights (ECtHR), where the court adapts
Convention standards to meet actual individual needs and thus ensures effective
human rights protection. However, the concept of vulnerability remains elusive
at the ECtHR and no prior NLP research has dealt with it. To enable future
research in this area, we present VECHR, a novel expert-annotated multi-label
dataset comprising of vulnerability type classification and explanation
rationale. We benchmark the performance of state-of-the-art models on VECHR
from both prediction and explainability perspectives. Our results demonstrate
the challenging nature of the task with lower prediction performance and
limited agreement between models and experts. Further, we analyze the
robustness of these models in dealing with out-of-domain (OOD) data and observe
overall limited performance. Our dataset poses unique challenges offering
significant room for improvement regarding performance, explainability, and
robustness."
2,Utilizing Weak Supervision To Generate Indonesian Conservation Dataset,http://arxiv.org/pdf/2310.11258v1,"[arxiv.Result.Author('Mega Fransiska'), arxiv.Result.Author('Diah Pitaloka'), arxiv.Result.Author('Saripudin'), arxiv.Result.Author('Satrio Putra'), arxiv.Result.Author('Lintang Sutawika')]",,2023-10-17 13:23:18+00:00,"Weak supervision has emerged as a promising approach for rapid and
large-scale dataset creation in response to the increasing demand for
accelerated NLP development. By leveraging labeling functions, weak supervision
allows practitioners to generate datasets quickly by creating learned label
models that produce soft-labeled datasets. This paper aims to show how such an
approach can be utilized to build an Indonesian NLP dataset from conservation
news text. We construct two types of datasets: multi-class classification and
sentiment classification. We then provide baseline experiments using various
pretrained language models. These baseline results demonstrate test
performances of 59.79% accuracy and 55.72% F1-score for sentiment
classification, 66.87% F1-score-macro, 71.5% F1-score-micro, and 83.67% ROC-AUC
for multi-class classification. Additionally, we release the datasets and
labeling functions used in this work for further research and exploration."
3,Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations,http://arxiv.org/pdf/2310.11207v1,"[arxiv.Result.Author('Shiyuan Huang'), arxiv.Result.Author('Siddarth Mamidanna'), arxiv.Result.Author('Shreedhar Jangam'), arxiv.Result.Author('Yilun Zhou'), arxiv.Result.Author('Leilani H. Gilpin')]",,2023-10-17 12:34:32+00:00,"Large language models (LLMs) such as ChatGPT have demonstrated superior
performance on a variety of natural language processing (NLP) tasks including
sentiment analysis, mathematical reasoning and summarization. Furthermore,
since these models are instruction-tuned on human conversations to produce
""helpful"" responses, they can and often will produce explanations along with
the response, which we call self-explanations. For example, when analyzing the
sentiment of a movie review, the model may output not only the positivity of
the sentiment, but also an explanation (e.g., by listing the sentiment-laden
words such as ""fantastic"" and ""memorable"" in the review). How good are these
automatically generated self-explanations? In this paper, we investigate this
question on the task of sentiment analysis and for feature attribution
explanation, one of the most commonly studied settings in the interpretability
literature (for pre-ChatGPT models). Specifically, we study different ways to
elicit the self-explanations, evaluate their faithfulness on a set of
evaluation metrics, and compare them to traditional explanation methods such as
occlusion or LIME saliency maps. Through an extensive set of experiments, we
find that ChatGPT's self-explanations perform on par with traditional ones, but
are quite different from them according to various agreement metrics, meanwhile
being much cheaper to produce (as they are generated along with the
prediction). In addition, we identified several interesting characteristics of
them, which prompt us to rethink many current model interpretability practices
in the era of ChatGPT(-like) LLMs."
4,ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing,http://arxiv.org/pdf/2310.11166v1,"[arxiv.Result.Author('Quoc-Nam Nguyen'), arxiv.Result.Author('Thang Chau Phan'), arxiv.Result.Author('Duc-Vu Nguyen'), arxiv.Result.Author('Kiet Van Nguyen')]",,2023-10-17 11:34:50+00:00,"English and Chinese, known as resource-rich languages, have witnessed the
strong development of transformer-based language models for natural language
processing tasks. Although Vietnam has approximately 100M people speaking
Vietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA,
performed well on general Vietnamese NLP tasks, including POS tagging and named
entity recognition. These pre-trained language models are still limited to
Vietnamese social media tasks. In this paper, we present the first monolingual
pre-trained language model for Vietnamese social media texts, ViSoBERT, which
is pre-trained on a large-scale corpus of high-quality and diverse Vietnamese
social media texts using XLM-R architecture. Moreover, we explored our
pre-trained model on five important natural language downstream tasks on
Vietnamese social media texts: emotion recognition, hate speech detection,
sentiment analysis, spam reviews detection, and hate speech spans detection.
Our experiments demonstrate that ViSoBERT, with far fewer parameters, surpasses
the previous state-of-the-art models on multiple Vietnamese social media tasks.
Our ViSoBERT model is
available\footnote{\url{https://huggingface.co/uitnlp/visobert}} only for
research purposes."
